<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bryan Travis Smith, Ph.D</title><link href="http://www.bryantravissmith.com/" rel="alternate"></link><link href="http://www.bryantravissmith.com/feeds/galvanize.atom.xml" rel="self"></link><id>http://www.bryantravissmith.com/</id><updated>2015-06-08T10:30:00-07:00</updated><entry><title>Galvanize - Week 02 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-01/" rel="alternate"></link><updated>2015-06-08T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-08:galvanize/galvanize-data-science-02-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 1&lt;/h2&gt;
&lt;p&gt;Since this is the first day of the week we started with an hour long assessment on what we did last week.  The assessment was straight forward, and very doable with a modest understanding of the previous material.   &lt;/p&gt;
&lt;p&gt;After the test, we started a lecture on probability, which we finished in the afternoon.  There was the individual sprint after the morning lecture, and a paired spring as the afternoon lecture&lt;/p&gt;
&lt;h2&gt;Conditional Probabilities&lt;/h2&gt;
&lt;p&gt;We started with some simple questions like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose two cards are drawn from a standard 52 card deck.  What's the probability that the first is a queen and the second is a king?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(K,Q\right) = P\left(K|Q\right)*P\left(Q\right) = \frac{4}{51} * \frac{4}{52} = \frac{16}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 16./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00603318250377&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that both cards are queens?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q|Q\right)*P\left(Q\right) = \frac{3}{51} * \frac{4}{52} = \frac{12}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 12./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00452488687783&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose that before the second card was drawn, the first was inserted back into the deck and the deck reshuffled. What's the probability that both cards are queens?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q\right)*P\left(Q\right) = \frac{4}{52} * \frac{4}{52} = \frac{16}{2705}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 16./2705

Answer:  0.00591497227357
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We had similar questions about tables of data:&lt;/p&gt;
&lt;p&gt;A Store Manager wants to understand how his customers use different payment methods, and suspects that the size of the purchase is a major deciding factor. He organizes the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="right"&gt;Cash&lt;/th&gt;
&lt;th align="right"&gt;Debit&lt;/th&gt;
&lt;th align="right"&gt;Credit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Under 20&lt;/td&gt;
&lt;td align="right"&gt;400&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20 - 50&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;1200&lt;/td&gt;
&lt;td align="right"&gt;800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Over 50&lt;/td&gt;
&lt;td align="right"&gt;100&lt;/td&gt;
&lt;td align="right"&gt;600&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer spent over $50, what's the probability that the customer used a credit card?&lt;/p&gt;
&lt;p&gt;$$P\left(C|S&amp;gt;$50\right) = \frac{1400}{100+600+1400}$$ &lt;/p&gt;
&lt;p&gt;print "Answer: ", 1400./(100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.666666666667&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer paid in cash, what's the probability that the customer spent less than $20?&lt;/p&gt;
&lt;p&gt;$$P\left(S&amp;lt;20|Cash\right) = \frac{400}{400+200+100}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+200+100)&lt;/p&gt;
&lt;p&gt;Answer:  0.571428571429&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that a customer spent under $20 using cash?&lt;/p&gt;
&lt;p&gt;$$P\left(S &amp;lt; $20,Cash\right) = \frac{400}{400+150+150+200+1200+800+100+600+1400}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+150+150+200+1200+800+100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.08&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also had a question about job offers - something near and dear to our hearts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A gSchool grad is looking for her first job!  Given that she is freaked out, her chances of not getting an offer are 70%.  Given that she isn't freaked out, her chances of not getting an offer are 30%.  Suppose that the probability that she's freaked out is 80%. What's the probability that she gets an offer?&lt;/p&gt;
&lt;p&gt;$$P\left(Offer|Freak Out\right) = 0.7$$  &lt;/p&gt;
&lt;p&gt;$$P\left(Offer|No Freak Oout\right) = 0.3$$&lt;/p&gt;
&lt;p&gt;$$P\left(Freak Out\right) = 0.8$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ P\left(A\right) = \Sigma_{B} P\left(A|B\right) $$&lt;/p&gt;
&lt;p&gt;$$ P\left(Offer\right) = P\left(Offer|Freak Out\right) * P\left(Freak Out\right) + P\left(Offer|No Freak Out\right) * P\left(No Freak Out\right)$$&lt;/p&gt;
&lt;p&gt;$$P\left(Offer\right) = 0.7 * 0.8 + 0.3 * 0.2$$ &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.7 * 0.8 + 0.3 * 0.2

Answer:  0.62
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also tackled the deep issue go heroin use at Google: &lt;/p&gt;
&lt;p&gt;*. Google decides to do random drug tests for heroin on their employees.
   They know that 3% of their population uses heroin. The drug test has the
   following accuracy: The test correctly identifies 95% of the
   heroin users (sensitivity) and 90% of the non-users (specificity).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test Results&lt;/th&gt;
&lt;th&gt;Uses heroin&lt;/th&gt;
&lt;th&gt;Doesn't use heroin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tests positive&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tests negative&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alice gets tested and the test comes back positive. What is the probability
   that she uses heroin?&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test)}$$&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test|Heroin) P(Heroin) + P(Positive Test|No Heroin) P(No Heroin)}$$ &lt;/p&gt;
&lt;p&gt;$$ = \frac{0.95 \ 0.03}{0.95 \ 0.03 + 0.1 \ 0.97}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.95*0.03/(0.95*0.03 + 0.1*0.97)

Answer:  0.227091633466
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally we had the manditory birthday problem:   &lt;/p&gt;
&lt;p&gt;*The Birthday Problem.  Suppose there are 23 people in a data science class, lined up in a single file line.&lt;br /&gt;
Let A_i be the probability that the i'th person doesn't have the same birthday as the j'th person for any j &amp;lt; i.&lt;br /&gt;
Use the chain rule from probability to calculate the probability that at least 2 people share the same birthday. &lt;/p&gt;
&lt;p&gt;$$P(1,2,3,...,23) = \mbox{Probability that 23 people do not have the same birthday}$$&lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2, \ 3, \ ..., \ 23) = P(1) \ P(2|1) \ P(3 \ | \ 2 \ , \ 1) \ ... \ P(23|22 \ , \ ... \ , \ 2, \ 1)$$&lt;/p&gt;
&lt;p&gt;Given that 2 people don't have the same birthday, there are 363 days that are not taken that a new person could have:&lt;/p&gt;
&lt;p&gt;$$P(3|2,1) = \frac{363}{365}$$&lt;/p&gt;
&lt;p&gt;Similarly, given that 3 people don't have the same birthday, then there are 362 days not occupied:&lt;/p&gt;
&lt;p&gt;$$P(4|3,2,1) = \frac{362}{365}$$&lt;/p&gt;
&lt;p&gt;Extending this to the problem we have the probability of no matching birthdays being &lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2,\ 3, \ ..., \ 23) = \frac{1 \ 364 \ 363 \ ... \ (365-23)}{365^{23}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bday(N=23):
    prob = 1
    for i in range(1,N+1):
        prob = prob*(365.0-i+1)/365.
    return prob

print bday()

0.492702765676
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The probability of having 2 or more matches is us the inverse of this&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1 - P(No Maches|23)$$&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1-0.4927$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, round(100-100*bday(),1)

Answer:  50.7
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Distributions&lt;/h2&gt;
&lt;p&gt;The afternoon paired programming assignment involved developing an intuition for and using various distributions.&lt;/p&gt;
&lt;h3&gt;Discrete:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model one instance of a success or failure trial (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binomial&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of successes out of a number of trials (n), each with probability of success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Poisson&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model the number of events occurring in a fixed interval&lt;/li&gt;
&lt;li&gt;Events occur at an average rate (lambda) independently of the last event&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Geometric&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence of Bernoulli trials until first success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Continuous:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uniform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any of the values in the interval of a to b are equally likely&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Commonly occurring distribution shaped like a bell curve&lt;/li&gt;
&lt;li&gt;Often comes up because of the Central Limit Theorem (to be discussed later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exponential&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model time between Poisson events&lt;/li&gt;
&lt;li&gt;Events occur continuously and independently&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of our questions involved being given examples and identify the distribution that describes it.&lt;/p&gt;
&lt;p&gt;Often we have to identify what distribution we should use to model a real-life
situation. This exercise is designed to equip you with the ability to do so.&lt;/p&gt;
&lt;p&gt;*. A typist makes on average 2 mistakes per page.  What is the probability of a particular page having no errors on it?&lt;/p&gt;
&lt;p&gt;$$X = Poisson(\lambda = 2 \frac{mistakes}{page})$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Mistakes on a Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of No Mistakes: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt; &lt;span class="n"&gt;Mistakes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Components are packed in boxes of 20. The probability of a component being
   defective is 0.1.  What is the probability of a box containing 2 defective components?&lt;/p&gt;
&lt;p&gt;$$ X = Binomial(p=0.1,k=2,n=20) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(20)
y = sc.binom.pmf(x,20,.1)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of Defective Components&amp;quot;)
plt.ylabel(&amp;quot;Probability&amp;quot;)
print &amp;quot;Prob of 2 Defects: &amp;quot;, sc.binom.pmf(2,20,.1)

Prob of 2 Defects:  0.285179807064
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_26_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Patrons arrive at a local bar at a mean rate of 30 per hour.  What is the probability that the bouncer has to wait more than 3 minutes to card the next patron?&lt;/p&gt;
&lt;p&gt;$$X = Exponential(\lambda = .5 \frac{cards}{minute})$$&lt;/p&gt;
&lt;p&gt;$$ P(t \ &amp;gt; \ 3min) = \int_{t=3}^{t=\infty} Exponential(\lambda = .5 \frac{cards}{minute}) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, sc.expon.cdf(1e10,scale=0.5)-sc.expon.cdf(3,scale=0.5)


 Anser:  0.00247875217667



x = np.linspace(0,5,1000)
y = sc.expon.pdf(x,scale = 0.5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.ylim([0,0.1])
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=3, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_29_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. A variable is normally distributed with a mean of 120 and a standard
   deviation of 5. One score is randomly sampled. What is the probability the score is above 127?&lt;/p&gt;
&lt;p&gt;$$Z = (127-120)/5 = 7/5 = 1.4$$&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;gt;1.4) = 1 - .91924 \ \mbox{(area to left)}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(100,140,1000)
y = sc.norm.pdf(x,loc=120,scale=5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Variable Value&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=127, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. You need to find a tall person, at least 6 feet tall, to help you reach
   a cookie jar. 8% of the population is 6 feet or taller.  If you wait on the sidewalk, how many people would you expect to have passed you by before you'd have a candidate to reach the jar?&lt;/p&gt;
&lt;p&gt;$$X = Geometric(p=0.08)$$&lt;/p&gt;
&lt;p&gt;$$average = \frac{1}{p} = \frac{1}{0.08} = 12.5$$&lt;/p&gt;
&lt;p&gt;We round up - 13th person is expected to be it - 12 people pass.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.pmf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106020790&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_33_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.cdf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Cumlative Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106451c50&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_34_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A harried passenger will be several minutes late for a scheduled 10 A.M.
   flight to NYC. Nevertheless, he might still make the flight, since boarding
   is always allowed until 10:10 A.M., and boarding is sometimes
   permitted up to 10:30 AM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assuming the extended boarding time is &lt;strong&gt;uniformly distributed&lt;/strong&gt; over the above
   limits, find the probability that the passenger will make his flight,
   assuming he arrives at the boarding gate at 10:25.&lt;/p&gt;
&lt;p&gt;$$X = Uniform(0,30)$$&lt;/p&gt;
&lt;p&gt;$$P( x &amp;gt; 25 ) = \int_{25}^{30} \frac{dx}{30}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 5./30

Answer:  0.166666666667



x = np.linspace(0,30,1000)
y = sc.uniform.pdf(x,loc=0,scale=30)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes Late&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=25, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_37_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Covariance and Joint Distribution&lt;/h2&gt;
&lt;p&gt;Suppose a university wants to look for factors that are correlated with the GPA of the students that they
are going to admit. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;admissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../probability/data/admissions.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;addmissions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;covariance&lt;/code&gt; function and compute the covariance matrix of the dataframe. Check your results 
   with &lt;code&gt;df.cov()&lt;/code&gt;. Make sure you understand what each of the numbers in the matrix represents&lt;/p&gt;
&lt;p&gt;def make_cov(df):
    N = len(df)
    cols = df.columns
    return [[(df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;from pprint import pprint
pprint (make_cov(addmissions))&lt;/p&gt;
&lt;p&gt;[[332910756.59847927, 4014.9337921708066, -1226.2147143883631],
 [4014.9337921708066, 0.087883196618951942, -0.028782641179958546],
 [-1226.2147143883631, -0.028782641179958546, 112]]&lt;/p&gt;
&lt;p&gt;addmissions.cov()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;3.329410e+08&lt;/td&gt;
      &lt;td&gt;4015.299085&lt;/td&gt;
      &lt;td&gt;-1226.326280&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;4.015299e+03&lt;/td&gt;
      &lt;td&gt;0.087891&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-1.226326e+03&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
      &lt;td&gt;112.977442&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;normalize&lt;/code&gt; function that would compute the correlation matrix from the covariance matrix.
   Check your results with &lt;code&gt;df.corr()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def make_corr(df):
    N = len(df)
    cols = df.columns
    return [[((df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2))/(df[x].std() * df[y].std()) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;pprint (make_corr(addmissions))&lt;/p&gt;
&lt;p&gt;[[0.99990902474526921, 0.74220186205662952, -0.0063224730309758576],
 [0.74220186205662952, 0.99990902474528243, -0.0091340229188836969],
 [-0.0063224730309758576, -0.0091340229188836969, 0.99134834685640671]]&lt;/p&gt;
&lt;p&gt;addmissions.corr()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;You should identify &lt;code&gt;family_income&lt;/code&gt; as being the most correlated with GPA. The university wants to make
   an effort to make sure people of all family income are being fairly represented in the admissions process.
   In order to achieve that, different GPA thresholds will be set according to family income. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The low, medium and high family income groups are &lt;code&gt;0 to 26832&lt;/code&gt;, &lt;code&gt;26833 to 37510&lt;/code&gt; and &lt;code&gt;37511 to 51112&lt;/code&gt; respectively. 
   Implement a function that would plot the distribution of GPA scores for each family income category. These are the 
   conditional probability distributions of &lt;code&gt;gpa&lt;/code&gt; given certain levels of &lt;code&gt;family_income&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_hist(df):
    low = df[df.family_income &amp;lt;= 26832]
    med = df[(df.family_income &amp;gt; 26832) &amp;amp; (df.family_income &amp;lt;= 37519)]
    high = df[(df.family_income &amp;gt; 37519) &amp;amp; (df.family_income &amp;lt;= 51112)]
    low.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;blue&amp;quot;,label=&amp;#39;Low Income&amp;#39;)
    med.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;green&amp;quot;,label=&amp;#39;Medium Income&amp;#39;)
    high.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;red&amp;quot;,label=&amp;#39;High Income&amp;#39;)
    plt.xlim([2.0, 4.0])
    plt.legend()
    plt.show()

make_hist(addmissions)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_47_0.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the university decides to accept students with GPA above the 90th percentile within the respective family 
   income categories, what are the GPA thresholds for each of the categories?&lt;/p&gt;
&lt;p&gt;low = addmissions[addmissions.family_income &amp;lt;= 26832]
med = addmissions[(addmissions.family_income &amp;gt; 26832) &amp;amp; (addmissions.family_income &amp;lt;= 37519)]
high = addmissions[(addmissions.family_income &amp;gt; 37519) &amp;amp; (addmissions.family_income &amp;lt;= 51112)]
print "Low 90th Percentile", low.gpa.quantile(.9)
print "Medium 90th Percentile", med.gpa.quantile(.9)
print "High 90th Percentile", high.gpa.quantile(.9)&lt;/p&gt;
&lt;p&gt;Low 90th Percentile 3.01
Medium 90th Percentile 3.26
High 90th Percentile 3.36&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pearson Correlation vs Spearman Correlation&lt;/h2&gt;
&lt;p&gt;The Pearson correlation evaluates the linear relationship between two continuous 
variables. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables
without assuming linearity of the variables. Spearman correlation is often more robust in capturing non-linear relationship
between variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In addition to the &lt;code&gt;family_income&lt;/code&gt; and &lt;code&gt;parent_avg_age&lt;/code&gt;, you are also given data about the number of hours the 
   students studied. Load the new data in from &lt;code&gt;data/admissions_with_study_hrs_and_sports.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;studydf = pd.read_csv('../probability/data/admissions_with_study_hrs_and_sports.csv')
studydf.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;family_income_cat&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;th&gt;hrs_studied&lt;/th&gt;
      &lt;th&gt;sport_performance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;49.463745&lt;/td&gt;
      &lt;td&gt;0.033196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;16.414467&lt;/td&gt;
      &lt;td&gt;0.000317&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;4.937079&lt;/td&gt;
      &lt;td&gt;0.021845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;160.210286&lt;/td&gt;
      &lt;td&gt;0.153819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;36.417860&lt;/td&gt;
      &lt;td&gt;0.010444&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make a scatter plot of the &lt;code&gt;gpa&lt;/code&gt; against &lt;code&gt;hrs_studied&lt;/code&gt;. Make the points more transperant so you can see the density
   of the points. Use the following command get the slope and intercept of a straight line to fit the data.&lt;/p&gt;
&lt;p&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.hrs_studied)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept&lt;/p&gt;
&lt;p&gt;studydf.plot(kind='scatter',x='gpa',y='hrs_studied',alpha=0.01)
plt.plot(x,y,color='red')
plt.xlabel("GPA")
plt.ylabel("Hours Studied")
plt.show()&lt;/p&gt;
&lt;p&gt;494.329335528 -1400.63719543 0.475940264662 0.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_53_1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the functions &lt;code&gt;scipy.stats.pearsonr&lt;/code&gt; and &lt;code&gt;scipy.stats.spearmanr&lt;/code&gt; to compute the Pearson and Spearman correlation&lt;/p&gt;
&lt;p&gt;print sc.pearsonr(studydf.gpa,studydf.hrs_studied)
print sc.spearmanr(studydf.gpa,studydf.hrs_studied)&lt;/p&gt;
&lt;p&gt;(0.47594026466220946, 0.0)
(0.98495916559333341, 0.0)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat step &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; for &lt;code&gt;gpa&lt;/code&gt; and &lt;code&gt;sport_performance&lt;/code&gt;. Is there a strong relationship between the two variables?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept

studydf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()

0.00979813693421 0.0585103217504 0.0238485969548 0.0124044928587
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_57_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print sc.pearsonr(studydf.gpa,studydf.sport_performance)
print sc.spearmanr(studydf.gpa,studydf.sport_performance)

(0.023848596954761905, 0.012404492858691094)
(0.0022881402736224248, 0.81043264616449484)



temp = studydf[studydf.gpa &amp;gt; 3.0]
slope, intercept, r_value, p_value, std_err = sc.linregress(temp.gpa,temp.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(temp.gpa.min(),temp.gpa.max(),100)
y = slope*x+intercept

temp.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()
print sc.pearsonr(temp.gpa,temp.sport_performance)
print sc.spearmanr(temp.gpa,temp.sport_performance)

0.65608660543 -2.03523616554 0.945140987506 0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_59_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.94514098750633013, 0.0)
(1.0, 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Part 4: Distribution Simulation&lt;/h2&gt;
&lt;p&gt;Often times in real life applications, we can specify the values of a variable to be of a particular distribution,
for example the number of sales made in the next month can be modeled as a uniform distribution over the range of
5000 and 6000.&lt;/p&gt;
&lt;p&gt;In this scenario, we are modeling &lt;code&gt;profit&lt;/code&gt; as a product of &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt;,
where &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt; can be modeled as probabilistic distributions.
By randomly drawing values from these distributions, we are able to get a distribution of the range of &lt;code&gt;profit&lt;/code&gt; 
based on the uncertainties in the other variables.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Profit = Number of views * Conversion * Profit per sale&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Assumptions:
- &lt;code&gt;Number of views&lt;/code&gt; is a uniform distribution over the range of 5000 and 6000
- &lt;code&gt;Conversion is a binomial distribution where the probability of success is&lt;/code&gt;0.12&lt;code&gt;for each sale among the&lt;/code&gt;Number on views made 
- &lt;code&gt;Profit per sale&lt;/code&gt; has &lt;code&gt;0.2&lt;/code&gt; probability of taking the value &lt;code&gt;50&lt;/code&gt; (for wholesale) and &lt;code&gt;0.8&lt;/code&gt; of 
  taking the value &lt;code&gt;60&lt;/code&gt; (non-wholesale)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given the distributions of each of variables, use scipy to write a function that would draw random values from each of the distributions to simulate a distribution for &lt;code&gt;profit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def get_profit():
    num_views = np.round(sc.uniform.rvs(loc=5000,scale=1000,size=1),0)
    conversions = sc.binom.rvs(num_views,0.12)
    wholesale = sc.binom.rvs(conversions,0.2)
    return wholesale&lt;em&gt;50+(conversions-wholesale)&lt;/em&gt;60&lt;/p&gt;
&lt;p&gt;profits = np.array([get_profit() for i in xrange(100000)])
plt.hist(profits)
plt.show()
print "Low: ",np.percentile(profits,2.5)
print "High: ",np.percentile(profits,97.5)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_61_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Low&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;33800.0&lt;/span&gt;
&lt;span class="n"&gt;High&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;42920.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-05/" rel="alternate"></link><updated>2015-06-05T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-05:galvanize/galvanize-data-science-01-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 5&lt;/h2&gt;
&lt;p&gt;This morning we started with a reflection about the week, and completed a survey about our progress and thoughts on the program.   I think it is a very strong, hands-on program so far.&lt;/p&gt;
&lt;p&gt;The morning lesson and sprint was on using (pandas)[http://pandas.pydata.org/].   We were given some hospital data in a CSV format, read in the data, and answered a number of questions about most common diseases, most expensive procedures, the post profitable hospitals, and various subsets of these question on different conditions.  It was a simple exercise that gave us practice making new variables, grouping, and subsetting to look massage the data into a form that allowed us to answer the questions.&lt;/p&gt;
&lt;p&gt;During our lunch we had a presentation on learning, and the approach and attitudes that facilitate the bests learning.  It was partly motivational and partly reflective.  If you are familiar with Carol Dweck's work and the research it created, then you have a feeling for the talk.&lt;/p&gt;
&lt;p&gt;After lunch we did a fun assignment that was to recreate the MoneyBall movie where we are trying to get a set of three players that in aggregate replace the 3 key players that were just lost.   &lt;/p&gt;
&lt;p&gt;The data we used is hosted (here)[http://www.seanlahman.com/baseball-archive/statistics/]&lt;/p&gt;
&lt;h2&gt;Money Ball&lt;/h2&gt;
&lt;p&gt;We first started by downloading the dataset and loading it into Pandas.  We started with the batting data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;batting&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/baseball-csvs/Batting.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SB&lt;/th&gt;
      &lt;th&gt;CS&lt;/th&gt;
      &lt;th&gt;BB&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SFN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BOS&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã— 24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We then loaded the salaryd data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;data/baseball-csvs/Salaries.csv&amp;#39;)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;murraed02&lt;/td&gt;
      &lt;td&gt;1472819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lynnfr01&lt;/td&gt;
      &lt;td&gt;1090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;ripkeca01&lt;/td&gt;
      &lt;td&gt;800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lacyle01&lt;/td&gt;
      &lt;td&gt;725000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;flanami01&lt;/td&gt;
      &lt;td&gt;641667&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary[salary.yearID==2001].salary.describe()




count         860.000000
mean      2279841.061628
std       2907710.250521
min        200000.000000
25%        269375.000000
50%        925000.000000
75%       3250000.000000
max      22000000.000000
Name: salary, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the year 2001, the year we are concerned with, the minimum salary was $200,000.&lt;/p&gt;
&lt;p&gt;The next thing we did was merged the two dataframes and limited the data to 2001.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf = batting.merge(salary,on=[&amp;#39;playerID&amp;#39;,&amp;#39;yearID&amp;#39;],how=&amp;#39;left&amp;#39;)
mergeddf = mergeddf[mergeddf.yearID==2001]
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã— 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can see some of the salaries are missing.  There are players that can be aquired, but are not on a payroll.   If we pick them up we have to pay them 200,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.salary = mergeddf.salary.fillna(200000)
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã— 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now we need to make some new variables.   The number of times they were on First Base,the Batting Average (BA), the On Base Percentage (OBP), and the Slugg (SLG)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf[&amp;#39;BA&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf.BA.describe()




count    1044.000000
mean        0.202532
std         0.140697
min         0.000000
25%         0.117647
50%         0.235227
75%         0.274705
max         1.000000
Name: BA, dtype: float64




mergeddf[&amp;#39;1B&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]-mergeddf[&amp;#39;2B&amp;#39;]-mergeddf[&amp;#39;3B&amp;#39;]-mergeddf[&amp;#39;HR&amp;#39;]
mergeddf[&amp;#39;1B&amp;#39;].describe()




count    1237.000000
mean       23.185125
std        34.327716
min         0.000000
25%         0.000000
50%         4.000000
75%        33.000000
max       192.000000
Name: 1B, dtype: float64




mergeddf[&amp;#39;SLG&amp;#39;]=(mergeddf[&amp;#39;1B&amp;#39;]+2*mergeddf[&amp;#39;2B&amp;#39;]+3*mergeddf[&amp;#39;3B&amp;#39;] \
                 +4*mergeddf[&amp;#39;HR&amp;#39;])/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf[&amp;#39;SLG&amp;#39;].describe()




count    1044.000000
mean        0.303628
std         0.214569
min         0.000000
25%         0.142857
50%         0.337722
75%         0.436874
max         2.000000
Name: SLG, dtype: float64




mergeddf[&amp;#39;OBP&amp;#39;]=(mergeddf[&amp;#39;H&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]) \
/(mergeddf[&amp;#39;AB&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]+mergeddf[&amp;#39;SF&amp;#39;])
mergeddf[&amp;#39;OBP&amp;#39;].describe()




count    1047.000000
mean        0.254084
std         0.159932
min         0.000000
25%         0.162162
50%         0.293103
75%         0.338235
max         1.000000
Name: OBP, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The A's lost Jason Giambi (&lt;code&gt;giambja01&lt;/code&gt;), Johnny Damon (&lt;code&gt;damonjo01&lt;/code&gt;), Jason Isringhausen (&lt;code&gt;isrinja01&lt;/code&gt;), and Rainer Gustavo "Ray" Olmedo (&lt;code&gt;'saenzol01'&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These player need to replaced with similar player that bat, in total, as much as these guys, get on base as often as these guys, and can be payed less than these guys.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;my_mask = mergeddf[&amp;#39;playerID&amp;#39;].isin([&amp;#39;giambja01&amp;#39;,&amp;#39;damonjo01&amp;#39;,&amp;#39;isrinja01&amp;#39;,&amp;#39;saenzol01&amp;#39;])
lostboysdf = mergeddf[my_mask]
imp_var = [&amp;#39;playerID&amp;#39;, &amp;#39;teamID_x&amp;#39;,&amp;#39;AB&amp;#39;,&amp;#39;HR&amp;#39;, &amp;#39;OBP&amp;#39;, &amp;#39;SLG&amp;#39;, &amp;#39;salary&amp;#39;]
lostboysdf[imp_var]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;HR&lt;/th&gt;
      &lt;th&gt;OBP&lt;/th&gt;
      &lt;th&gt;SLG&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;7065&lt;/th&gt;
      &lt;td&gt;damonjo01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;644&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.323529&lt;/td&gt;
      &lt;td&gt;0.363354&lt;/td&gt;
      &lt;td&gt;7100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10836&lt;/th&gt;
      &lt;td&gt;giambja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;520&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0.476900&lt;/td&gt;
      &lt;td&gt;0.659615&lt;/td&gt;
      &lt;td&gt;4103333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14911&lt;/th&gt;
      &lt;td&gt;isrinja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27408&lt;/th&gt;
      &lt;td&gt;saenzol01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.291176&lt;/td&gt;
      &lt;td&gt;0.383607&lt;/td&gt;
      &lt;td&gt;290000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Avg OBP Needed to be replaced:&amp;quot;, 3*lostboysdf[imp_var].OBP.mean()
print &amp;quot;Total Bats needed to be replaced:&amp;quot;, lostboysdf[imp_var].AB.sum()

Avg OBP Needed to be replaced: 1.09160603138
Total Bats needed to be replaced: 1469.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We would ideally like to get every combination of 3 players that are available.  That would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;factorial&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;1335&lt;/span&gt;
&lt;span class="mi"&gt;395654395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is almost 400,000,000 combinations to search through.   Less make some reasonable assumptions about about the minimum At Bats and On Base Percentages&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;AB&amp;#39;,y=&amp;#39;OBP&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1063f3f10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/gw1d5_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the variance does not change much above 200 AB.  We probably want them to bat about 500 times a season, however.  We also want the average to be above 0.33 for the OPB, so that seems like a reasonable initial cutoff.  We also want there average salary to be less than 5000000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;size = len(mergeddf[(mergeddf.AB &amp;gt; 400) &amp;amp; (mergeddf.OBP &amp;gt; 0.33)])
nCr(size,3)




246905L
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us 11,480 combinations to search through.  Lets do it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#mdf = mergeddf[(mergeddf.yearID==2001)&amp;amp;(mergeddf.AB&amp;gt;50)] #.salary.describe()&lt;/span&gt;
&lt;span class="c"&gt;#mdf.salary = mdf.salary.fillna(200000)&lt;/span&gt;
&lt;span class="c"&gt;#mdf[imp_var].head()&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;
&lt;span class="n"&gt;good_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_AB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_OBP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;15000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0961&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;total_AB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;player1&lt;/th&gt;
      &lt;th&gt;player2&lt;/th&gt;
      &lt;th&gt;player3&lt;/th&gt;
      &lt;th&gt;total_AB&lt;/th&gt;
      &lt;th&gt;total_OBP&lt;/th&gt;
      &lt;th&gt;total_salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;28&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1.261767&lt;/td&gt;
      &lt;td&gt;5338333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;32&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1754&lt;/td&gt;
      &lt;td&gt;1.264850&lt;/td&gt;
      &lt;td&gt;5455000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;martied01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1637&lt;/td&gt;
      &lt;td&gt;1.256603&lt;/td&gt;
      &lt;td&gt;6005000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;edmonji01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;1.243410&lt;/td&gt;
      &lt;td&gt;6838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;38&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;olerujo01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1739&lt;/td&gt;
      &lt;td&gt;1.234375&lt;/td&gt;
      &lt;td&gt;7205000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gilesbr02&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1743&lt;/td&gt;
      &lt;td&gt;1.236756&lt;/td&gt;
      &lt;td&gt;7838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;alomaro01&lt;/td&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1742&lt;/td&gt;
      &lt;td&gt;1.247866&lt;/td&gt;
      &lt;td&gt;8255000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;43&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;thomeji01&lt;/td&gt;
      &lt;td&gt;1693&lt;/td&gt;
      &lt;td&gt;1.249345&lt;/td&gt;
      &lt;td&gt;8380000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;55&lt;/th&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1786&lt;/td&gt;
      &lt;td&gt;1.263189&lt;/td&gt;
      &lt;td&gt;9983333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;1773&lt;/td&gt;
      &lt;td&gt;1.290459&lt;/td&gt;
      &lt;td&gt;10088333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print len(gc)

66
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This allowed us to find 66 combination of players that would, in aggregate, have better statistics that the players that were lost.  It also turns out to be cheaper to do that.   This is the story of money ball.  It was a fun project.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-04/" rel="alternate"></link><updated>2015-06-04T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-04:galvanize/galvanize-data-science-01-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 4&lt;/h2&gt;
&lt;p&gt;The day started out with a mini-quiz on object-oriented programming, and that was followed by an introduction to git and sophisticated join queries.   Our instructor for the day used to work at Facebook, and she walked us through some the queries she would do on the job.  &lt;/p&gt;
&lt;p&gt;She then gave us a simulated data set that match the structure, but not the content, of Facebook tables and we had an individual sprint attempting to complete 10 queries in 2 hours.&lt;/p&gt;
&lt;p&gt;After lunch we had a lecture on pyscopg2, a python library to use to connect and interact with a PostgreSQL server.   We ran a server locally, loaded with the same data as the morning, and were given an assignment to construct a pipeline that we could run each day to give us an updated status of our users.   We were to check on results for today being set to Aug 14, 2014. &lt;/p&gt;
&lt;p&gt;Our resulting script is below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;psycopg2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;2014-08-14&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;%Y-%M-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;CREATE TABLE logins_7d_%s AS&lt;/span&gt;
&lt;span class="sd"&gt;    WITH&lt;/span&gt;
&lt;span class="sd"&gt;    main AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        r.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        tmstmp::date AS reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        CASE WHEN optout.userid IS NULL then 0 ELSE 1 END AS opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM registrations r&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN optout&lt;/span&gt;
&lt;span class="sd"&gt;    ON r.userid = optout.userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY r.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid,&lt;/span&gt;
&lt;span class="sd"&gt;        MAX(tmstmp::date) AS last_login&lt;/span&gt;
&lt;span class="sd"&gt;    FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7d&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7m AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT t.userid, COUNT(t.dt) AS logins_7m&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;mobile&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7w AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7w&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;web&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    uf1 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT * FROM friends)&lt;/span&gt;
&lt;span class="sd"&gt;    UNION ALL&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT userid2, userid1 FROM friends)),&lt;/span&gt;
&lt;span class="sd"&gt;    uf2 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT DISTINCT *&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf1),&lt;/span&gt;
&lt;span class="sd"&gt;    friend_cnt AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid1 AS userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(1) AS num_friends&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf2&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid)&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        main.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        last_login,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7d,0) AS logins_7d,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7m,0) AS logins_7d_mobile,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7w,0) AS logins_7d_web,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(num_friends,0) AS num_friends,&lt;/span&gt;
&lt;span class="sd"&gt;        opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM main&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7m&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7m.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7w&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7w.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN friend_cnt&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = friend_cnt.userid;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;timestamp&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;commit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also learned how pull data and load the data into a pandas dataframe.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sql&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sql&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;SELECT * FROM logins_7d_1389686880 LIMIT 20;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;userid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coerce_float&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reg_date&lt;/th&gt;
      &lt;th&gt;last_login&lt;/th&gt;
      &lt;th&gt;logins_7d&lt;/th&gt;
      &lt;th&gt;logins_7d_mobile&lt;/th&gt;
      &lt;th&gt;logins_7d_web&lt;/th&gt;
      &lt;th&gt;num_friends&lt;/th&gt;
      &lt;th&gt;opt_out&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;userid&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2014-06-23&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-12-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2014-04-18&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-12-17&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2014-08-09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2013-08-18&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2014-03-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2014-05-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2014-06-06&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2013-08-16&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2013-09-12&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2014-07-29&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2013-11-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2013-10-09&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2014-02-16&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;2014-04-20&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;2014-07-02&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;2014-05-10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Today was a very intense day.   But the programming is delivering on what it promised: Hands On Learning From Experience Professions!&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category><category term="psycopg2"></category></entry><entry><title>Galvanize - Week 01 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-03/" rel="alternate"></link><updated>2015-06-03T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-03:galvanize/galvanize-data-science-01-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 3&lt;/h2&gt;
&lt;p&gt;Today was an 'introduction' to SQL and PostgreSQL.  I put introduction in quotes because it does not properly describe what we did.  The pre-reading was to complete all 9 (1-9) tutorials on &lt;a href="http://sqlzoo.net/"&gt;SQLZoo&lt;/a&gt;.  This took me about 5 hours.   During lecture we have a review of the order of operation of SQL queries, as well as a detailed explanation of joins.    &lt;/p&gt;
&lt;p&gt;The sprint for the day involved install &lt;a href="http://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; locally, loading a database into it, then completing ~25 basic and 10 advance (extra credit) queries.   Our database had 3 tables with 300k, 500k, and 5k entries respectively.&lt;/p&gt;
&lt;p&gt;These were a great set of assignment because of how they 'leveled-up'.  Even the few among us that were sophisticated with SQL had difficulty with the advance problems.&lt;/p&gt;
&lt;p&gt;Now that I have completed ~10 hours of SQL queries today, I am going to end this post now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT * 
FROM bryan JOIN bed 
ON bryan.location=bed.location 
AND bryan.state=&amp;#39;sleep&amp;#39; 
AND bed.state=&amp;#39;comfy&amp;#39;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category></entry><entry><title>Galvanize - Week 01 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-02/" rel="alternate"></link><updated>2015-06-02T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-02:galvanize/galvanize-data-science-01-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 2&lt;/h2&gt;
&lt;p&gt;Today was he first 'regular' day in the program.  I showed up about 90 minutes before the mini-quiz to review the readings for the day's lecture on Object Oriented Programming (OOP).   At 9:30 we started the mini-quiz on SQL statements and results.  I found it rather simple.  We were given 30 minutes to complete it, and I finished in about 10 minutes.  Most the topics involved analogs in pandas that I am familiar with, so I think that's why I finished rather quickly.&lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;We had two lectures today.  The first lecture was on object oriented structures, and how to implement them in python.  The afternoon lecture was on scoping in python, and a little bit of debugging.  We were introduced to pdb, but told that the use of debuggers is not well integrated in the data science community.&lt;/p&gt;
&lt;h3&gt;LEGB&lt;/h3&gt;
&lt;p&gt;We were told the variables are looked for in the order of local, enclosing function, global, and python build-in.   I made a set of functions to try to illustrate it for myself.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;locally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;built&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Paired Programming Sprint&lt;/h2&gt;
&lt;p&gt;Today's project involved programming a text based game of black jack with a dealer and 1 number of players.  We also had the extra credit options adding n-players, AI/Bot players, double down, and split.  I am happy to report that we that my partner and I were able to complete the first three, but ran out of time before implementing split.&lt;/p&gt;
&lt;p&gt;We started off with pencil and paper using the noun,verb method of abstraction.   We settled on making a Deck, a Player, and Hand, and Game, and an AI.    Its clear at the end that we should have abstracted the game more, and given the Hand class more responsibilities to best implement the split method.&lt;/p&gt;
&lt;p&gt;After we finished we had our dealer hit until 17 or above, while our AI bot hit until soft 17 below.   We also had it implement a doubling bettering strategy.   In our sample, the AI agent one more often then not came out ahead from this setup.   It added a little credence to the ways dealer's seem to play in Las Vegas.&lt;/p&gt;
&lt;p&gt;The repo is currently private, because it could be a project for future cohorts.   I do not want to make a copy public, but will if they give permission.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry><entry><title>Galvanize - Week 01 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-01/" rel="alternate"></link><updated>2015-06-01T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-01:galvanize/galvanize-data-science-01-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 1&lt;/h2&gt;
&lt;p&gt;Today is my first day attending Galvanize's Immersive Data Science Program in San Francisco, CA.   The program is a 12 week program that is approximately 10 hours a day of learning and activities to reinforce and refine the learning.   I am very excited to be a part of this program.&lt;/p&gt;
&lt;h2&gt;My Background&lt;/h2&gt;
&lt;p&gt;I have a Ph.d in Theoretical High Energy Particle Physics and Cosmology, earned a Data Analysis Nano-degree from Udacity.com, and also am current working on a M.S. in Computer Science from GA Tech.    I have also spent the last 8 years teaching high school physics and robotics.   &lt;/p&gt;
&lt;p&gt;I will likely have some strength with math and theory, but I have no doubt that my programming will significantly improve over the next 12 weeks.   Everyone in the program is well educated and intelligent, and each one of them have strengths in some areas and room for improvements in others.  It seems to be a strength for this program.   No matter your weakness, there are students that have that as a strength.&lt;/p&gt;
&lt;h2&gt;Summary of the Day&lt;/h2&gt;
&lt;p&gt;The first half of the day is getting to know our instructors, hour cohort, and the amazingly nice galvanize complex.  It is a 5 story building filled with startup companies, work spaces, galvanize students, and other tech visitors.   I found this to be an impressive building.&lt;/p&gt;
&lt;p&gt;After about an hour of getting to know everyone, we were given a presentation.  That was followed by a tour of the galvanize building.  We were then given an assessment on the pre-course material that was given to us before we showed up.&lt;/p&gt;
&lt;p&gt;After lunch, we had an 90 minute lecture, then worked on a paired sprint assignment for about 3 hours.   This assignment involved...&lt;/p&gt;
&lt;p&gt;After we finished the sprint, we were invited to a Galvanize happy hour to socialize over beer and wine.   I feel very lucky to be apart of this program, and have been impressed with my cohort, the instructors, and Galvanize.  &lt;/p&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;The pre-course material required us to complete material on the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Numpy/Pandas&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Web Awareness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The initial assessment we were given was a 120 minute test on the first 5 topics.  It was an 'open book' test, but that does not mean it was easy.  A majority of my peers did not finish within the allotted time.   &lt;/p&gt;
&lt;p&gt;I am not going to post details on the test because I would hate to ruin the thrill of discovery for potential future students.&lt;/p&gt;
&lt;h2&gt;LUNCH!!!!&lt;/h2&gt;
&lt;p&gt;They provide us a lunch on the first day, but most days we have an 75 minute break for lunch.  There are kitchen, fridges, storage for us to use if we wish.   The lunch was nice, from a local Thai place.   &lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;The lecture, in my opinion, was a little redundant with the course material.  It seemed structure under the assumption that you didn't read or review the python pre-course materials.  I understand its important that everyone is on the same starting point, but I wish we got to jump in a little deeper.&lt;/p&gt;
&lt;p&gt;I did learn and see the importance of using generators when possible.   It save both memory and time.   &lt;/p&gt;
&lt;h2&gt;Paired programming&lt;/h2&gt;
&lt;p&gt;After the lecture we grouped up for a paired programming assignment.   We trade off roles of being the driver and the navigator in 20 to 30 minute rotations for a 3 hour block of programming.  We start of by forking the day's assignment from a Github repo and cloning it locally.  Today we then worked two projects.  The first project was completing a list of functions based on a description of the function, including inputs and outputs.  The second project was fixing inefficiently running code.&lt;/p&gt;
&lt;p&gt;A simple example is checking if a key is in a dictionary.   Before today I might have checked to see if it was in the keys() results, but we can see that for medium size dictionaries that it is almost 40x slower than just using in in the dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;randint&lt;/span&gt;

&lt;span class="n"&gt;cnt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;

&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.17&lt;/span&gt; &lt;span class="err"&gt;Âµ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="mi"&gt;10000000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;111&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We saw similar results for iter compared to iteritems, range to xrange, and izip to zip.   It was a useful assignment for the content and the practice of collaborating with someone else.&lt;/p&gt;
&lt;h2&gt;After reception&lt;/h2&gt;
&lt;p&gt;Galvanize SF now runs two cohorts 6 weeks apart.  We had a mixer with previous cohort, enjoying beer, wine, and conversation on the roof of the building.  &lt;/p&gt;
&lt;p&gt;After 11 hours at Galvanize, I decided it was time to head home.   Definitely looking forward to day 2. &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry></feed>