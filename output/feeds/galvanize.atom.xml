<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bryan Travis Smith, Ph.D</title><link href="http://www.bryantravissmith.com/" rel="alternate"></link><link href="http://www.bryantravissmith.com/feeds/galvanize.atom.xml" rel="self"></link><id>http://www.bryantravissmith.com/</id><updated>2015-06-10T10:30:00-07:00</updated><entry><title>Galvanize - Week 02 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-03/" rel="alternate"></link><updated>2015-06-10T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-10:galvanize/galvanize-data-science-02-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 3&lt;/h2&gt;
&lt;p&gt;Today we had a miniquiz on making a python package that could contain an arbitrary probability mass function as a dictionary.  It had to allow new values to be set, maintain a normalized pmf, and return probabilities for values in the dictionary, None otherwise.   &lt;/p&gt;
&lt;h2&gt;Morning&lt;/h2&gt;
&lt;p&gt;The morning lecture was on Hypthesis testing and multiple testing corrections.  We reviewed the languaged, phrasing, caveots, and particulars about the frame works.  Our sprint involved investigating some questions involving multiple testing and clickthru rates&lt;/p&gt;
&lt;h2&gt;Multiple Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A study attempted to measure the influence of patients' astrological signs on their risk for heart failure.
   12 groups of patients (1 group for each astrological sign) were reviewed and the incidence of heart failure in each group was recorded. For each of the 12 groups, the researchers performed a z-test comparing the incidence of heart failure in one group to the incidence among the patients of all the other groups (i.e. 12 tests). The group with the highest rate of heart failure was Pisces, which had a p-value of .026 when assessing the null hypothesis that it had the same heart failure rate as all the other groups. What is the the problem with concluding from this p-value that Pisces have a higher rate of heart failure at a significance level of 0.05? How might you correct this p-value?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We have 12 tests, with a 5% false positive rate.  Using the Bernoulli Distribution we can see the chance of getting x number of false positive.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;


&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Probability of False Positive Count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability Of False Positives For 12 Tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 false postiive with 12 tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 or More false positives with 12 test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;postiive&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="n"&gt;tests&lt;/span&gt; &lt;span class="mf"&gt;0.341280055366&lt;/span&gt;
&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;More&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;positives&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="kp"&gt;test&lt;/span&gt; &lt;span class="mf"&gt;0.459639912337&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_1_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is a 34% chance that one of the 12 tests will be a false positive value.  Instead, it would make more sense to chose a false positive rate such that the chance of having false positives out of 12 tests would be less than 0.05&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p = np.linspace(0,1,10000)
cdf = 1-sc.binom.pmf(0,12,p)
plt.plot(p,cdf,label=&amp;quot;CDF of P&amp;quot;,color=&amp;#39;indianred&amp;#39;)
plt.xlabel(&amp;#39;False Positive Rate For 12 Tests&amp;#39;)
plt.ylabel(&amp;#39;Prob of 1+ False Positives&amp;#39;)
plt.fill_between(p, cdf, where=cdf&amp;lt;0.05, interpolate=True, color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.show()
print &amp;quot;New Significant Level: &amp;quot;, p[cdf &amp;lt; 0.05].max()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_3_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;New Significant Level:  0.004200420042
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;In this case we would want to have a false positive rate for a single test of 0.0042.  By this criteria the p-value of 0.026 is not significant for the relation between Picese and heart failure. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Bonferonni correct suggests taking the significant goal of an individual test and divide it by the number of tests.  This gives a value of 0.000417.  This is very close to the above results&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Significance: &amp;quot;, 0.05/12

Significance:  0.00416666666667
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Click Through Rate&lt;/h2&gt;
&lt;p&gt;We will use hypothesis testing to analyze &lt;strong&gt;Click Through Rate (CTR)&lt;/strong&gt; on the New York Times website.
CTR is defined as the number of clicks the user make per impression that is made upon the user.
We are going to determine if there is statistically significant difference between the mean CTR for
the following groups:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Signed in users v.s. Not signed in users&lt;/li&gt;
&lt;li&gt;Male v.s. Female&lt;/li&gt;
&lt;li&gt;Each of 7 age groups against each other (7 choose 2 = 21 tests)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Because we are construction 23 hypothesis tests on this data set, we can use the Bonferroni Correction of dividing the 5% false error rate by 23 to get:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$\alpha_{23} = 0.00217 $$&lt;/p&gt;
&lt;p&gt;We can now load the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt = pd.read_csv(&amp;quot;../ab-testing/data/nyt1.csv&amp;quot;)
nyt.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 458441 entries, 0 to 458440
Data columns (total 5 columns):
Age            458441 non-null int64
Gender         458441 non-null int64
Impressions    458441 non-null int64
Clicks         458441 non-null int64
Signed_In      458441 non-null int64
dtypes: int64(5)
memory usage: 21.0 MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are not any null values in the data set, but we need to construct a click through rate for each user.   We will do this by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Removing users without Impressions&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dividing the Clicks by Impressions&lt;/p&gt;
&lt;p&gt;nyt = nyt[nyt.Impressions!=0]
nyt['CTR'] = np.nan
nyt.CTR = nyt.Clicks.astype(float)/nyt2.Impressions.astype(float)
nyt.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Impressions&lt;/th&gt;
      &lt;th&gt;Clicks&lt;/th&gt;
      &lt;th&gt;Signed_In&lt;/th&gt;
      &lt;th&gt;CTR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have the click through lets look at the distributions of variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt2.hist(figsize=(15,8),bins=30,color=&amp;#39;indianred&amp;#39;,alpha=0.5)




array([[&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088e9f10&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088d9a50&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x108944150&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x104598750&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x10882f990&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088cafd0&amp;gt;]], dtype=object)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_11_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The age's with zero appear to be users that are not signed in by the height of the bars in the age and signed_in graphs.   The number of total clicks is less then 50,000, and the click through rates are small are mostly zero. &lt;/p&gt;
&lt;p&gt;To answer question 1, we need to split the date between signed-in users and signed-out users.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==1]
nosign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==0]
plt.figure()
sign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
nosign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed32690&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_13_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x108bbc950&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_13_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can construct a hypothesis test on the data where we have the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H0: The mean click through rate between signed in users and signed-out users are the same&lt;/li&gt;
&lt;li&gt;HA: The mean click through rates between signed-in users and signed-out users aer different&lt;/li&gt;
&lt;li&gt;We requrire a p-value less than 0.00217 to rejec the Null Hypothesis in favor for the Alterative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do this with a Weltch's (Non Equal Variance Assigned) t-test results in the following results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign.CTR, nosign.CTR, equal_var=False)




(array(-55.37611793427461), 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The results of the test states that the test statistic to measure the difference is -55, given a p-value very close to zero, which is less than 0.00217.   In this case we can see there is a mean difference in the click through rates between the two populations.  In this case the signed-out users have a higher CTR than signed-in users.&lt;/p&gt;
&lt;p&gt;Question two has to do with the difference in click through rates between genders.   This requires that we only investigate the users that are signed in, because signed out users do not have a gender variable.   Lets look at the data and perform a Wetch's t-test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
sign[sign.Gender==0].hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
sign[sign.Gender==1].hist(figsize=(15,8),bins=10,color=&amp;#39;blue&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed3a550&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_17_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x109a90610&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_17_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;A visual inspect of the data shows that men and women have similar distirubtions, with the exceptions fo the impressions distributions.  Men seem to have a wider range of impressons compared to women.  &lt;/p&gt;
&lt;p&gt;The hypthesis test we are constructing is:&lt;/p&gt;
&lt;p&gt;H0:  Signed-in Men and Women have the same mean CTR&lt;br /&gt;
HA:  Signed-in Men and Women have different mean CTR&lt;br /&gt;
&lt;em&gt;significance is 0.00217&lt;/em&gt;  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign[sign.Gender==0].CTR, sign[sign.Gender==1].CTR, equal_var=False)




(array(3.2897560659373846), 0.0010028527313066396)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our test possted a t-value of 3.29 and a p-value of 0.001, which is significant.  We reject the null in favor of the alternative, concluding that men and women have different mean click through rates.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Click through rates (SI Men, SI Women, NOT Si)&amp;quot;,sign[sign.Gender==1].CTR.mean(),sign[sign.Gender==1].CTR.mean(),nosign.CTR.mean()

Click through rates (SI Men, SI Women, NOT Si) 0.0139185242976 0.0139185242976 0.0283549070617
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The difference in the mean CTR between men and women is significant, but not large. The difference between both groups and not-signed-in users are much more different.&lt;/p&gt;
&lt;p&gt;Now we will construct a set of hypothesis tests comparing different age groups against each other.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign[&amp;#39;AgeGroup&amp;#39;] = np.nan
sign.loc[:,&amp;#39;AgeGroup&amp;#39;] = pd.cut(sign.Age, [0,7, 18, 24, 34, 44, 54, 64, 1000])
df = pd.DataFrame(columns=[&amp;#39;group1&amp;#39;,&amp;#39;group2&amp;#39;,&amp;#39;meanCTR1&amp;#39;,&amp;#39;meanCTR2&amp;#39;,&amp;#39;p_value&amp;#39;,&amp;#39;mean_difference&amp;#39;])
k = 0
AG = sign.AgeGroup.unique()
for i,x in enumerate(AG):
    for j,y in enumerate(AG):
        if x != &amp;#39;(0, 7]&amp;#39; and y!=&amp;#39;(0, 7]&amp;#39; and i &amp;gt; j:
            g1 = sign[sign.AgeGroup==x].CTR
            g2 = sign[sign.AgeGroup==y].CTR
            p = sc.ttest_ind(g1,g2, equal_var=False)[1]
            diff = g1.mean()-g2.mean()
            df.loc[k] = [x,y,g1.mean(),g2.mean(),p,diff]
            k += 1

df = df[df.p_value &amp;lt; 0.00217]
df = df.sort(&amp;#39;mean_difference&amp;#39;).reset_index().drop(&amp;#39;index&amp;#39;,axis=1)
df

/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  if __name__ == &amp;#39;__main__&amp;#39;:
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group1&lt;/th&gt;
      &lt;th&gt;group2&lt;/th&gt;
      &lt;th&gt;meanCTR1&lt;/th&gt;
      &lt;th&gt;meanCTR2&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
      &lt;th&gt;mean_difference&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;2.458627e-272&lt;/td&gt;
      &lt;td&gt;-0.020082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;1.430923e-295&lt;/td&gt;
      &lt;td&gt;-0.019845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;7.860398e-285&lt;/td&gt;
      &lt;td&gt;-0.019656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;6.900980e-144&lt;/td&gt;
      &lt;td&gt;-0.016865&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;9.214903e-56&lt;/td&gt;
      &lt;td&gt;-0.009496&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;8.273993e-20&lt;/td&gt;
      &lt;td&gt;-0.006278&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;3.563408e-05&lt;/td&gt;
      &lt;td&gt;-0.003218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;7.523228e-144&lt;/td&gt;
      &lt;td&gt;0.010020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;5.668132e-141&lt;/td&gt;
      &lt;td&gt;0.010160&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;2.525271e-151&lt;/td&gt;
      &lt;td&gt;0.010349&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;1.007813e-130&lt;/td&gt;
      &lt;td&gt;0.010586&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;4.575147e-146&lt;/td&gt;
      &lt;td&gt;0.016299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;7.449266e-146&lt;/td&gt;
      &lt;td&gt;0.016439&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;4.014382e-151&lt;/td&gt;
      &lt;td&gt;0.016628&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;5.245541e-288&lt;/td&gt;
      &lt;td&gt;0.019516&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We itereated through all 21 groups and found 14 pairs who's mean CTR are different from each other with a signicance less than 0.00217.   These results can be summarized in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;People older than 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 54 adn 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 7 and 18 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The order of CTR seems to be older than 65, 7-18 year olds, and then 54-64 year olds.  &lt;/p&gt;
&lt;p&gt;Its interest that the high click through rate of non-signed-in users matches those of the older groups.   &lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;In the afternoon we learned about AB testing, and our paired programming assignment had to do with analyzing the results of simulated data for &lt;a href="http://etsy.com"&gt;Etsy&lt;/a&gt;.  We were asked to analysis the data from a given Tuesday where an AB test was performed on a website between two pages.   The goal is to change the conversion rate from 10% to 10.1% with this new page, a lift of 1%.  We are told that the weekend users are different than weeday users, and most of Etsy's revenue is made on the weekend.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df = pd.read_csv(&amp;#39;../ab-testing/data/experiment.csv&amp;#39;)
df.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4040615247&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4365389205&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4256174578&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8122359922&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6077269891&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
print dfa.user_id.nunique(),dfa.user_id.count()
print dfb.user_id.nunique(),dfb.user_id.count()
print dfc.user_id.nunique(),dfc.user_id.count()
print dfd.user_id.nunique(),dfd.user_id.count()

95574 95574
90814 90815
0 0
4759 4759
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Right away we have a problem in the data.  We have 4759 users that are in treatment group that have seen the new page and old page.  There is also one person in the control group that appear's twice.  &lt;/p&gt;
&lt;p&gt;In my minds, a good experiment outlines how to handle missing data, mistakes, and errors in the analysis before they a found.  This is not the case for this assignment, so we will have to decided what to do ourselves.  I am going to exam the 4759 users.  First I will get a datetime in the dataframe, and then I will try to figure out what is happening with this group.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df[&amp;#39;dt&amp;#39;] = np.nan
df[&amp;#39;dt&amp;#39;] = pd.to_datetime(df.ts,unit=&amp;#39;s&amp;#39;)
dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)

dfd.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189992&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084275&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151793&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066970&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114751&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050318&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99066&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043251&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104055&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045528&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa[dfa.user_id.isin(dfd.user_id)].head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189991&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084274&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151790&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066969&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114746&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050317&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99064&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043250&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104052&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045527&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;In these 5 cases we see that the users saw the new landing page, then one second later saw the old landing page.  None of these users displayed converted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print dfd[dfd.converted==1].user_id.count()
dfd[dfd.converted==1].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100082&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043708&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67975&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029144&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42109&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017491&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22040&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008350&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87355&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037889&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:09&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].user_id.count()
dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100079&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043707&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67973&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029143&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42108&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017490&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22038&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008349&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87352&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037888&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We have 501 users that saw the new page, then 1 second later saw the old page and converted.  This does not make sense interms of our project/assignment, so I think the safest thing to do for this analysis is throw out these mistakes, and only do the analysis with the untainted results.  If this was a real experiment, I would definatley investigate the details of the test.&lt;/p&gt;
&lt;p&gt;The goal of the experiment is to have a new pages that has a conversion lift of 1 percent.   With that goal in mine we define the following test:&lt;/p&gt;
&lt;p&gt;H0:  The lift in conversions from the new page and old page is equal to 1%
HA:  the lift if conversions from the new page to the old page is less than 1%&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def z_test(old_conversion, new_conversion, old_nrow, new_nrow,
           effect_size=0., two_tailed=True, alpha=.05):
    &amp;quot;&amp;quot;&amp;quot;z-test&amp;quot;&amp;quot;&amp;quot;
    conversion = (old_conversion * old_nrow + new_conversion * new_nrow) / \
                 (old_nrow + new_nrow)

    se = np.sqrt(conversion * (1 - conversion) * (1 / old_nrow + 1 / new_nrow))

    z_score = (new_conversion - old_conversion - effect_size) / se

    if not two_tailed:
        p_val = 1 - sc.norm.cdf(abs(z_score))
    else:
        p_val = (1 - sc.norm.cdf(abs(z_score))) * 2

    reject_null = p_val &amp;lt; alpha
    #print &amp;#39;z-score: %s, p-value: %s, reject null: %s&amp;#39; % (z_score, p_val, reject_null)
    return z_score, p_val, reject_null



conA,cntA = dfa.converted.mean(),dfa.converted.count()
conB,cntB = dfb.converted.mean(),dfb.converted.count()
print conA,conB,cntA,cntB,0.01*conB
z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)

0.0996819218616 0.0996421296041 95574 90815 0.000996421296041





(-0.74648172622270292, 0.22768823318094589, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this frame there results are not significantly different from a 1% left that we can rule them out.  But we could have also tested if there is a difference between them.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;z_test(conA,conB,cntA,cntB,two_tailed=True,effect_size=0.0)




(-0.028666091979081442, 0.9771308999283459, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is also not signifcant difference between the groups either.  The results of our Tuesday experiment are really inconclusive.  Ultimately we are concerned with the effect on weekend users, because they are responsible for most of Etsy's revenue.   We were told this user base is different from the weekend users.  &lt;/p&gt;
&lt;p&gt;AirBNB had a talks (&lt;a href="http://nerds.airbnb.com/experiments-airbnb/"&gt;here&lt;/a&gt; and &lt;a href="http://nerds.airbnb.com/experiments-at-airbnb/"&gt;here&lt;/a&gt;) about looking at the hourly change in a p-vale, and examing if and when it level's off as the 'true' p-value for an experiment.   We are going to explore this method.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_values = []
effect = 0
last_effect=0
for i in range(23):

    # Grab the hour
    df_houra = dfa[dfa.dt.dt.hour&amp;lt;=i]
    df_hourb = dfb[dfb.dt.dt.hour&amp;lt;=i]

    conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
    conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

    p_values.append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)[1] )

plt.figure()
plt.plot(range(23),p_values,color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;From this method we see that the p-value is still changing, making me believe that our experiment could be under-powered.&lt;/p&gt;
&lt;p&gt;There is additional data about the country of each user.  It could be interesting to look that these results by country.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;countries = pd.read_csv(&amp;quot;../ab-testing/data/country.csv&amp;quot;)
countries.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;9160993935&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5879439034&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;8915383273&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2917824565&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3980216975&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries,on=&amp;#39;user_id&amp;#39;)
dfbc = dfb.merge(countries,on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
print dfbc.user_id.count(),dfbc.user_id.nunique()
dfac.head()

97151 92554
87927 87924
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23267&lt;/td&gt;
      &lt;td&gt;1357066015&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 18:46:55&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;79973&lt;/td&gt;
      &lt;td&gt;1357018111&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:28:31&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;338650&lt;/td&gt;
      &lt;td&gt;1357083484&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:38:04&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;340147&lt;/td&gt;
      &lt;td&gt;1357083599&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:39:59&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;382429&lt;/td&gt;
      &lt;td&gt;1357002072&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 01:01:12&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac.groupby(&amp;#39;user_id&amp;#39;).country.count().max()




2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that some users have two countries listed.  Since the user country data is not time sensitive, we have to drop them.  We do not know which country they were in at the type of the experiment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
dfbc = dfb.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfbc.user_id.count(),dfbc.user_id.nunique()

92554 92554
87925 87924




p_values = {&amp;#39;US&amp;#39;:[],&amp;#39;CA&amp;#39;:[],&amp;#39;UK&amp;#39;:[]}
effect = {&amp;#39;US&amp;#39;:0,&amp;#39;CA&amp;#39;:0,&amp;#39;UK&amp;#39;:0}
last_effect=0
for i in range(23):
    for country in dfac.country.unique():
    # Grab the hour
        df_houra = dfac[(dfac.dt.dt.hour&amp;lt;=i)&amp;amp;(dfac.country==country)]
        df_hourb = dfbc[(dfbc.dt.dt.hour&amp;lt;=i)&amp;amp;(dfbc.country==country)]

        conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
        conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

        p_values[country].append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*effect[country])[1] )
        effect[country] = conB

plt.figure()
plt.plot(range(23),p_values[&amp;#39;US&amp;#39;],color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;US&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;CA&amp;#39;],color=&amp;#39;blue&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;CA&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;UK&amp;#39;],color=&amp;#39;green&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;UK&amp;#39;)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we constructed a hypthoesis at the end of day on Tuesday, the US would look like they were responsive to the new page, but the time plot shows that this could be a cherry-picked value.  If the experiment was allowed to run for more time, or we collect more data, the value would have changed.   It is clear from these plots that the sample sizes are not large enough that a additional data does not heavily influence the result.   I would be hesitant to make any strong conclusions about the new page based on these results.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="statistics"></category><category term="hypthesis testing"></category></entry><entry><title>Galvanize - Week 02 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-02/" rel="alternate"></link><updated>2015-06-09T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-09:galvanize/galvanize-data-science-02-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 2&lt;/h2&gt;
&lt;p&gt;Today we started with a mini-quiz, had a lecture on sampling methods, were given a talk about searching for a job, and finished the day with lecture on estimations/bootstraping and a reinforcement paired programming section.&lt;/p&gt;
&lt;h2&gt;Mini-Quiz&lt;/h2&gt;
&lt;p&gt;The mini-quiz is interesting because it involved using pandas, which I have found to be great and flexible, while also mysterious.&lt;/p&gt;
&lt;p&gt;We were given a salary dataset and asked to make some changes to it and answer some questions.  The first was to read in the data and convert the names to a human readiable text and transform variables to the correct type.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../estimation-sampling/data/salary_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;info&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nc"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;core&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;Int64Index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;32159&lt;/span&gt;
&lt;span class="n"&gt;Data&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;          &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;job_title&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;department&lt;/span&gt;    &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;        &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;Join&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;



&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;job_title&lt;/th&gt;
      &lt;th&gt;department&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.columns = [&amp;#39;Name&amp;#39;,&amp;#39;Position Title&amp;#39;,&amp;#39;Department&amp;#39;,&amp;#39;Employee Annual Salary&amp;#39;,&amp;#39;Join Date&amp;#39;]
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This is how I traditionally have renamed columns.   I learned a new way that involed using pandas' 'rename' function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;../estimation-sampling/data/salary_data.csv&amp;#39;)
salary.rename(columns={&amp;#39;name&amp;#39;: &amp;#39;Name&amp;#39;,
                  &amp;#39;job_title&amp;#39;: &amp;#39;Position Title&amp;#39;,
                  &amp;#39;department&amp;#39;:&amp;#39;Department&amp;#39;,
                  &amp;#39;salary&amp;#39;:&amp;#39;Employee Annual Salary&amp;#39;,
                   &amp;#39;join_data&amp;#39;: &amp;#39;Join Date&amp;#39;},
                   inplace=True)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;I personally do not like these names for the columns because they involve spaces.   That removes the ability to us the pd.variable notation.   &lt;/p&gt;
&lt;p&gt;I have also found multiple ways to update a variable type.  I am still not sure if there is a better method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary[&amp;#39;Employee Annual Salary&amp;#39;] = salary[&amp;#39;Employee Annual Salary&amp;#39;].str.replace(&amp;quot;$&amp;quot;,&amp;quot;&amp;quot;).astype(float)
salary[&amp;#39;Join Date&amp;#39;] = pd.to_datetime(salary[&amp;#39;Join Date&amp;#39;])
salary.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 32160 entries, 0 to 32159
Data columns (total 5 columns):
Name                      32160 non-null object
Position Title            32160 non-null object
Department                32160 non-null object
Employee Annual Salary    32160 non-null float64
Join Date                 32160 non-null datetime64[ns]
dtypes: datetime64[ns](1), float64(1), object(3)
memory usage: 1.5+ MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have the data in the correct format, we can now answer questions about the dataset.  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the top 5 paying job titles?&lt;/li&gt;
&lt;li&gt;How many people have "Police" in their title?&lt;/li&gt;
&lt;li&gt;What fraction of the people in 2 are a 'Police Officer'&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How many people were hired from July 30, 2000 to Aug 08, 2000&lt;/p&gt;
&lt;p&gt;salary.groupby('Position Title')['Employee Annual Salary'].mean().order(ascending=False).head()&lt;/p&gt;
&lt;p&gt;Position Title
SUPERINTENDENT OF POLICE          260004
MAYOR                             216210
FIRE COMMISSIONER                 202728
FIRST DEPUTY SUPERINTENDENT       188316
FIRST DEPUTY FIRE COMMISSIONER    188316
Name: Employee Annual Salary, dtype: float64&lt;/p&gt;
&lt;p&gt;print "Contains 'POLICE': ", salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].count()
salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].value_counts(normalize=True)&lt;/p&gt;
&lt;p&gt;Contains 'POLICE':  11141&lt;/p&gt;
&lt;p&gt;POLICE OFFICER                                      0.847051
POLICE OFFICER (ASSIGNED AS DETECTIVE)              0.076025
POLICE COMMUNICATIONS OPERATOR II                   0.019747
POLICE COMMUNICATIONS OPERATOR I                    0.012925
POLICE OFFICER / FLD TRNG OFFICER                   0.010232
POLICE OFFICER (ASSIGNED AS EVIDENCE TECHNICIAN)    0.006463
POLICE OFFICER/EXPLSV DETECT K9 HNDLR               0.003590
POLICE OFFICER (ASGND AS MARINE OFFICER)            0.002783
POLICE CADET                                        0.002603
ELECTRICAL MECHANIC-AUTO-POLICE MTR MNT             0.002423
MACHINIST (AUTO) POLICE MOTOR MAINT                 0.002244
POLICE OFFICER (ASSIGNED AS CANINE HANDLER)         0.001885
POLICE OFFICER (ASSIGNED AS TRAFFIC SPECIALIST)     0.001795
SUPERVISING POLICE COMMUNICATIONS OPERATOR          0.001616
POLICE OFFICER (ASGND AS MOUNTED PATROL OFFICER)    0.001346
POLICE OFFICER (ASSIGNED AS SECURITY SPECIALIST)    0.001346
POLICE AGENT                                        0.001167
POLICE FORENSIC INVESTIGATOR I                      0.001077
POLICE OFFICER(ASGND AS LATENT PRINT EX)            0.000987
POLICE OFFICER (PER ARBITRATION AWARD)              0.000898
POLICE TECHNICIAN                                   0.000539
POLICE LEGAL OFFICER II                             0.000359
POLICE LEGAL OFFICER I                              0.000269
DIR OF POLICE RECORDS                               0.000090
MANAGER OF POLICE PAYROLLS                          0.000090
POLICE OFFICER(ASGND AS SUPVG LATENT PRINT EX)      0.000090
EXECUTIVE DIR - POLICE BOARD                        0.000090
SUPERINTENDENT OF POLICE                            0.000090
ASST SUPVSR OF POLICE RECORDS                       0.000090
MANAGER OF POLICE PERSONNEL                         0.000090
dtype: float64&lt;/p&gt;
&lt;p&gt;salary.set_index('Join Date',inplace=True)
salary.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-09-27&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;87228&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-08-04&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-20&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-04-27&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;80916&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-02-11&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;99648&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.ix[&amp;#39;2000-07-13&amp;#39; : &amp;#39;2000-08-13&amp;#39;].count()




Name                      2866
Position Title            2866
Department                2866
Employee Annual Salary    2866
dtype: int64
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Morning Sprint&lt;/h2&gt;
&lt;p&gt;The individual morning sprint covered sampling and estimation.   We were given a dataset on rain fall and attempted to use &lt;a href="http://en.wikipedia.org/wiki/Method_of_moments_%28statistics%29"&gt;Method of Moments&lt;/a&gt; estimates on the data to approximate the distributions.  We then followed up by looking at &lt;a href="http://en.wikipedia.org/wiki/Maximum_likelihood"&gt;Maximum Likelihood Estimates&lt;/a&gt; of the parameters.  &lt;/p&gt;
&lt;p&gt;I first looked at the data for January rainfall over the course of several years.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = pd.read_csv(&amp;quot;../estimation-sampling/data/rainfall.csv&amp;quot;)
print data.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 140 entries, 0 to 139
Data columns (total 13 columns):
Year    140 non-null int64
Jan     140 non-null float64
Feb     140 non-null float64
Mar     140 non-null float64
Apr     140 non-null float64
May     140 non-null float64
Jun     140 non-null float64
Jul     140 non-null float64
Aug     140 non-null float64
Sep     140 non-null float64
Oct     140 non-null float64
Nov     140 non-null float64
Dec     140 non-null float64
dtypes: float64(12), int64(1)
memory usage: 15.3 KB
None



data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Jan&lt;/th&gt;
      &lt;th&gt;Feb&lt;/th&gt;
      &lt;th&gt;Mar&lt;/th&gt;
      &lt;th&gt;Apr&lt;/th&gt;
      &lt;th&gt;May&lt;/th&gt;
      &lt;th&gt;Jun&lt;/th&gt;
      &lt;th&gt;Jul&lt;/th&gt;
      &lt;th&gt;Aug&lt;/th&gt;
      &lt;th&gt;Sep&lt;/th&gt;
      &lt;th&gt;Oct&lt;/th&gt;
      &lt;th&gt;Nov&lt;/th&gt;
      &lt;th&gt;Dec&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1871&lt;/td&gt;
      &lt;td&gt;2.76&lt;/td&gt;
      &lt;td&gt;4.58&lt;/td&gt;
      &lt;td&gt;5.01&lt;/td&gt;
      &lt;td&gt;4.13&lt;/td&gt;
      &lt;td&gt;3.30&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;0.95&lt;/td&gt;
      &lt;td&gt;1.31&lt;/td&gt;
      &lt;td&gt;2.13&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1872&lt;/td&gt;
      &lt;td&gt;2.32&lt;/td&gt;
      &lt;td&gt;2.11&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;5.91&lt;/td&gt;
      &lt;td&gt;3.09&lt;/td&gt;
      &lt;td&gt;5.17&lt;/td&gt;
      &lt;td&gt;6.10&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;4.50&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.25&lt;/td&gt;
      &lt;td&gt;2.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1873&lt;/td&gt;
      &lt;td&gt;2.96&lt;/td&gt;
      &lt;td&gt;7.14&lt;/td&gt;
      &lt;td&gt;4.11&lt;/td&gt;
      &lt;td&gt;3.59&lt;/td&gt;
      &lt;td&gt;6.31&lt;/td&gt;
      &lt;td&gt;4.20&lt;/td&gt;
      &lt;td&gt;4.63&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
      &lt;td&gt;4.28&lt;/td&gt;
      &lt;td&gt;4.36&lt;/td&gt;
      &lt;td&gt;5.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1874&lt;/td&gt;
      &lt;td&gt;5.22&lt;/td&gt;
      &lt;td&gt;9.23&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;11.84&lt;/td&gt;
      &lt;td&gt;1.49&lt;/td&gt;
      &lt;td&gt;2.87&lt;/td&gt;
      &lt;td&gt;2.65&lt;/td&gt;
      &lt;td&gt;3.52&lt;/td&gt;
      &lt;td&gt;3.12&lt;/td&gt;
      &lt;td&gt;2.63&lt;/td&gt;
      &lt;td&gt;6.12&lt;/td&gt;
      &lt;td&gt;4.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1875&lt;/td&gt;
      &lt;td&gt;6.15&lt;/td&gt;
      &lt;td&gt;3.06&lt;/td&gt;
      &lt;td&gt;8.14&lt;/td&gt;
      &lt;td&gt;4.22&lt;/td&gt;
      &lt;td&gt;1.73&lt;/td&gt;
      &lt;td&gt;5.63&lt;/td&gt;
      &lt;td&gt;8.12&lt;/td&gt;
      &lt;td&gt;1.60&lt;/td&gt;
      &lt;td&gt;3.79&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
      &lt;td&gt;5.46&lt;/td&gt;
      &lt;td&gt;4.30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_16_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;To me this looks like it could be well fitted by a &lt;a href="http://en.wikipedia.org/wiki/Poisson_distribution"&gt;Poisson distribution&lt;/a&gt; or a &lt;a href="http://en.wikipedia.org/wiki/Gamma_distribution"&gt;Gamma distribution&lt;/a&gt;.   A poisson distribution models random events occuring in a fixed time interval, and is a discreate distribution.  Gamma distributions are a continuous distribution that model how long one must way for N events to happen.   That seems like a better framework to think about rain fall.&lt;/p&gt;
&lt;p&gt;The mean and variance for a Poisson distribution is the lambda parameter:&lt;/p&gt;
&lt;p&gt;$$\mu = \lambda$$&lt;/p&gt;
&lt;p&gt;So we will use this to see the fit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;

&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Jan Rain&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Poisson Fit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Fall In Janary For All Years&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mf"&gt;4.54457142857&lt;/span&gt; &lt;span class="mf"&gt;6.91677463515&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_18_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This gives a fair fit to the distribution for january.   I want to compare this with the gama distribution.&lt;/p&gt;
&lt;p&gt;The gamma function is given by:&lt;/p&gt;
&lt;p&gt;$$X = Gamma(\alpha \ ,\beta) = \frac{\beta^\alpha \ x^{\alpha-1} \ e^{-\beta x}}{\Gamma(\alpha)}$$&lt;/p&gt;
&lt;p&gt;The mean and variance of the gamma distribution is given by&lt;/p&gt;
&lt;p&gt;$$\mu = \frac{\alpha}{\beta}$$&lt;/p&gt;
&lt;p&gt;$$\sigma^2 = \frac{\alpha}{\beta^2}$$&lt;/p&gt;
&lt;p&gt;So the estimate of alpha and beta are given by:&lt;/p&gt;
&lt;p&gt;$$\beta = \frac{\mu}{\sigma^2}$$&lt;/p&gt;
&lt;p&gt;$$\alpha = \frac{\mu^2}{\sigma^2}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = mean/var
alpha = mean**2/var
print alpha,beta
x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1/beta)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x,y,color=&amp;#39;red&amp;#39;,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Poisson Fit&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_20_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Gamma distribution fit matches the distribution's peak and tail better than the Poisson distribution fit.  There are method's to test the relative fit but I saved that for another day.&lt;/p&gt;
&lt;p&gt;Now lets look at the Gamma fits for all months.  The reason we bin by months is that we have the prior that rain and weather is season.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;f, axarr = plt.subplots(3, 4,figsize=(14, 8))

x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    y = sc.gamma.pdf(x,alpha,scale=1/beta)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;Gamma Fit&amp;quot;,color=&amp;#39;red&amp;#39;)
    label = &amp;#39;alpha = %.2f\nbeta = %.2f&amp;#39; % (alpha, beta)
    axarr[i/4,i%4].annotate(label, xy=(4, 0.25))

plt.tight_layout()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_22_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have a Method of Moments fit of the gamma distribution of rainfall for each month.   Now lets doo Maximum Likely Hood.  &lt;/p&gt;
&lt;p&gt;First we need to make a funciton.  In order to test the method I will try it on a poisson generated dataset.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def poisson_likelihood(x, lam):
    return sc.poisson.pmf(x,lam)

##produces the probabilty of of each lambda(lam) given a value of 6
plt.plot(range(1,20),[poisson_likelihood(6, lam) for lam in range(1,20)])
plt.xlabel(&amp;quot;Lambda Value&amp;quot;)
plt.ylabel(&amp;quot;Probability of Lambda Value Given x=6&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c5d0290&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This make sense because the maximum likelihood is 6, but there are still changes that the value is different.  Lets run this on the data now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_data = pd.read_csv(&amp;#39;../estimation-sampling/data/poisson.txt&amp;#39;,header=None)
p_data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;x_values = np.linspace(1,20,1000)
likelihoods = np.array([np.log(poisson_likelihood(p_data.values[:,0],i)).sum() for i in x_values])
plt.plot(x_values,likelihoods)
plt.ylabel(&amp;quot;Log Likelyhood For Lambda Fro Data&amp;quot;)
plt.xlabel(&amp;quot;Lambda Values&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c67f090&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_27_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We want to compare the maximum likelihood (argmax) of this distribution to the mean fo the data since the mean of Poisson distribution should be the lambda parameter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x_values[likelihoods.argmax()],p_data.mean()




(5.0510510510510516, 0    5.0437
 dtype: float64)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The maximum likelihood estimate of the lambda parameter is very close to the sample mean, which also matches the value of lambda used to generated the data ($$\lambda = 5$$).&lt;/p&gt;
&lt;p&gt;Scipy Stats has a fit function for each of the distributions that uses the maximum likelihood method.   I want to compare the plots the difference between the fits of the Method of Moments and the Maximum Likelihood.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mean = data.Jan.mean()
var = data.Jan.var()
beta = mean/var
alpha = mean**2/var

x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1./beta)
alpha_MLE,loc_MFL,one_over_beta_MLE = sc.gamma.fit(data.Jan,floc=0) #Loc = 0 - no rainfall minimum
y2 = sc.gamma.pdf(x1,alpha_MLE,scale=one_over_beta_MLE)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MoM)&amp;#39;,linestyle=&amp;#39;--&amp;#39;)
plt.plot(x1,y2,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MLE)&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()
print alpha, alpha_MLE
print beta, beta_MLE

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_31_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;2.98594801173 3.25219914651
0.65703621533 1.39738411574
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the two distributions are similar, but slightly different.   The MLE is more skewed right, and the MLE fits are larger then the method of moments.  I just want to finish this section with a plot of all the months.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    print month, alpha, alpha_fit, beta, beta_fit
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()

Jan 2.98594801173 3.25219914651 0.65703621533 0.715622847528
Feb 3.0418721755 3.0803224672 0.740681272732 0.750043734186
Mar 4.67867768543 4.64576013378 0.946813252137 0.94015180285
Apr 4.28032831959 4.25175417646 1.01660157558 1.00981505905
May 3.53902182175 3.82055049055 0.815644176548 0.880528551612
Jun 2.97083704473 2.89974494499 0.765862938963 0.747535846757
Jul 3.98358361758 3.6314371778 1.02531889482 0.934681309897
Aug 3.02948804114 3.20185713476 0.907886646459 0.959542766645
Sep 2.29238948802 2.14489278382 0.678766821038 0.635093671449
Oct 2.46786112353 1.96116795936 0.945359556993 0.751261428601
Nov 3.69539855825 3.30306402837 1.00014653216 0.893962581139
Dec 3.23590721109 3.52396472416 0.77216125712 0.840898349041



&amp;lt;matplotlib.figure.Figure at 0x10c387e10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_33_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;For each moths the distributions are similar, but like january the alpha and beta values are different. &lt;/p&gt;
&lt;h2&gt;Kernal Density Estimates&lt;/h2&gt;
&lt;p&gt;The last topic we had was to use the non-parametric method for fitting a distributions using gaussian kernal density estimates.   The idea is that each data point is fit with a gausian for some unknown variance, and the variance is shared for each such gaussian.   The variance paramenter is adjusted until an 'optimal' fit is found.&lt;/p&gt;
&lt;p&gt;We can do this with an example by convoluting two gaussian data sets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data2 = [sc.norm.rvs(loc=0,scale=2) for x in range(500)]+[sc.norm.rvs(loc=4,scale=1) for x in range(400)]
plt.figure()
plt.hist(data,bins=30,color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_35_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fit = sc.gaussian_kde(data2)
plt.figure()
plt.hist(data,bins=30,normed=True,alpha=0.2)
x=np.linspace(-6,8,100)
plt.plot(x,fit(x),color=&amp;#39;blue&amp;#39;,label=&amp;#39;KDE Fit&amp;#39;)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The fit function can be used for a density estimate when we do not wish to model the data with a particlar model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    gfit = sc.gaussian_kde(data[month])
    yg = gfit(x)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y2,label=&amp;quot;KDE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;blue&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10c65e490&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_38_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In each case the non-KDE function seem tot fit the data better, but it could be used to for other estimates if we did not have a model.&lt;/p&gt;
&lt;h2&gt;Paired Programming&lt;/h2&gt;
&lt;p&gt;In the afternoon session we had to investigate the centeral limit theorem, produce confidence intervals, and attempt some bootstrapping estimates&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_draws(distribution, parameters, size):
    &amp;#39;&amp;#39;&amp;#39;
        returns distribrution or None if valid distribution is not selected
    &amp;#39;&amp;#39;&amp;#39;    
    dist = None

    if distribution.lower() == &amp;#39;binomial&amp;#39;:
        n, p = parameters[&amp;#39;n&amp;#39;], parameters[&amp;#39;p&amp;#39;]
        dist = sc.binom(n, p).rvs(size)

    elif distribution.lower() == &amp;#39;exponential&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.expon(scale = l).rvs(size)

    elif distribution.lower() == &amp;#39;poisson&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.poisson(mu=l).rvs(size)

    elif distribution.lower() == &amp;#39;gamma&amp;#39;:
        a, b = parameters[&amp;#39;alpha&amp;#39;],parameters[&amp;#39;beta&amp;#39;]
        dist = sc.gamma(a=a,scale=1./b).rvs(size)

    elif distribution.lower() == &amp;#39;normal&amp;#39;:
        mean, var = parameters[&amp;#39;mean&amp;#39;], parameters[&amp;#39;var&amp;#39;]
        dist = sc.norm(loc=mean, scale=var).rvs(size)

    elif distribution.lower() == &amp;#39;uniform&amp;#39;:
        low, high = parameters[&amp;#39;low&amp;#39;], parameters[&amp;#39;high&amp;#39;]
        dist = sc.uniform(loc=low, scale=(high-low)).rvs(size)

    return dist

def plot_means(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).mean())
    plt.figure()
    plt.title(&amp;quot;Centeral Limit Theorem: &amp;quot; + distribution + &amp;quot; for N = &amp;quot; + str(size))
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 10, 5000)
plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these distirubtions we see that for N = 10, espeicaly for the discrete distriubiton, that the sampling distirubiton of the mean is not normal.   If the underlying distribution is skewed, so is the sampling distribution.  If we look at means of samples of size 200, the central limit theorm holds and the sampling distribution is normal even if the underlying distribution is skewed or discrete.  &lt;/p&gt;
&lt;p&gt;The central limit theorm does not hold for all statistics.  We can look at the max, for instance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def plot_max(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).max())
    plt.figure()
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_max(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:5, &amp;#39;high&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_5.png" /&gt;&lt;/p&gt;
&lt;p&gt;These distributions are clearly not normally distributed, and the discrete distribution results remain discrete.&lt;/p&gt;
&lt;h3&gt;Population Inference and Confidence Interval&lt;/h3&gt;
&lt;p&gt;Our next section had to do with constructing confidence intervals on means for different situations.   We were given some lunch data, and attempted to construct the confidence interval for the mean lunch break.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lunch_hour = np.loadtxt(&amp;#39;../estimation-sampling/data/lunch_hour.txt&amp;#39;)
plt.figure()
plt.hist(lunch_hour)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_50_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;There are 25 data points in the data.   Even though this distrubtion is not normal, the sampling distribution of the mean should be approching a normal distribution.  &lt;/p&gt;
&lt;p&gt;The standard deviation of the sampling distirubiton is suppose to be well approximated by the standard error of the sample.&lt;/p&gt;
&lt;p&gt;$$s = \sqrt{ \frac{\Sigma_{i}(x_i \ - \ \bar{x})^2}{N-1} }$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = lunch_hour.std(ddof=1) / np.sqrt( len(lunch_hour) )
se, sc.sem(lunch_hour) ##scipy standard error comparison




(0.040925827625524797, 0.040925827625524797)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using this standard error we can attempt to construct the confidence interval on the population mean.  We choose Z=1.96 for a 95% confidence interval&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lm = lunch_hour.mean()
(ci_95_low, ci_95_hi) = (lm-1.96*se,lm+1.96*se)
(ci_95_low, ci_95_hi)




(2.1042853778539716, 2.264714622146029)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The 95% confidence interval interpretation is that 95% of the confidence intervals constructed through this method will contrain the population mean lunch hour.   If the sample size was smaller, the both the standard error and normal approximation would change in a way that would not allow this method to work.   &lt;/p&gt;
&lt;p&gt;For smaller sample sizes we would want to try another method.  Bootstrapping could be effective.&lt;/p&gt;
&lt;p&gt;Bootstrapping does not assume normality, or more any assumptions about the underlying distribution.   It is a non-parametric method of constructiong an confidence interval.   If the distribution is well approximateldy by some common distribution, the bootstrapped CI will overestimate the boundaries compared to this distribution.  &lt;/p&gt;
&lt;p&gt;We will try this for another data set involving productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;productivity = np.loadtxt(&amp;#39;../estimation-sampling/data/productivity.txt&amp;#39;)
productivity




array([-19.1, -15.2, -12.4, -15.4,  -8.7,  -6.7,  -5.9,  -3.5,  -3.1,
        -2.1,   4.2,   6.1,   7. ,   9.1,  10. ,  10.3,  13.2,  10.1,
        14.1,  14.4,  20.1,  26.3,  27.7,  22.2,  23.4])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because the sample size is large enough, we would expect the centeral limit to hold.  Lets see if the boot strapping gives similar values.  If we do not know the population variance, we should us the t-distribution.   We will also check that.  The two results should be close&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = sc.sem(productivity)
mean = productivity.mean()
ci95lo, ci95hi = mean-1.96*se, mean+1.96*se
print ci95lo, ci95hi

-0.330202770421 10.4182027704



ci95lo, ci95hi = mean+sc.t.ppf(0.025,24)*se, mean+sc.t.ppf(0.975,24)*se
print ci95lo, ci95hi

-0.615086412127 10.7030864121
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These intervals are very close.   Bootstrapping should give a similar result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt; &lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;np.random.randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

    &lt;span class="nx"&gt;statistic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;array&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;means&lt;/span&gt;
    &lt;span class="nx"&gt;low&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="nx"&gt;high&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;low&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;high&lt;/span&gt;

&lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.49619999999999997&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.265000000000001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is in line with the previous estimates.   Lets look at the histogram of bootstrapped means.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bootstrap_plot_means(sample, iterations=1000):
    samples = bootstrap(sample, iterations)
    means = np.apply_along_axis(np.mean, 1, samples)
    plt.figure()
    plt.hist(means,normed=True,color=&amp;#39;red&amp;#39;,alpha=0.1)
    plt.xlabel(&amp;#39;Mean Values&amp;#39;)
    plt.ylabel(&amp;#39;Probability Density&amp;#39;)
    plt.show()

bootstrap_plot_means(productivity)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Even though the results are not a statistically significant difference from zero, the results suggest that the population value is likely to be different from zero.   The uncertainty from the sample does not allow us to know that it is not zero, but we do know that it does not significantly harm productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping Correlation&lt;/h3&gt;
&lt;p&gt;We can bootstrap other variables.   We will try it for the correlation between LSAT and GPA from law data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;law_sample = np.loadtxt(&amp;#39;../estimation-sampling/data/law_sample.txt&amp;#39;)
plt.scatter(law_sample[:,0],law_sample[:,1])
plt.xlabel(&amp;quot;LSAT Score&amp;quot;)
plt.ylabel(&amp;quot;GPA&amp;quot;)
print sc.pearsonr(law_sample[:,0],law_sample[:,1])

(0.77637449128940705, 0.00066510201110281625)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_65_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = bootstrap(law_sample,B=10000)
corrs = np.array([sc.pearsonr(mat[:,0],mat[:,1])[0] for mat in data])
plt.figure()
plt.hist(corrs)
plt.show()
np.percentile(corrs,2.5),np.percentile(corrs,97.5)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_66_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.45069334540504685, 0.96239529249176581)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bootstrapping the correlation between the variables from the sampple finds that a 95% confidence interval estimates the population correlation of LSAT with GPA should be between 0.45 and 0.96.   Thankfully we have the full dataset from which this sample was pulled.   Lets compare.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;all_law = np.loadtxt(&amp;#39;../estimation-sampling/data/law_all.txt&amp;#39;)
sc.pearsonr(all_law[:,0],all_law[:,1])[0]




0.75999785550389798
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is smack in the middle of the confidence interval.   Pretty cool.   &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="bootstraping"></category><category term="statistics"></category><category term="confidence interval"></category></entry><entry><title>Galvanize - Week 02 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-01/" rel="alternate"></link><updated>2015-06-08T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-08:galvanize/galvanize-data-science-02-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 1&lt;/h2&gt;
&lt;p&gt;Since this is the first day of the week we started with an hour long assessment on what we did last week.  The assessment was straight forward, and very doable with a modest understanding of the previous material.   &lt;/p&gt;
&lt;p&gt;After the test, we started a lecture on probability, which we finished in the afternoon.  There was the individual sprint after the morning lecture, and a paired spring as the afternoon lecture&lt;/p&gt;
&lt;h2&gt;Conditional Probabilities&lt;/h2&gt;
&lt;p&gt;We started with some simple questions like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose two cards are drawn from a standard 52 card deck.  What's the probability that the first is a queen and the second is a king?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(K,Q\right) = P\left(K|Q\right)*P\left(Q\right) = \frac{4}{51} * \frac{4}{52} = \frac{16}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 16./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00603318250377&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that both cards are queens?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q|Q\right)*P\left(Q\right) = \frac{3}{51} * \frac{4}{52} = \frac{12}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 12./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00452488687783&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose that before the second card was drawn, the first was inserted back into the deck and the deck reshuffled. What's the probability that both cards are queens?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q\right)*P\left(Q\right) = \frac{4}{52} * \frac{4}{52} = \frac{16}{2705}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 16./2705

Answer:  0.00591497227357
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We had similar questions about tables of data:&lt;/p&gt;
&lt;p&gt;A Store Manager wants to understand how his customers use different payment methods, and suspects that the size of the purchase is a major deciding factor. He organizes the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="right"&gt;Cash&lt;/th&gt;
&lt;th align="right"&gt;Debit&lt;/th&gt;
&lt;th align="right"&gt;Credit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Under 20&lt;/td&gt;
&lt;td align="right"&gt;400&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20 - 50&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;1200&lt;/td&gt;
&lt;td align="right"&gt;800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Over 50&lt;/td&gt;
&lt;td align="right"&gt;100&lt;/td&gt;
&lt;td align="right"&gt;600&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer spent over $50, what's the probability that the customer used a credit card?&lt;/p&gt;
&lt;p&gt;$$P\left(C|S&amp;gt;$50\right) = \frac{1400}{100+600+1400}$$ &lt;/p&gt;
&lt;p&gt;print "Answer: ", 1400./(100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.666666666667&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer paid in cash, what's the probability that the customer spent less than $20?&lt;/p&gt;
&lt;p&gt;$$P\left(S&amp;lt;20|Cash\right) = \frac{400}{400+200+100}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+200+100)&lt;/p&gt;
&lt;p&gt;Answer:  0.571428571429&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that a customer spent under $20 using cash?&lt;/p&gt;
&lt;p&gt;$$P\left(S &amp;lt; $20,Cash\right) = \frac{400}{400+150+150+200+1200+800+100+600+1400}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+150+150+200+1200+800+100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.08&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also had a question about job offers - something near and dear to our hearts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A gSchool grad is looking for her first job!  Given that she is freaked out, her chances of not getting an offer are 70%.  Given that she isn't freaked out, her chances of not getting an offer are 30%.  Suppose that the probability that she's freaked out is 80%. What's the probability that she gets an offer?&lt;/p&gt;
&lt;p&gt;$$P\left(Offer|Freak Out\right) = 0.7$$  &lt;/p&gt;
&lt;p&gt;$$P\left(Offer|No Freak Oout\right) = 0.3$$&lt;/p&gt;
&lt;p&gt;$$P\left(Freak Out\right) = 0.8$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ P\left(A\right) = \Sigma_{B} P\left(A|B\right) $$&lt;/p&gt;
&lt;p&gt;$$ P\left(Offer\right) = P\left(Offer|Freak Out\right) * P\left(Freak Out\right) + P\left(Offer|No Freak Out\right) * P\left(No Freak Out\right)$$&lt;/p&gt;
&lt;p&gt;$$P\left(Offer\right) = 0.7 * 0.8 + 0.3 * 0.2$$ &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.7 * 0.8 + 0.3 * 0.2

Answer:  0.62
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also tackled the deep issue go heroin use at Google: &lt;/p&gt;
&lt;p&gt;*. Google decides to do random drug tests for heroin on their employees.
   They know that 3% of their population uses heroin. The drug test has the
   following accuracy: The test correctly identifies 95% of the
   heroin users (sensitivity) and 90% of the non-users (specificity).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test Results&lt;/th&gt;
&lt;th&gt;Uses heroin&lt;/th&gt;
&lt;th&gt;Doesn't use heroin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tests positive&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tests negative&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alice gets tested and the test comes back positive. What is the probability
   that she uses heroin?&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test)}$$&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test|Heroin) P(Heroin) + P(Positive Test|No Heroin) P(No Heroin)}$$ &lt;/p&gt;
&lt;p&gt;$$ = \frac{0.95 \ 0.03}{0.95 \ 0.03 + 0.1 \ 0.97}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.95*0.03/(0.95*0.03 + 0.1*0.97)

Answer:  0.227091633466
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally we had the manditory birthday problem:   &lt;/p&gt;
&lt;p&gt;*The Birthday Problem.  Suppose there are 23 people in a data science class, lined up in a single file line.&lt;br /&gt;
Let A_i be the probability that the i'th person doesn't have the same birthday as the j'th person for any j &amp;lt; i.&lt;br /&gt;
Use the chain rule from probability to calculate the probability that at least 2 people share the same birthday. &lt;/p&gt;
&lt;p&gt;$$P(1,2,3,...,23) = \mbox{Probability that 23 people do not have the same birthday}$$&lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2, \ 3, \ ..., \ 23) = P(1) \ P(2|1) \ P(3 \ | \ 2 \ , \ 1) \ ... \ P(23|22 \ , \ ... \ , \ 2, \ 1)$$&lt;/p&gt;
&lt;p&gt;Given that 2 people don't have the same birthday, there are 363 days that are not taken that a new person could have:&lt;/p&gt;
&lt;p&gt;$$P(3|2,1) = \frac{363}{365}$$&lt;/p&gt;
&lt;p&gt;Similarly, given that 3 people don't have the same birthday, then there are 362 days not occupied:&lt;/p&gt;
&lt;p&gt;$$P(4|3,2,1) = \frac{362}{365}$$&lt;/p&gt;
&lt;p&gt;Extending this to the problem we have the probability of no matching birthdays being &lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2,\ 3, \ ..., \ 23) = \frac{1 \ 364 \ 363 \ ... \ (365-23)}{365^{23}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bday(N=23):
    prob = 1
    for i in range(1,N+1):
        prob = prob*(365.0-i+1)/365.
    return prob

print bday()

0.492702765676
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The probability of having 2 or more matches is us the inverse of this&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1 - P(No Maches|23)$$&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1-0.4927$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, round(100-100*bday(),1)

Answer:  50.7
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Distributions&lt;/h2&gt;
&lt;p&gt;The afternoon paired programming assignment involved developing an intuition for and using various distributions.&lt;/p&gt;
&lt;h3&gt;Discrete:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model one instance of a success or failure trial (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binomial&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of successes out of a number of trials (n), each with probability of success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Poisson&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model the number of events occurring in a fixed interval&lt;/li&gt;
&lt;li&gt;Events occur at an average rate (lambda) independently of the last event&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Geometric&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence of Bernoulli trials until first success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Continuous:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uniform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any of the values in the interval of a to b are equally likely&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Commonly occurring distribution shaped like a bell curve&lt;/li&gt;
&lt;li&gt;Often comes up because of the Central Limit Theorem (to be discussed later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exponential&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model time between Poisson events&lt;/li&gt;
&lt;li&gt;Events occur continuously and independently&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of our questions involved being given examples and identify the distribution that describes it.&lt;/p&gt;
&lt;p&gt;Often we have to identify what distribution we should use to model a real-life
situation. This exercise is designed to equip you with the ability to do so.&lt;/p&gt;
&lt;p&gt;*. A typist makes on average 2 mistakes per page.  What is the probability of a particular page having no errors on it?&lt;/p&gt;
&lt;p&gt;$$X = Poisson(\lambda = 2 \frac{mistakes}{page})$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Mistakes on a Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of No Mistakes: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt; &lt;span class="n"&gt;Mistakes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Components are packed in boxes of 20. The probability of a component being
   defective is 0.1.  What is the probability of a box containing 2 defective components?&lt;/p&gt;
&lt;p&gt;$$ X = Binomial(p=0.1,k=2,n=20) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(20)
y = sc.binom.pmf(x,20,.1)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of Defective Components&amp;quot;)
plt.ylabel(&amp;quot;Probability&amp;quot;)
print &amp;quot;Prob of 2 Defects: &amp;quot;, sc.binom.pmf(2,20,.1)

Prob of 2 Defects:  0.285179807064
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_26_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Patrons arrive at a local bar at a mean rate of 30 per hour.  What is the probability that the bouncer has to wait more than 3 minutes to card the next patron?&lt;/p&gt;
&lt;p&gt;$$X = Exponential(\lambda = .5 \frac{cards}{minute})$$&lt;/p&gt;
&lt;p&gt;$$ P(t \ &amp;gt; \ 3min) = \int_{t=3}^{t=\infty} Exponential(\lambda = .5 \frac{cards}{minute}) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, sc.expon.cdf(1e10,scale=0.5)-sc.expon.cdf(3,scale=0.5)


 Anser:  0.00247875217667



x = np.linspace(0,5,1000)
y = sc.expon.pdf(x,scale = 0.5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.ylim([0,0.1])
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=3, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_29_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. A variable is normally distributed with a mean of 120 and a standard
   deviation of 5. One score is randomly sampled. What is the probability the score is above 127?&lt;/p&gt;
&lt;p&gt;$$Z = (127-120)/5 = 7/5 = 1.4$$&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;gt;1.4) = 1 - .91924 \ \mbox{(area to left)}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(100,140,1000)
y = sc.norm.pdf(x,loc=120,scale=5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Variable Value&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=127, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. You need to find a tall person, at least 6 feet tall, to help you reach
   a cookie jar. 8% of the population is 6 feet or taller.  If you wait on the sidewalk, how many people would you expect to have passed you by before you'd have a candidate to reach the jar?&lt;/p&gt;
&lt;p&gt;$$X = Geometric(p=0.08)$$&lt;/p&gt;
&lt;p&gt;$$average = \frac{1}{p} = \frac{1}{0.08} = 12.5$$&lt;/p&gt;
&lt;p&gt;We round up - 13th person is expected to be it - 12 people pass.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.pmf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106020790&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_33_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.cdf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Cumlative Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106451c50&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_34_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A harried passenger will be several minutes late for a scheduled 10 A.M.
   flight to NYC. Nevertheless, he might still make the flight, since boarding
   is always allowed until 10:10 A.M., and boarding is sometimes
   permitted up to 10:30 AM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assuming the extended boarding time is &lt;strong&gt;uniformly distributed&lt;/strong&gt; over the above
   limits, find the probability that the passenger will make his flight,
   assuming he arrives at the boarding gate at 10:25.&lt;/p&gt;
&lt;p&gt;$$X = Uniform(0,30)$$&lt;/p&gt;
&lt;p&gt;$$P( x &amp;gt; 25 ) = \int_{25}^{30} \frac{dx}{30}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 5./30

Answer:  0.166666666667



x = np.linspace(0,30,1000)
y = sc.uniform.pdf(x,loc=0,scale=30)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes Late&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=25, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_37_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Covariance and Joint Distribution&lt;/h2&gt;
&lt;p&gt;Suppose a university wants to look for factors that are correlated with the GPA of the students that they
are going to admit. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;admissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../probability/data/admissions.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;addmissions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;covariance&lt;/code&gt; function and compute the covariance matrix of the dataframe. Check your results 
   with &lt;code&gt;df.cov()&lt;/code&gt;. Make sure you understand what each of the numbers in the matrix represents&lt;/p&gt;
&lt;p&gt;def make_cov(df):
    N = len(df)
    cols = df.columns
    return [[(df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;from pprint import pprint
pprint (make_cov(addmissions))&lt;/p&gt;
&lt;p&gt;[[332910756.59847927, 4014.9337921708066, -1226.2147143883631],
 [4014.9337921708066, 0.087883196618951942, -0.028782641179958546],
 [-1226.2147143883631, -0.028782641179958546, 112]]&lt;/p&gt;
&lt;p&gt;addmissions.cov()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;3.329410e+08&lt;/td&gt;
      &lt;td&gt;4015.299085&lt;/td&gt;
      &lt;td&gt;-1226.326280&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;4.015299e+03&lt;/td&gt;
      &lt;td&gt;0.087891&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-1.226326e+03&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
      &lt;td&gt;112.977442&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;normalize&lt;/code&gt; function that would compute the correlation matrix from the covariance matrix.
   Check your results with &lt;code&gt;df.corr()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def make_corr(df):
    N = len(df)
    cols = df.columns
    return [[((df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2))/(df[x].std() * df[y].std()) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;pprint (make_corr(addmissions))&lt;/p&gt;
&lt;p&gt;[[0.99990902474526921, 0.74220186205662952, -0.0063224730309758576],
 [0.74220186205662952, 0.99990902474528243, -0.0091340229188836969],
 [-0.0063224730309758576, -0.0091340229188836969, 0.99134834685640671]]&lt;/p&gt;
&lt;p&gt;addmissions.corr()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;You should identify &lt;code&gt;family_income&lt;/code&gt; as being the most correlated with GPA. The university wants to make
   an effort to make sure people of all family income are being fairly represented in the admissions process.
   In order to achieve that, different GPA thresholds will be set according to family income. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The low, medium and high family income groups are &lt;code&gt;0 to 26832&lt;/code&gt;, &lt;code&gt;26833 to 37510&lt;/code&gt; and &lt;code&gt;37511 to 51112&lt;/code&gt; respectively. 
   Implement a function that would plot the distribution of GPA scores for each family income category. These are the 
   conditional probability distributions of &lt;code&gt;gpa&lt;/code&gt; given certain levels of &lt;code&gt;family_income&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_hist(df):
    low = df[df.family_income &amp;lt;= 26832]
    med = df[(df.family_income &amp;gt; 26832) &amp;amp; (df.family_income &amp;lt;= 37519)]
    high = df[(df.family_income &amp;gt; 37519) &amp;amp; (df.family_income &amp;lt;= 51112)]
    low.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;blue&amp;quot;,label=&amp;#39;Low Income&amp;#39;)
    med.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;green&amp;quot;,label=&amp;#39;Medium Income&amp;#39;)
    high.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;red&amp;quot;,label=&amp;#39;High Income&amp;#39;)
    plt.xlim([2.0, 4.0])
    plt.legend()
    plt.show()

make_hist(addmissions)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_47_0.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the university decides to accept students with GPA above the 90th percentile within the respective family 
   income categories, what are the GPA thresholds for each of the categories?&lt;/p&gt;
&lt;p&gt;low = addmissions[addmissions.family_income &amp;lt;= 26832]
med = addmissions[(addmissions.family_income &amp;gt; 26832) &amp;amp; (addmissions.family_income &amp;lt;= 37519)]
high = addmissions[(addmissions.family_income &amp;gt; 37519) &amp;amp; (addmissions.family_income &amp;lt;= 51112)]
print "Low 90th Percentile", low.gpa.quantile(.9)
print "Medium 90th Percentile", med.gpa.quantile(.9)
print "High 90th Percentile", high.gpa.quantile(.9)&lt;/p&gt;
&lt;p&gt;Low 90th Percentile 3.01
Medium 90th Percentile 3.26
High 90th Percentile 3.36&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pearson Correlation vs Spearman Correlation&lt;/h2&gt;
&lt;p&gt;The Pearson correlation evaluates the linear relationship between two continuous 
variables. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables
without assuming linearity of the variables. Spearman correlation is often more robust in capturing non-linear relationship
between variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In addition to the &lt;code&gt;family_income&lt;/code&gt; and &lt;code&gt;parent_avg_age&lt;/code&gt;, you are also given data about the number of hours the 
   students studied. Load the new data in from &lt;code&gt;data/admissions_with_study_hrs_and_sports.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;studydf = pd.read_csv('../probability/data/admissions_with_study_hrs_and_sports.csv')
studydf.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;family_income_cat&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;th&gt;hrs_studied&lt;/th&gt;
      &lt;th&gt;sport_performance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;49.463745&lt;/td&gt;
      &lt;td&gt;0.033196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;16.414467&lt;/td&gt;
      &lt;td&gt;0.000317&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;4.937079&lt;/td&gt;
      &lt;td&gt;0.021845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;160.210286&lt;/td&gt;
      &lt;td&gt;0.153819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;36.417860&lt;/td&gt;
      &lt;td&gt;0.010444&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make a scatter plot of the &lt;code&gt;gpa&lt;/code&gt; against &lt;code&gt;hrs_studied&lt;/code&gt;. Make the points more transperant so you can see the density
   of the points. Use the following command get the slope and intercept of a straight line to fit the data.&lt;/p&gt;
&lt;p&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.hrs_studied)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept&lt;/p&gt;
&lt;p&gt;studydf.plot(kind='scatter',x='gpa',y='hrs_studied',alpha=0.01)
plt.plot(x,y,color='red')
plt.xlabel("GPA")
plt.ylabel("Hours Studied")
plt.show()&lt;/p&gt;
&lt;p&gt;494.329335528 -1400.63719543 0.475940264662 0.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_53_1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the functions &lt;code&gt;scipy.stats.pearsonr&lt;/code&gt; and &lt;code&gt;scipy.stats.spearmanr&lt;/code&gt; to compute the Pearson and Spearman correlation&lt;/p&gt;
&lt;p&gt;print sc.pearsonr(studydf.gpa,studydf.hrs_studied)
print sc.spearmanr(studydf.gpa,studydf.hrs_studied)&lt;/p&gt;
&lt;p&gt;(0.47594026466220946, 0.0)
(0.98495916559333341, 0.0)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat step &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; for &lt;code&gt;gpa&lt;/code&gt; and &lt;code&gt;sport_performance&lt;/code&gt;. Is there a strong relationship between the two variables?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept

studydf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()

0.00979813693421 0.0585103217504 0.0238485969548 0.0124044928587
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_57_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print sc.pearsonr(studydf.gpa,studydf.sport_performance)
print sc.spearmanr(studydf.gpa,studydf.sport_performance)

(0.023848596954761905, 0.012404492858691094)
(0.0022881402736224248, 0.81043264616449484)



temp = studydf[studydf.gpa &amp;gt; 3.0]
slope, intercept, r_value, p_value, std_err = sc.linregress(temp.gpa,temp.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(temp.gpa.min(),temp.gpa.max(),100)
y = slope*x+intercept

temp.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()
print sc.pearsonr(temp.gpa,temp.sport_performance)
print sc.spearmanr(temp.gpa,temp.sport_performance)

0.65608660543 -2.03523616554 0.945140987506 0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_59_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.94514098750633013, 0.0)
(1.0, 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Part 4: Distribution Simulation&lt;/h2&gt;
&lt;p&gt;Often times in real life applications, we can specify the values of a variable to be of a particular distribution,
for example the number of sales made in the next month can be modeled as a uniform distribution over the range of
5000 and 6000.&lt;/p&gt;
&lt;p&gt;In this scenario, we are modeling &lt;code&gt;profit&lt;/code&gt; as a product of &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt;,
where &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt; can be modeled as probabilistic distributions.
By randomly drawing values from these distributions, we are able to get a distribution of the range of &lt;code&gt;profit&lt;/code&gt; 
based on the uncertainties in the other variables.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Profit = Number of views * Conversion * Profit per sale&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Assumptions:
- &lt;code&gt;Number of views&lt;/code&gt; is a uniform distribution over the range of 5000 and 6000
- &lt;code&gt;Conversion is a binomial distribution where the probability of success is&lt;/code&gt;0.12&lt;code&gt;for each sale among the&lt;/code&gt;Number on views made 
- &lt;code&gt;Profit per sale&lt;/code&gt; has &lt;code&gt;0.2&lt;/code&gt; probability of taking the value &lt;code&gt;50&lt;/code&gt; (for wholesale) and &lt;code&gt;0.8&lt;/code&gt; of 
  taking the value &lt;code&gt;60&lt;/code&gt; (non-wholesale)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given the distributions of each of variables, use scipy to write a function that would draw random values from each of the distributions to simulate a distribution for &lt;code&gt;profit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def get_profit():
    num_views = np.round(sc.uniform.rvs(loc=5000,scale=1000,size=1),0)
    conversions = sc.binom.rvs(num_views,0.12)
    wholesale = sc.binom.rvs(conversions,0.2)
    return wholesale&lt;em&gt;50+(conversions-wholesale)&lt;/em&gt;60&lt;/p&gt;
&lt;p&gt;profits = np.array([get_profit() for i in xrange(100000)])
plt.hist(profits)
plt.show()
print "Low: ",np.percentile(profits,2.5)
print "High: ",np.percentile(profits,97.5)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_61_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Low&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;33800.0&lt;/span&gt;
&lt;span class="n"&gt;High&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;42920.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-05/" rel="alternate"></link><updated>2015-06-05T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-05:galvanize/galvanize-data-science-01-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 5&lt;/h2&gt;
&lt;p&gt;This morning we started with a reflection about the week, and completed a survey about our progress and thoughts on the program.   I think it is a very strong, hands-on program so far.&lt;/p&gt;
&lt;p&gt;The morning lesson and sprint was on using (pandas)[http://pandas.pydata.org/].   We were given some hospital data in a CSV format, read in the data, and answered a number of questions about most common diseases, most expensive procedures, the post profitable hospitals, and various subsets of these question on different conditions.  It was a simple exercise that gave us practice making new variables, grouping, and subsetting to look massage the data into a form that allowed us to answer the questions.&lt;/p&gt;
&lt;p&gt;During our lunch we had a presentation on learning, and the approach and attitudes that facilitate the bests learning.  It was partly motivational and partly reflective.  If you are familiar with Carol Dweck's work and the research it created, then you have a feeling for the talk.&lt;/p&gt;
&lt;p&gt;After lunch we did a fun assignment that was to recreate the MoneyBall movie where we are trying to get a set of three players that in aggregate replace the 3 key players that were just lost.   &lt;/p&gt;
&lt;p&gt;The data we used is hosted (here)[http://www.seanlahman.com/baseball-archive/statistics/]&lt;/p&gt;
&lt;h2&gt;Money Ball&lt;/h2&gt;
&lt;p&gt;We first started by downloading the dataset and loading it into Pandas.  We started with the batting data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;batting&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/baseball-csvs/Batting.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SB&lt;/th&gt;
      &lt;th&gt;CS&lt;/th&gt;
      &lt;th&gt;BB&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SFN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BOS&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We then loaded the salaryd data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;data/baseball-csvs/Salaries.csv&amp;#39;)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;murraed02&lt;/td&gt;
      &lt;td&gt;1472819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lynnfr01&lt;/td&gt;
      &lt;td&gt;1090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;ripkeca01&lt;/td&gt;
      &lt;td&gt;800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lacyle01&lt;/td&gt;
      &lt;td&gt;725000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;flanami01&lt;/td&gt;
      &lt;td&gt;641667&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary[salary.yearID==2001].salary.describe()




count         860.000000
mean      2279841.061628
std       2907710.250521
min        200000.000000
25%        269375.000000
50%        925000.000000
75%       3250000.000000
max      22000000.000000
Name: salary, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the year 2001, the year we are concerned with, the minimum salary was $200,000.&lt;/p&gt;
&lt;p&gt;The next thing we did was merged the two dataframes and limited the data to 2001.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf = batting.merge(salary,on=[&amp;#39;playerID&amp;#39;,&amp;#39;yearID&amp;#39;],how=&amp;#39;left&amp;#39;)
mergeddf = mergeddf[mergeddf.yearID==2001]
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can see some of the salaries are missing.  There are players that can be aquired, but are not on a payroll.   If we pick them up we have to pay them 200,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.salary = mergeddf.salary.fillna(200000)
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now we need to make some new variables.   The number of times they were on First Base,the Batting Average (BA), the On Base Percentage (OBP), and the Slugg (SLG)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf[&amp;#39;BA&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf.BA.describe()




count    1044.000000
mean        0.202532
std         0.140697
min         0.000000
25%         0.117647
50%         0.235227
75%         0.274705
max         1.000000
Name: BA, dtype: float64




mergeddf[&amp;#39;1B&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]-mergeddf[&amp;#39;2B&amp;#39;]-mergeddf[&amp;#39;3B&amp;#39;]-mergeddf[&amp;#39;HR&amp;#39;]
mergeddf[&amp;#39;1B&amp;#39;].describe()




count    1237.000000
mean       23.185125
std        34.327716
min         0.000000
25%         0.000000
50%         4.000000
75%        33.000000
max       192.000000
Name: 1B, dtype: float64




mergeddf[&amp;#39;SLG&amp;#39;]=(mergeddf[&amp;#39;1B&amp;#39;]+2*mergeddf[&amp;#39;2B&amp;#39;]+3*mergeddf[&amp;#39;3B&amp;#39;] \
                 +4*mergeddf[&amp;#39;HR&amp;#39;])/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf[&amp;#39;SLG&amp;#39;].describe()




count    1044.000000
mean        0.303628
std         0.214569
min         0.000000
25%         0.142857
50%         0.337722
75%         0.436874
max         2.000000
Name: SLG, dtype: float64




mergeddf[&amp;#39;OBP&amp;#39;]=(mergeddf[&amp;#39;H&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]) \
/(mergeddf[&amp;#39;AB&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]+mergeddf[&amp;#39;SF&amp;#39;])
mergeddf[&amp;#39;OBP&amp;#39;].describe()




count    1047.000000
mean        0.254084
std         0.159932
min         0.000000
25%         0.162162
50%         0.293103
75%         0.338235
max         1.000000
Name: OBP, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The A's lost Jason Giambi (&lt;code&gt;giambja01&lt;/code&gt;), Johnny Damon (&lt;code&gt;damonjo01&lt;/code&gt;), Jason Isringhausen (&lt;code&gt;isrinja01&lt;/code&gt;), and Rainer Gustavo "Ray" Olmedo (&lt;code&gt;'saenzol01'&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These player need to replaced with similar player that bat, in total, as much as these guys, get on base as often as these guys, and can be payed less than these guys.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;my_mask = mergeddf[&amp;#39;playerID&amp;#39;].isin([&amp;#39;giambja01&amp;#39;,&amp;#39;damonjo01&amp;#39;,&amp;#39;isrinja01&amp;#39;,&amp;#39;saenzol01&amp;#39;])
lostboysdf = mergeddf[my_mask]
imp_var = [&amp;#39;playerID&amp;#39;, &amp;#39;teamID_x&amp;#39;,&amp;#39;AB&amp;#39;,&amp;#39;HR&amp;#39;, &amp;#39;OBP&amp;#39;, &amp;#39;SLG&amp;#39;, &amp;#39;salary&amp;#39;]
lostboysdf[imp_var]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;HR&lt;/th&gt;
      &lt;th&gt;OBP&lt;/th&gt;
      &lt;th&gt;SLG&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;7065&lt;/th&gt;
      &lt;td&gt;damonjo01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;644&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.323529&lt;/td&gt;
      &lt;td&gt;0.363354&lt;/td&gt;
      &lt;td&gt;7100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10836&lt;/th&gt;
      &lt;td&gt;giambja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;520&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0.476900&lt;/td&gt;
      &lt;td&gt;0.659615&lt;/td&gt;
      &lt;td&gt;4103333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14911&lt;/th&gt;
      &lt;td&gt;isrinja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27408&lt;/th&gt;
      &lt;td&gt;saenzol01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.291176&lt;/td&gt;
      &lt;td&gt;0.383607&lt;/td&gt;
      &lt;td&gt;290000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Avg OBP Needed to be replaced:&amp;quot;, 3*lostboysdf[imp_var].OBP.mean()
print &amp;quot;Total Bats needed to be replaced:&amp;quot;, lostboysdf[imp_var].AB.sum()

Avg OBP Needed to be replaced: 1.09160603138
Total Bats needed to be replaced: 1469.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We would ideally like to get every combination of 3 players that are available.  That would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;factorial&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;1335&lt;/span&gt;
&lt;span class="mi"&gt;395654395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is almost 400,000,000 combinations to search through.   Less make some reasonable assumptions about about the minimum At Bats and On Base Percentages&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;AB&amp;#39;,y=&amp;#39;OBP&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1063f3f10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/gw1d5_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the variance does not change much above 200 AB.  We probably want them to bat about 500 times a season, however.  We also want the average to be above 0.33 for the OPB, so that seems like a reasonable initial cutoff.  We also want there average salary to be less than 5000000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;size = len(mergeddf[(mergeddf.AB &amp;gt; 400) &amp;amp; (mergeddf.OBP &amp;gt; 0.33)])
nCr(size,3)




246905L
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us 11,480 combinations to search through.  Lets do it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#mdf = mergeddf[(mergeddf.yearID==2001)&amp;amp;(mergeddf.AB&amp;gt;50)] #.salary.describe()&lt;/span&gt;
&lt;span class="c"&gt;#mdf.salary = mdf.salary.fillna(200000)&lt;/span&gt;
&lt;span class="c"&gt;#mdf[imp_var].head()&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;
&lt;span class="n"&gt;good_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_AB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_OBP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;15000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0961&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;total_AB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;player1&lt;/th&gt;
      &lt;th&gt;player2&lt;/th&gt;
      &lt;th&gt;player3&lt;/th&gt;
      &lt;th&gt;total_AB&lt;/th&gt;
      &lt;th&gt;total_OBP&lt;/th&gt;
      &lt;th&gt;total_salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;28&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1.261767&lt;/td&gt;
      &lt;td&gt;5338333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;32&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1754&lt;/td&gt;
      &lt;td&gt;1.264850&lt;/td&gt;
      &lt;td&gt;5455000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;martied01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1637&lt;/td&gt;
      &lt;td&gt;1.256603&lt;/td&gt;
      &lt;td&gt;6005000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;edmonji01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;1.243410&lt;/td&gt;
      &lt;td&gt;6838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;38&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;olerujo01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1739&lt;/td&gt;
      &lt;td&gt;1.234375&lt;/td&gt;
      &lt;td&gt;7205000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gilesbr02&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1743&lt;/td&gt;
      &lt;td&gt;1.236756&lt;/td&gt;
      &lt;td&gt;7838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;alomaro01&lt;/td&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1742&lt;/td&gt;
      &lt;td&gt;1.247866&lt;/td&gt;
      &lt;td&gt;8255000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;43&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;thomeji01&lt;/td&gt;
      &lt;td&gt;1693&lt;/td&gt;
      &lt;td&gt;1.249345&lt;/td&gt;
      &lt;td&gt;8380000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;55&lt;/th&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1786&lt;/td&gt;
      &lt;td&gt;1.263189&lt;/td&gt;
      &lt;td&gt;9983333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;1773&lt;/td&gt;
      &lt;td&gt;1.290459&lt;/td&gt;
      &lt;td&gt;10088333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print len(gc)

66
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This allowed us to find 66 combination of players that would, in aggregate, have better statistics that the players that were lost.  It also turns out to be cheaper to do that.   This is the story of money ball.  It was a fun project.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-04/" rel="alternate"></link><updated>2015-06-04T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-04:galvanize/galvanize-data-science-01-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 4&lt;/h2&gt;
&lt;p&gt;The day started out with a mini-quiz on object-oriented programming, and that was followed by an introduction to git and sophisticated join queries.   Our instructor for the day used to work at Facebook, and she walked us through some the queries she would do on the job.  &lt;/p&gt;
&lt;p&gt;She then gave us a simulated data set that match the structure, but not the content, of Facebook tables and we had an individual sprint attempting to complete 10 queries in 2 hours.&lt;/p&gt;
&lt;p&gt;After lunch we had a lecture on pyscopg2, a python library to use to connect and interact with a PostgreSQL server.   We ran a server locally, loaded with the same data as the morning, and were given an assignment to construct a pipeline that we could run each day to give us an updated status of our users.   We were to check on results for today being set to Aug 14, 2014. &lt;/p&gt;
&lt;p&gt;Our resulting script is below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;psycopg2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;2014-08-14&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;%Y-%M-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;CREATE TABLE logins_7d_%s AS&lt;/span&gt;
&lt;span class="sd"&gt;    WITH&lt;/span&gt;
&lt;span class="sd"&gt;    main AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        r.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        tmstmp::date AS reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        CASE WHEN optout.userid IS NULL then 0 ELSE 1 END AS opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM registrations r&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN optout&lt;/span&gt;
&lt;span class="sd"&gt;    ON r.userid = optout.userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY r.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid,&lt;/span&gt;
&lt;span class="sd"&gt;        MAX(tmstmp::date) AS last_login&lt;/span&gt;
&lt;span class="sd"&gt;    FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7d&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7m AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT t.userid, COUNT(t.dt) AS logins_7m&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;mobile&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7w AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7w&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;web&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    uf1 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT * FROM friends)&lt;/span&gt;
&lt;span class="sd"&gt;    UNION ALL&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT userid2, userid1 FROM friends)),&lt;/span&gt;
&lt;span class="sd"&gt;    uf2 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT DISTINCT *&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf1),&lt;/span&gt;
&lt;span class="sd"&gt;    friend_cnt AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid1 AS userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(1) AS num_friends&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf2&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid)&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        main.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        last_login,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7d,0) AS logins_7d,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7m,0) AS logins_7d_mobile,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7w,0) AS logins_7d_web,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(num_friends,0) AS num_friends,&lt;/span&gt;
&lt;span class="sd"&gt;        opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM main&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7m&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7m.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7w&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7w.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN friend_cnt&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = friend_cnt.userid;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;timestamp&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;commit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also learned how pull data and load the data into a pandas dataframe.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sql&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sql&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;SELECT * FROM logins_7d_1389686880 LIMIT 20;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;userid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coerce_float&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reg_date&lt;/th&gt;
      &lt;th&gt;last_login&lt;/th&gt;
      &lt;th&gt;logins_7d&lt;/th&gt;
      &lt;th&gt;logins_7d_mobile&lt;/th&gt;
      &lt;th&gt;logins_7d_web&lt;/th&gt;
      &lt;th&gt;num_friends&lt;/th&gt;
      &lt;th&gt;opt_out&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;userid&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2014-06-23&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-12-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2014-04-18&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-12-17&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2014-08-09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2013-08-18&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2014-03-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2014-05-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2014-06-06&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2013-08-16&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2013-09-12&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2014-07-29&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2013-11-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2013-10-09&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2014-02-16&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;2014-04-20&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;2014-07-02&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;2014-05-10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Today was a very intense day.   But the programming is delivering on what it promised: Hands On Learning From Experience Professions!&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category><category term="psycopg2"></category></entry><entry><title>Galvanize - Week 01 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-03/" rel="alternate"></link><updated>2015-06-03T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-03:galvanize/galvanize-data-science-01-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 3&lt;/h2&gt;
&lt;p&gt;Today was an 'introduction' to SQL and PostgreSQL.  I put introduction in quotes because it does not properly describe what we did.  The pre-reading was to complete all 9 (1-9) tutorials on &lt;a href="http://sqlzoo.net/"&gt;SQLZoo&lt;/a&gt;.  This took me about 5 hours.   During lecture we have a review of the order of operation of SQL queries, as well as a detailed explanation of joins.    &lt;/p&gt;
&lt;p&gt;The sprint for the day involved install &lt;a href="http://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; locally, loading a database into it, then completing ~25 basic and 10 advance (extra credit) queries.   Our database had 3 tables with 300k, 500k, and 5k entries respectively.&lt;/p&gt;
&lt;p&gt;These were a great set of assignment because of how they 'leveled-up'.  Even the few among us that were sophisticated with SQL had difficulty with the advance problems.&lt;/p&gt;
&lt;p&gt;Now that I have completed ~10 hours of SQL queries today, I am going to end this post now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT * 
FROM bryan JOIN bed 
ON bryan.location=bed.location 
AND bryan.state=&amp;#39;sleep&amp;#39; 
AND bed.state=&amp;#39;comfy&amp;#39;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category></entry><entry><title>Galvanize - Week 01 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-02/" rel="alternate"></link><updated>2015-06-02T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-02:galvanize/galvanize-data-science-01-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 2&lt;/h2&gt;
&lt;p&gt;Today was he first 'regular' day in the program.  I showed up about 90 minutes before the mini-quiz to review the readings for the day's lecture on Object Oriented Programming (OOP).   At 9:30 we started the mini-quiz on SQL statements and results.  I found it rather simple.  We were given 30 minutes to complete it, and I finished in about 10 minutes.  Most the topics involved analogs in pandas that I am familiar with, so I think that's why I finished rather quickly.&lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;We had two lectures today.  The first lecture was on object oriented structures, and how to implement them in python.  The afternoon lecture was on scoping in python, and a little bit of debugging.  We were introduced to pdb, but told that the use of debuggers is not well integrated in the data science community.&lt;/p&gt;
&lt;h3&gt;LEGB&lt;/h3&gt;
&lt;p&gt;We were told the variables are looked for in the order of local, enclosing function, global, and python build-in.   I made a set of functions to try to illustrate it for myself.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;locally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;built&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Paired Programming Sprint&lt;/h2&gt;
&lt;p&gt;Today's project involved programming a text based game of black jack with a dealer and 1 number of players.  We also had the extra credit options adding n-players, AI/Bot players, double down, and split.  I am happy to report that we that my partner and I were able to complete the first three, but ran out of time before implementing split.&lt;/p&gt;
&lt;p&gt;We started off with pencil and paper using the noun,verb method of abstraction.   We settled on making a Deck, a Player, and Hand, and Game, and an AI.    Its clear at the end that we should have abstracted the game more, and given the Hand class more responsibilities to best implement the split method.&lt;/p&gt;
&lt;p&gt;After we finished we had our dealer hit until 17 or above, while our AI bot hit until soft 17 below.   We also had it implement a doubling bettering strategy.   In our sample, the AI agent one more often then not came out ahead from this setup.   It added a little credence to the ways dealer's seem to play in Las Vegas.&lt;/p&gt;
&lt;p&gt;The repo is currently private, because it could be a project for future cohorts.   I do not want to make a copy public, but will if they give permission.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry><entry><title>Galvanize - Week 01 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-01/" rel="alternate"></link><updated>2015-06-01T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-01:galvanize/galvanize-data-science-01-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 1&lt;/h2&gt;
&lt;p&gt;Today is my first day attending Galvanize's Immersive Data Science Program in San Francisco, CA.   The program is a 12 week program that is approximately 10 hours a day of learning and activities to reinforce and refine the learning.   I am very excited to be a part of this program.&lt;/p&gt;
&lt;h2&gt;My Background&lt;/h2&gt;
&lt;p&gt;I have a Ph.d in Theoretical High Energy Particle Physics and Cosmology, earned a Data Analysis Nano-degree from Udacity.com, and also am current working on a M.S. in Computer Science from GA Tech.    I have also spent the last 8 years teaching high school physics and robotics.   &lt;/p&gt;
&lt;p&gt;I will likely have some strength with math and theory, but I have no doubt that my programming will significantly improve over the next 12 weeks.   Everyone in the program is well educated and intelligent, and each one of them have strengths in some areas and room for improvements in others.  It seems to be a strength for this program.   No matter your weakness, there are students that have that as a strength.&lt;/p&gt;
&lt;h2&gt;Summary of the Day&lt;/h2&gt;
&lt;p&gt;The first half of the day is getting to know our instructors, hour cohort, and the amazingly nice galvanize complex.  It is a 5 story building filled with startup companies, work spaces, galvanize students, and other tech visitors.   I found this to be an impressive building.&lt;/p&gt;
&lt;p&gt;After about an hour of getting to know everyone, we were given a presentation.  That was followed by a tour of the galvanize building.  We were then given an assessment on the pre-course material that was given to us before we showed up.&lt;/p&gt;
&lt;p&gt;After lunch, we had an 90 minute lecture, then worked on a paired sprint assignment for about 3 hours.   This assignment involved...&lt;/p&gt;
&lt;p&gt;After we finished the sprint, we were invited to a Galvanize happy hour to socialize over beer and wine.   I feel very lucky to be apart of this program, and have been impressed with my cohort, the instructors, and Galvanize.  &lt;/p&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;The pre-course material required us to complete material on the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Numpy/Pandas&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Web Awareness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The initial assessment we were given was a 120 minute test on the first 5 topics.  It was an 'open book' test, but that does not mean it was easy.  A majority of my peers did not finish within the allotted time.   &lt;/p&gt;
&lt;p&gt;I am not going to post details on the test because I would hate to ruin the thrill of discovery for potential future students.&lt;/p&gt;
&lt;h2&gt;LUNCH!!!!&lt;/h2&gt;
&lt;p&gt;They provide us a lunch on the first day, but most days we have an 75 minute break for lunch.  There are kitchen, fridges, storage for us to use if we wish.   The lunch was nice, from a local Thai place.   &lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;The lecture, in my opinion, was a little redundant with the course material.  It seemed structure under the assumption that you didn't read or review the python pre-course materials.  I understand its important that everyone is on the same starting point, but I wish we got to jump in a little deeper.&lt;/p&gt;
&lt;p&gt;I did learn and see the importance of using generators when possible.   It save both memory and time.   &lt;/p&gt;
&lt;h2&gt;Paired programming&lt;/h2&gt;
&lt;p&gt;After the lecture we grouped up for a paired programming assignment.   We trade off roles of being the driver and the navigator in 20 to 30 minute rotations for a 3 hour block of programming.  We start of by forking the day's assignment from a Github repo and cloning it locally.  Today we then worked two projects.  The first project was completing a list of functions based on a description of the function, including inputs and outputs.  The second project was fixing inefficiently running code.&lt;/p&gt;
&lt;p&gt;A simple example is checking if a key is in a dictionary.   Before today I might have checked to see if it was in the keys() results, but we can see that for medium size dictionaries that it is almost 40x slower than just using in in the dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;randint&lt;/span&gt;

&lt;span class="n"&gt;cnt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;

&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.17&lt;/span&gt; &lt;span class="err"&gt;µ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="mi"&gt;10000000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;111&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We saw similar results for iter compared to iteritems, range to xrange, and izip to zip.   It was a useful assignment for the content and the practice of collaborating with someone else.&lt;/p&gt;
&lt;h2&gt;After reception&lt;/h2&gt;
&lt;p&gt;Galvanize SF now runs two cohorts 6 weeks apart.  We had a mixer with previous cohort, enjoying beer, wine, and conversation on the roof of the building.  &lt;/p&gt;
&lt;p&gt;After 11 hours at Galvanize, I decided it was time to head home.   Definitely looking forward to day 2. &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry></feed>