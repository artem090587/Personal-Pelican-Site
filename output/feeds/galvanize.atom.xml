<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bryan Travis Smith, Ph.D</title><link href="http://www.bryantravissmith.com/" rel="alternate"></link><link href="http://www.bryantravissmith.com/feeds/galvanize.atom.xml" rel="self"></link><id>http://www.bryantravissmith.com/</id><updated>2015-06-29T10:30:00-07:00</updated><entry><title>Galvanize - Week 05 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-05-01/" rel="alternate"></link><updated>2015-06-29T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-29:galvanize/galvanize-data-science-05-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 5 - Day 1&lt;/h2&gt;
&lt;p&gt;Today we had an introduction to webscraping, web apis, and &lt;a href="http://www.mongodb.org/"&gt;MongoDB&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;The morning quiz was on answer questions using data stored in a 5 table PostgreSQL database that were suppose to simulate &lt;a href="http://hitchapp.com"&gt;Hithc&lt;/a&gt;.  An example question is: Find the number of unique users that used the service over the last 10 days that were driven by drivers who started driving between DATE1 and DATE2.  Are not important.  &lt;/p&gt;
&lt;h2&gt;MongoDB&lt;/h2&gt;
&lt;p&gt;We started off by downloading and installing a local copy of MongoDB from &lt;a href="http://www.mongodb.org/downloads?_ga=1.2370361.886345798.1422741448"&gt;here&lt;/a&gt;.  Some of the issues my cohort ran into was the database locking because proper permissions were to given to the 'data/db/' directory on the mac.   Other then that, it was a very easy processes.   &lt;/p&gt;
&lt;p&gt;Though it was not part of the exercise, I also installed pymongo and used that.   I would then do what was asked directly in the database terminal, then replicate it with pymongo in an IPython Notebook.  We were given an option of downloading some GUI interfaces, but I opted not to use them.   Just incase I change my mind in the future I will list them here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://robomongo.org/"&gt;Robomongo (Multiplatform)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fotonauts/MongoHub-Mac"&gt;MongoHub (Mac OSX)&lt;/a&gt; 
   with down-loadable &lt;a href="https://mongohub.s3.amazonaws.com/MongoHub.zip"&gt;binary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bagwanpankaj/humongous"&gt;Humongous (web based)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Mongo Queries&lt;/h2&gt;
&lt;p&gt;Our first task was to load in some messy click data into our MongoDB database.  I used the command line for this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;mongoimport --db clicks --collection log &amp;lt; click_log.json&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I then used the mongo db to findOne() and find().limit(3).  There are over 2000 click results of different lengths.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.findOne()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;{ "&lt;em&gt;id" : ObjectId("559198d72c8e29706d471d8e"), "_heartbeat&lt;/em&gt;" : 1368774601 }&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;db.log.find().limit(3)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p -116.345802_="-116.345802," 0_="0," 1368774179_="1368774179," 33.7724="33.7724" 6_1_3="6_1_3" :=":" AppleWebKit_536.26="AppleWebKit/536.26" CPU="CPU" Desert_="Desert&amp;quot;," Gecko_="Gecko)" ISODate_2013-05-17T07:09:59Z_="ISODate(&amp;quot;2013-05-17T07:09:59Z&amp;quot;)," Mac="Mac" Mobile_10B329_="Mobile/10B329&amp;quot;," OS="OS" ObjectId_559198d72c8e29706d471d90_="ObjectId(&amp;quot;559198d72c8e29706d471d90&amp;quot;)," X_="X)" _="]" _1.usa.gov_="&amp;quot;1.usa.gov&amp;quot;," _1084Psg_="&amp;quot;1084Psg&amp;quot;," _19Cztuz_="&amp;quot;19Cztuz&amp;quot;," _America_Los_Angeles_="&amp;quot;America/Los_Angeles&amp;quot;," _CA_="&amp;quot;CA&amp;quot;," _KHTML_="(KHTML," _Mozilla_5.0="&amp;quot;Mozilla/5.0" _Palm="&amp;quot;Palm" _US_="&amp;quot;US&amp;quot;," __id_="&amp;quot;_id&amp;quot;" _a_="&amp;quot;a&amp;quot;" _al_="&amp;quot;al&amp;quot;" _c_="&amp;quot;c&amp;quot;" _cy_="&amp;quot;cy&amp;quot;" _en-us_="&amp;quot;en-us&amp;quot;," _g_="&amp;quot;g&amp;quot;" _gr_="&amp;quot;gr&amp;quot;" _h_="&amp;quot;h&amp;quot;" _hc_="&amp;quot;hc&amp;quot;" _hh_="&amp;quot;hh&amp;quot;" _http:_science.nasa.gov_science-news_science-at-nasa_2013_16may_lunarimpact_="&amp;quot;http://science.nasa.gov/science-news/science-at-nasa/2013/16may_lunarimpact/&amp;quot;," _http:_t.co_btKvKFBaF5_="&amp;quot;http://t.co/btKvKFBaF5&amp;quot;," _iPhone_="(iPhone;" _l_="&amp;quot;l&amp;quot;" _ll_="&amp;quot;ll&amp;quot;" _nk_="&amp;quot;nk&amp;quot;" _r_="&amp;quot;r&amp;quot;" _t_="&amp;quot;t&amp;quot;" _tweetdeckapi_="&amp;quot;tweetdeckapi&amp;quot;," _tz_="&amp;quot;tz&amp;quot;" _u_="&amp;quot;u&amp;quot;" iPhone="iPhone" like="like"&gt;{ "&lt;em&gt;id" : ObjectId("559198d72c8e29706d471d8e"), "_heartbeat&lt;/em&gt;" : 1368774601 }
{ "_id" : ObjectId("559198d72c8e29706d471d8f"), "a" : "Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)", "c" : "NL", "nk" : 0, "tz" : "Europe/Amsterdam", "gr" : "06", "g" : "15r91", "h" : "10OBm3W", "l" : "pontifier", "al" : "en-GB", "hh" : "j.mp", "r" : "direct", "u" : "http://www.nsa.gov/", "t" : ISODate("2013-05-17T07:09:59Z"), "hc" : 1365701422, "cy" : "Oss", "ll" : [ 5.5333, 51.766701 ] }&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pymongo&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MongoClient&lt;/span&gt;
&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MongoClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;27017&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;db_nyt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_database&lt;/span&gt;
&lt;span class="n"&gt;ny&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db_nyt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ny_times&lt;/span&gt;
&lt;span class="n"&gt;db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clicks&lt;/span&gt;
&lt;span class="n"&gt;log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_one&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Limit 3&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; 
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;


&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;559198d72c8e29706d471d8e&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;_heartbeat_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1368774601&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;Limit&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;559198d72c8e29706d471d8e&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;_heartbeat_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1368774601&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;NL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;nk&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;tz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;Europe/Amsterdam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;gr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;15r91&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;10OBm3W&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;cy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;Oss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;pontifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;al&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;en-GB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;hh&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;j.mp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;direct&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;http://www.nsa.gov/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2013&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;hc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1365701422&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;559198d72c8e29706d471d8f&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;ll&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;5.5333&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;51.766701&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;

&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;Mozilla/5.0 (iPhone; CPU iPhone OS 6_1_3 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Mobile/10B329&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;US&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;nk&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;tz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;America/Los_Angeles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;gr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;CA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;1084Psg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;19Cztuz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;cy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;Palm Desert&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;tweetdeckapi&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;al&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;en-us&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;hh&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;1.usa.gov&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;http://t.co/btKvKFBaF5&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;http://science.nasa.gov/science-news/science-at-nasa/2013/16may_lunarimpact/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2013&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;hc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1368774179&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;559198d72c8e29706d471d90&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;ll&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;116.345802&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;33.7724&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the pymongo results are identical to the commandline results.   What I like about pymongo, as we will see later, is that I can pull data from different sources and use that information to make queries to the MongoDB database.&lt;/p&gt;
&lt;p&gt;Our next task was to find out how many clicks were in San Francisco.  This is relatively easy becasue we can see that the records that are clearly user click have an element 'cy' which looks to be the city they are in.  It seems the two users above are in Palm Desert and Oss.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find({'cy':'San Francisco'}).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;11&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;log.find({&amp;#39;cy&amp;#39;:&amp;#39;San Francisco&amp;#39;}).count()




11
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Back in my "I want to be a front end web developer" days, I was very much aware that there are different browsers, and they can have different features avaialbe or represent css in slightly different ways.   I had no idea how many different browers there were.  We can see in the query results there is an element 'a' that is the webbrowser.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.distinct('a').length&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;559&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are over 559 different browers types/versions in this dataset.   I do not miss front end development in the least! &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;len(log.distinct(&amp;#39;a&amp;#39;))




559
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We learned that one of the strong use cases for MongoDB is text data.   We did an afternoon project that will be covered later scaping the New York Times.  More on that to come.&lt;/p&gt;
&lt;p&gt;MongoDB, like almost all other databases, support regular expressions.  We can do a simple query and find out how many user use Mozilla, Opera, or both.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find({'a': {'\$regex':'Mozilla|Opera'} }).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2830&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;db.log.find({'a': {'\$regex':'Mozilla'} }).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2723&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;db.log.find({'a': {'\$regex':'Opera'} }).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;107&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;print log.find(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;a&amp;#39;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;regex&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;:&amp;#39;Mozilla|Opera&amp;#39;} }).count()&lt;/span&gt;
&lt;span class="x"&gt;print log.find(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;a&amp;#39;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;regex&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;:&amp;#39;Mozilla&amp;#39;} }).count()&lt;/span&gt;
&lt;span class="x"&gt;print log.find(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;a&amp;#39;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;regex&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;:&amp;#39;Opera&amp;#39;} }).count()&lt;/span&gt;

&lt;span class="x"&gt;2830&lt;/span&gt;
&lt;span class="x"&gt;2723&lt;/span&gt;
&lt;span class="x"&gt;107&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A careful inspect will show that there is a variable 't' that is a DateTime object.   Originally it was the unix timestamp in seconds.   We had ot convert it to the datetime object, that was was doing with the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;   &lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\$exists&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;}}).&lt;/span&gt;&lt;span class="nx"&gt;forEach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; 
       &lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nx"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
       &lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$set&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt;
   &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is important because we were next asked to findout how many clicks were in the first hours.  I just happened to notice this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find({'t':{\$exists:1}}).sort({'t':-1})[0].t&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ISODate("2013-05-17T08:09:56Z")&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;db.log.find({'t':{\$exists:1}}).sort({'t':1})[0].t&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ISODate("2013-05-17T07:09:57Z")&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;We were were given only 1 hours worth of data!  So all the records should be there when we do the correct query.  The query to do this if we did not notice this fact would look like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;t1 = db.log.find({'t':{'\$exists':'true'}}).sort({'t':1})[0].t&lt;/p&gt;
&lt;p&gt;t2 = new Date(t1.getTime()+3600000)&lt;/p&gt;
&lt;p&gt;db.log.find({'t':{\$gte:t1,\$lte:t2}}).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2949&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;And if were just to count rectors where the 't' variable exists:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find({'t':{'$exists':'true'}}).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2949&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt;
&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$exists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;true&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;t2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hours&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$gte&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$lte&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;t2&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="mi"&gt;2949&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last question we had to answer was about what links the users clicked on the most.   This was an introduction to MongoDB's &lt;a href="http://docs.mongodb.org/manual/reference/sql-aggregation-comparison/"&gt;aggregation&lt;/a&gt; functionality.&lt;/p&gt;
&lt;p&gt;My first idea worked.  In terms of SQL I would group by the link address, count the number of records for each link, and sort by the link results.  In MongoDB this looks like the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.aggregate([{\$group:{_id:'\$u',count:{\$sum:1}}},{\$sort:{count:-1}},{\$limit:1}])&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;{ "_id" : "http://www.nsa.gov/", "count" : 478 }&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;pipeline = [&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;group&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;_id&amp;#39;:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;u&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;,&amp;#39;count&amp;#39;:&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;sum&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;:1}}},&lt;/span&gt;
&lt;span class="x"&gt;            &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;sort&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;count&amp;quot;: -1}},&lt;/span&gt;
&lt;span class="x"&gt;            &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;limit&lt;/span&gt;&lt;span class="x"&gt;&amp;quot;: 1}]&lt;/span&gt;
&lt;span class="x"&gt;for r in log.aggregate(pipeline):&lt;/span&gt;
&lt;span class="x"&gt;    print str(r) + &amp;quot;\n&amp;quot;&lt;/span&gt;

&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;u&amp;#39;count&amp;#39;: 478, u&amp;#39;_id&amp;#39;: u&amp;#39;http://www.nsa.gov/&amp;#39;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Geospacial&lt;/h2&gt;
&lt;p&gt;We were given some extra-credit MongoDB tasks involving &lt;a href="http://docs.mongodb.org/manual/administration/indexes-geo/"&gt;geospatial&lt;/a&gt; data.  MongoDB requires that the data be longitude, latitude, but our data is stored as latitude, longitude.  We need to go through the data base and fix this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ll&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;$exists&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;true&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}}).&lt;/span&gt;&lt;span class="nx"&gt;forEach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; 
        &lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;entry.ll&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ll&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt; 
        &lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="nx"&gt;$set&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; 
    &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that our database is in the correct format, we need to create an 2D Spacial indexo n the data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.createIndex( { ll : "2d" } )&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we can ask questions about how many clicks are within 50 miles of San Francisco:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find( { 'll' : { \$geoWithin :{ \$centerSphere : [ [  -122.4167, 37.7833 ] , 50 / 3963.2 ] } } } ).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;226&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;log.find(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;ll&amp;#39;:&lt;/span&gt;
&lt;span class="x"&gt;          &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;geoWithin&lt;/span&gt;&lt;span class="x"&gt;&amp;#39; :&lt;/span&gt;
&lt;span class="x"&gt;           &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;centerSphere&lt;/span&gt;&lt;span class="x"&gt;&amp;#39; : [ [ -122.4167, 37.7833 ] , 50 / 3963.2 ] } &lt;/span&gt;&lt;span class="cp"&gt;#&lt;/span&gt;&lt;span class="nf"&gt;Lat&lt;/span&gt;&lt;span class="x"&gt;,Lng SF 50 Miles/3964.2 Miles/Degree&lt;/span&gt;
&lt;span class="x"&gt;          } &lt;/span&gt;
&lt;span class="x"&gt;         }).count()&lt;/span&gt;




&lt;span class="x"&gt;226&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could ask the questions:  "How many users are in Maine?" &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;db.log.find({'gr':'ME'}).count()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;log.find({&amp;#39;gr&amp;#39;:&amp;#39;ME&amp;#39;}).count()




2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That was easy, but if we did not have the state information, we would use the location data.   I found a site that hosts maps of the state boundries as polylines of gps coordinates.   This is where pymongo comes in handy over the command line.  I read in the data from a webrequest, then constructed a query from the GPS data using MongoDB's geo-features.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xml.etree.ElementTree&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ET&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt;
&lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://econym.org.uk/gmap/states.xml&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;contents&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;lng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;lat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ET&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromstring&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;contents&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getchildren&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attrib&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Maine&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getchildren&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attrib&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lng&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attrib&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])])&lt;/span&gt;
            &lt;span class="n"&gt;lng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attrib&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lng&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attrib&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ll&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;$geoWithin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;$geometry&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Polygon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;coordinates&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;}}})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also get 2, without using any pre-identified cate information.   This allows us to check for consistency incase we need to clean data.  This is awesome.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ME&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ll&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ll&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;go&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;62&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Longitude&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Latitude&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW05D1/output_19_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see where our two Maine users in the map.  This is a feature of using pymongo that is not avaialble in just the termial application of MongoDB.&lt;/p&gt;
&lt;p&gt;Just for reference I wanted to list the geospacial tools listed for us if we wanted to explore them.  I did not, but I am using this post as a reference.  &lt;/p&gt;
&lt;p&gt;-&lt;a href="http://cartodb.com/"&gt;CartoDB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;-&lt;a href="http://blog.cartodb.com/post/66687861735/torque-is-live-try-it-on-your-cartodb-maps-today"&gt;torque map&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Web Scraping:  Ebay&lt;/h2&gt;
&lt;p&gt;In the afternoon we were asked to engage in a couple of web scraping projects.  One was to find a topic on ebay, and retreive all the images on the search result page.   I, of course, searched for &lt;a href="http://www.ebay.com/sch/i.html?_from=R40&amp;amp;_trksid=p2050601.m570.l1313.TR5.TRC0.A0.H0.Xtesla.TRS0&amp;amp;_nkw=tesla&amp;amp;_sacat=0"&gt;Teslas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first thing we need is a place to store our images:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nf"&gt;%mkdir&lt;/span&gt; &lt;span class="n"&gt;tesla&lt;/span&gt;
&lt;span class="nf"&gt;%ls&lt;/span&gt; &lt;span class="n"&gt;tesla&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see I just made the directory and that it is empty.  Now its type to scrape the webpage and pull out the images features.  The process we will do is request the webpage, use Beautiful soup to get the image sources, then down load each image source and store it in the tesla directory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;


&lt;span class="n"&gt;tesla_html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ebay.com/sch/i.html?_from=R40&amp;amp;_trksid=p2050601.m570.l1313.TR5.TRC0.A0.H0.Xtesla.TRS0&amp;amp;_nkw=tesla&amp;amp;_sacat=0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tesla_html&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;imgs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;el&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;src&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;div.lvpicinner a img&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_str&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;imgs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
    &lt;span class="n"&gt;to_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tesla/&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;img_str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;to_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt; &lt;span class="n"&gt;tesla&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;

&lt;span class="n"&gt;m0Q3jSK6NQ9_Hu_YIl38T9g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mbbybBkxozI9joXb1CgjfKQ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;m45xVWpc86gaOwXzaWdXXOQ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mgTnEkhYXi4kepjp3aNCx7Q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;m52qwfleCJ&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;encMaAJO9Taw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mgoIrvmOD&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;scAVdqXD1pirA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mDbg8VxsScOpYeQ6VH1TBEg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mjMTLKMNaSsJNsRzn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;zhQPg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mEDv_smHtanyCtScA1bCnpA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;ml5LUVDTC8ysTyvl70jOZMQ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mEEQdnmeKYAH32KJpZElmfw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mmstVHfStN09f4KjbYZpQyA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mE_xrXWO7xW9JF0uKYi6YZA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mpSnWCeMxvy3kVTjq3Ij44A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mV4lwZBpH6&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;wbB6xoZhRB1A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mu5fgrRMhaOCwRhDu_eI1sw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mVP1Wc7_ekfxObQrj4DaqtQ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mwdcx7MTbRMNuGi_4k7FiSw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mWrzlrW8xau7TE3WtJJtE9Q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;mzqPumoh3YFjZ8kvXLi2oMw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;span class="n"&gt;mXyFRK9gbpV9&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="n"&gt;QVuzA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;  &lt;span class="n"&gt;s_1x2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gif&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that we have downlaoded 22&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;img = plt.imread(open(&amp;#39;tesla/m0Q3jSK6NQ9_Hu_YIl38T9g.jpg&amp;#39;))
plt.imshow(img)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW05D1/output_26_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;I'm in love!!!!!   Only 55K, what a steal!&lt;/p&gt;
&lt;p&gt;This was fun, but my partner got burned out.  Ebay is full of iframes when looking at an individual description of a car, and I had us jumping to thhose pages to also scrape descriptions and prices.   Since we were not asked to do this, we moved on with our afternoon sprint. &lt;/p&gt;
&lt;h1&gt;New York Times Scraping&lt;/h1&gt;
&lt;p&gt;The NY times has, as we learned, over 15 Million articles posted on this website, some going back to 1851.   These are images/pdfs that you can view, but text you can easily scrape.  Reguardless, they are there.  &lt;/p&gt;
&lt;p&gt;Probably because of people like us, the New York Times also has an API to access aspects of their data base.  This makes it easier for us to get data and allows them to manage/mitigate the affect these requests have on their servers.&lt;/p&gt;
&lt;p&gt;The api requires an api.  You will see reference to it as a variable, but for obvious reasons I will not post my actual api key.  We have a simple function that allows us to send and process the response from the NY Times API.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;single_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status_code&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;WARNING&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status_code&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;pay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sort&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;oldest&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;api-key&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ny_key&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;http://api.nytimes.com/svc/search/v2/articlesearch.json&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;ny_articles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;single_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;ny_articles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;response&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;copyright&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The response is a JSON document, and the first level has data about the query.   The next level has information about the responses, and then we have meta data about the article documents.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ny_articles[&amp;#39;response&amp;#39;].keys()




[u&amp;#39;docs&amp;#39;, u&amp;#39;meta&amp;#39;]




ny_articles[&amp;#39;response&amp;#39;][&amp;#39;meta&amp;#39;]




{u&amp;#39;hits&amp;#39;: 15569986, u&amp;#39;offset&amp;#39;: 0, u&amp;#39;time&amp;#39;: 179}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that our response has 15,569,986 matches, taking 179ms to response.  Our offset is 0.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ny_articles[&amp;#39;response&amp;#39;][&amp;#39;docs&amp;#39;][0]




{u&amp;#39;_id&amp;#39;: u&amp;#39;4fbfd23e45c1498b0d004db6&amp;#39;,
 u&amp;#39;abstract&amp;#39;: None,
 u&amp;#39;blog&amp;#39;: [],
 u&amp;#39;byline&amp;#39;: None,
 u&amp;#39;document_type&amp;#39;: u&amp;#39;article&amp;#39;,
 u&amp;#39;headline&amp;#39;: {u&amp;#39;kicker&amp;#39;: u&amp;#39;1&amp;#39;,
  u&amp;#39;main&amp;#39;: u&amp;#39;??itive Salve Case in Philadelphia&amp;#39;},
 u&amp;#39;keywords&amp;#39;: [],
 u&amp;#39;lead_paragraph&amp;#39;: None,
 u&amp;#39;multimedia&amp;#39;: [],
 u&amp;#39;news_desk&amp;#39;: None,
 u&amp;#39;print_page&amp;#39;: u&amp;#39;4&amp;#39;,
 u&amp;#39;pub_date&amp;#39;: u&amp;#39;1851-09-18T00:03:58Z&amp;#39;,
 u&amp;#39;section_name&amp;#39;: None,
 u&amp;#39;snippet&amp;#39;: None,
 u&amp;#39;source&amp;#39;: u&amp;#39;The New York Times&amp;#39;,
 u&amp;#39;subsection_name&amp;#39;: None,
 u&amp;#39;type_of_material&amp;#39;: u&amp;#39;Article&amp;#39;,
 u&amp;#39;web_url&amp;#39;: u&amp;#39;http://query.nytimes.com/gst/abstract.html?res=9904E7DE1430EF33A2575BC1A96F9C946092D7CF&amp;#39;,
 u&amp;#39;word_count&amp;#39;: 74}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The docs are a list of 10 document's JSON formated meta data.  We originally sorted by oldest, but we can look at the meta data for current documents.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pay = {&amp;#39;sort&amp;#39;:&amp;#39;newest&amp;#39;, &amp;#39;api-key&amp;#39;:ny_key}
ny_articles = single_query(link,pay)
print ny_articles[&amp;#39;response&amp;#39;][&amp;#39;docs&amp;#39;][0]

{u&amp;#39;type_of_material&amp;#39;: u&amp;#39;Op-Ed&amp;#39;, u&amp;#39;blog&amp;#39;: [], u&amp;#39;news_desk&amp;#39;: u&amp;#39;OpEd&amp;#39;, u&amp;#39;lead_paragraph&amp;#39;: u&amp;#39;The Greek debt crisis taxes the best minds of Europe.&amp;#39;, u&amp;#39;headline&amp;#39;: {u&amp;#39;main&amp;#39;: u&amp;#39;Do We Have a Plan?&amp;#39;, u&amp;#39;content_kicker&amp;#39;: u&amp;#39;Op-Ed Columnist&amp;#39;}, u&amp;#39;abstract&amp;#39;: None, u&amp;#39;print_page&amp;#39;: None, u&amp;#39;word_count&amp;#39;: u&amp;#39;10&amp;#39;, u&amp;#39;_id&amp;#39;: u&amp;#39;557ec00038f0d86829e412aa&amp;#39;, u&amp;#39;snippet&amp;#39;: u&amp;#39;The Greek debt crisis taxes the best minds of Europe.&amp;#39;, u&amp;#39;source&amp;#39;: u&amp;#39;The New York Times&amp;#39;, u&amp;#39;web_url&amp;#39;: u&amp;#39;http://www.nytimes.com/2015/07/12/opinion/do-we-have-a-plan.html&amp;#39;, u&amp;#39;multimedia&amp;#39;: [{u&amp;#39;subtype&amp;#39;: u&amp;#39;wide&amp;#39;, u&amp;#39;url&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-thumbWide.jpg&amp;#39;, u&amp;#39;height&amp;#39;: 126, u&amp;#39;width&amp;#39;: 190, u&amp;#39;legacy&amp;#39;: {u&amp;#39;wide&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-thumbWide.jpg&amp;#39;, u&amp;#39;wideheight&amp;#39;: u&amp;#39;126&amp;#39;, u&amp;#39;widewidth&amp;#39;: u&amp;#39;190&amp;#39;}, u&amp;#39;type&amp;#39;: u&amp;#39;image&amp;#39;}, {u&amp;#39;subtype&amp;#39;: u&amp;#39;xlarge&amp;#39;, u&amp;#39;url&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-articleLarge.jpg&amp;#39;, u&amp;#39;height&amp;#39;: 443, u&amp;#39;width&amp;#39;: 600, u&amp;#39;legacy&amp;#39;: {u&amp;#39;xlargewidth&amp;#39;: u&amp;#39;600&amp;#39;, u&amp;#39;xlarge&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-articleLarge.jpg&amp;#39;, u&amp;#39;xlargeheight&amp;#39;: u&amp;#39;443&amp;#39;}, u&amp;#39;type&amp;#39;: u&amp;#39;image&amp;#39;}, {u&amp;#39;subtype&amp;#39;: u&amp;#39;thumbnail&amp;#39;, u&amp;#39;url&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-thumbStandard.jpg&amp;#39;, u&amp;#39;height&amp;#39;: 75, u&amp;#39;width&amp;#39;: 75, u&amp;#39;legacy&amp;#39;: {u&amp;#39;thumbnailheight&amp;#39;: u&amp;#39;75&amp;#39;, u&amp;#39;thumbnail&amp;#39;: u&amp;#39;images/2015/06/15/opinion/15edchapart/15edchapart-thumbStandard.jpg&amp;#39;, u&amp;#39;thumbnailwidth&amp;#39;: u&amp;#39;75&amp;#39;}, u&amp;#39;type&amp;#39;: u&amp;#39;image&amp;#39;}], u&amp;#39;subsection_name&amp;#39;: None, u&amp;#39;keywords&amp;#39;: [{u&amp;#39;value&amp;#39;: u&amp;#39;European Sovereign Debt Crisis (2010- )&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;1&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;subject&amp;#39;}, {u&amp;#39;value&amp;#39;: u&amp;#39;Greece&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;2&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;glocations&amp;#39;}, {u&amp;#39;value&amp;#39;: u&amp;#39;Europe&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;3&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;glocations&amp;#39;}, {u&amp;#39;value&amp;#39;: u&amp;#39;European Union&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;4&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;organizations&amp;#39;}, {u&amp;#39;value&amp;#39;: u&amp;#39;European Central Bank&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;5&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;organizations&amp;#39;}, {u&amp;#39;value&amp;#39;: u&amp;#39;Eurozone&amp;#39;, u&amp;#39;is_major&amp;#39;: u&amp;#39;N&amp;#39;, u&amp;#39;rank&amp;#39;: u&amp;#39;6&amp;#39;, u&amp;#39;name&amp;#39;: u&amp;#39;organizations&amp;#39;}], u&amp;#39;byline&amp;#39;: {u&amp;#39;person&amp;#39;: [{u&amp;#39;organization&amp;#39;: u&amp;#39;&amp;#39;, u&amp;#39;role&amp;#39;: u&amp;#39;reported&amp;#39;, u&amp;#39;rank&amp;#39;: 1, u&amp;#39;firstname&amp;#39;: u&amp;#39;Patrick&amp;#39;, u&amp;#39;lastname&amp;#39;: u&amp;#39;CHAPPATTE&amp;#39;}], u&amp;#39;original&amp;#39;: u&amp;#39;By PATRICK CHAPPATTE&amp;#39;}, u&amp;#39;document_type&amp;#39;: u&amp;#39;article&amp;#39;, u&amp;#39;pub_date&amp;#39;: u&amp;#39;2015-07-12T00:00:00Z&amp;#39;, u&amp;#39;section_name&amp;#39;: u&amp;#39;Opinion&amp;#39;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The NYT has different information for different articles.  We decided we would see how many we would download before we reached our daily limit.  It turns out that it was over 30,000 articles.   Since we were encoraged to see how many articles we could pull, we did not do it thoughtfully.  The NY times is fill of videos and receipes and other content that is not text based.   In retrospect I wish we focused on only 'News' articles.&lt;/p&gt;
&lt;p&gt;A number of our cohort had trouble getting past 1000 articles.  This is because the NY Times limits the pagation of their response to 99.  We got around this by cycling through endates as well as pages in the response.   We also found, that because of the type of material that they post, some days have more than 1000 articles.  We limited our selves to 1000 articles from a given day, then moved on to the previous day.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="c"&gt;#day we did the assignment&lt;/span&gt;
&lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;20150622&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;pay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sort&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;newest&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;end_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;api-key&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ny_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;page&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;articles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;single_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;articles&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;response&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;docs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="c"&gt;##We upsert into the database because we when change the &lt;/span&gt;
        &lt;span class="c"&gt;##enddate we get some redundant articles&lt;/span&gt;
        &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;upsert&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; 
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;articles&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;response&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;docs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;pub_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="c"&gt;##Avoid being blocked by a too high query rate&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I am not going to run this code for this post, but it allowed us to pull 30,000+ articles and store them in our MongoDB database.  But we can look at the results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ny.find().count()




31140
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is only meta-data however, and not the actual articles from the website.   The final project was to download all the article text for each article.  It is clear that we were not expected to be able to download 30,000+ articles for this assignment.   We started the project by only downloading the articles a few news articles.&lt;/p&gt;
&lt;p&gt;We we also structured it in a way that allowed us to look for articles that have not yet been scraped and stored in our database.   We wanted to avoid double scraping at all cost.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_articles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;type_of_material&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;News&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;HTML_content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$exists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;web_url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$exists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;true&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;pub_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;sup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;web_url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;art&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;.story-content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;art&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;HTML_content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;art&lt;/span&gt;
        &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pull_articles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;movable&lt;/span&gt; &lt;span class="n"&gt;feast&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;fashion&lt;/span&gt; &lt;span class="n"&gt;weeks&lt;/span&gt; &lt;span class="n"&gt;continues&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;After&lt;/span&gt; &lt;span class="n"&gt;Valentino&lt;/span&gt; &lt;span class="n"&gt;decided&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;ho&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I did not print out the entire articles because I am affid of being in violation of copyright with the New York Times.  We did scrape a modest amount of articles.  After pulling the meta data for 30,000 articles we did not want to continue to scrape the web articles as well&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;ny.find(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;HTML_content&amp;#39;: &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;exists&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;:1}}).count()&lt;/span&gt;




&lt;span class="x"&gt;65&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="web scraping"></category></entry><entry><title>Galvanize - Week 04 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-05/" rel="alternate"></link><updated>2015-06-26T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-26:galvanize/galvanize-data-science-04-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 4 - Day 5&lt;/h2&gt;
&lt;p&gt;Today we had an accessment on regression and classification methods we have covered the prevous two weeks.  It was a conceptional test, making sure we understood the underlying models and their applications.  &lt;/p&gt;
&lt;p&gt;We then had a lecture on blogging.  Personally, I did not appreciate it.   I am obviously already blogging about what I am doing at Galvanize, but I also did not appreciate the frame given to the presentation: "I know you don't want to do this, but..."  It reminds me that we can all take issues based on attributes of an interaction that are not content based.  Good to be reminded of this going forward.&lt;/p&gt;
&lt;h2&gt;Profit Curves&lt;/h2&gt;
&lt;p&gt;The afternoon sprint was on a profit curves.  You and read about them from chapter 8 of &lt;a href="http://www.amazon.com/Data-Science-Business-data-analytic-thinking/dp/1449361323"&gt;Data Science For Business&lt;/a&gt;.   &lt;/p&gt;
&lt;p&gt;The goal of the assignment is to define a cost-benefit matrix for a business problem.  An example from the book involves calculating Lift, the increase in conversions.&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  TP &amp;amp; FP \ FN &amp;amp; TN \end{array} \right) =&amp;gt; \left( \begin{array}{cc}  4 &amp;amp; -5 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;In this case if we correct identify someone who will convert, we can spend mondy to convert them and make 5 dollars.   On the other hand, if we have a false positive and invest in someone who will not convert, we lose the 5 dollars of cost.   &lt;/p&gt;
&lt;p&gt;If we do not predict someone to churn, we do not assume any cost.  But we also do not make any profit.   &lt;/p&gt;
&lt;p&gt;In this case the best model will maximize True Positive and Minimize False Positives.   It needs high precisions, but not necessarily hight accuracy or recall.&lt;/p&gt;
&lt;p&gt;The data set we are working with today is a cell phone dataset of users that churned.  We will start with the obligitory cleaning.   Since we have worked with this dataset before I will not be exploring it.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;churn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/churn.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;State&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Area Code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Phone&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;churn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;churn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;no&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;churn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;False.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;churn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;True.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;churn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Account Length&lt;/th&gt;
      &lt;th&gt;Int'l Plan&lt;/th&gt;
      &lt;th&gt;VMail Plan&lt;/th&gt;
      &lt;th&gt;VMail Message&lt;/th&gt;
      &lt;th&gt;Day Mins&lt;/th&gt;
      &lt;th&gt;Day Calls&lt;/th&gt;
      &lt;th&gt;Day Charge&lt;/th&gt;
      &lt;th&gt;Eve Mins&lt;/th&gt;
      &lt;th&gt;Eve Calls&lt;/th&gt;
      &lt;th&gt;Eve Charge&lt;/th&gt;
      &lt;th&gt;Night Mins&lt;/th&gt;
      &lt;th&gt;Night Calls&lt;/th&gt;
      &lt;th&gt;Night Charge&lt;/th&gt;
      &lt;th&gt;Intl Mins&lt;/th&gt;
      &lt;th&gt;Intl Calls&lt;/th&gt;
      &lt;th&gt;Intl Charge&lt;/th&gt;
      &lt;th&gt;CustServ Calls&lt;/th&gt;
      &lt;th&gt;Churn?&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;265.1&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;45.07&lt;/td&gt;
      &lt;td&gt;197.4&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;16.78&lt;/td&gt;
      &lt;td&gt;244.7&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;11.01&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;161.6&lt;/td&gt;
      &lt;td&gt;123&lt;/td&gt;
      &lt;td&gt;27.47&lt;/td&gt;
      &lt;td&gt;195.5&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;16.62&lt;/td&gt;
      &lt;td&gt;254.4&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;11.45&lt;/td&gt;
      &lt;td&gt;13.7&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;137&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;243.4&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
      &lt;td&gt;41.38&lt;/td&gt;
      &lt;td&gt;121.2&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;10.30&lt;/td&gt;
      &lt;td&gt;162.6&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;7.32&lt;/td&gt;
      &lt;td&gt;12.2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3.29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;299.4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;50.90&lt;/td&gt;
      &lt;td&gt;61.9&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;5.26&lt;/td&gt;
      &lt;td&gt;196.9&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;8.86&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;1.78&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;166.7&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;28.34&lt;/td&gt;
      &lt;td&gt;148.3&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt;12.61&lt;/td&gt;
      &lt;td&gt;186.9&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt;8.41&lt;/td&gt;
      &lt;td&gt;10.1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;y = churn.pop(&amp;quot;Churn?&amp;quot;).values
x = churn.values
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because we are dealing with a cell phone companies, the model is different.   I wil fillow suit with the Data Science For Buisness example and not worry about the fixed cost of the business.  Instead we will make a simple model that if we correcy identify someone who will churn then we will invest and make a profit.  If we incorrectly predict someone is going to churn and invest in keeping them, we lose the investment of cost.  &lt;/p&gt;
&lt;p&gt;$$ \mbox{Profit Matrix} = \left( \begin{array}{cc}  80 &amp;amp; -20 \ 0 &amp;amp; 0 \end{array} \right)$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;profit_matrix = np.array([[80,-20],[0,0]])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The profit for a sample of uses will change if the total number of users changes.  For this reason we need make a rate to estimate the average profit per user.   We will have some model with a confusion matrix, and we will want to convert it a rate:&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  TP &amp;amp; FP \ FN &amp;amp; TN \end{array} \right) =&amp;gt; \left( \begin{array}{cc}  \frac{TP}{TP+FP} &amp;amp; \frac{FN}{FN+TN} \ \frac{FP}{TP+FP} &amp;amp; \frac{TN}{FN+TN} \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;This will allow us to get a feel for the acutal rate of misclassification and correct classification in the populations we are concerned with if we know the population proportions $P_+$ and $P_-$.&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  \frac{TP}{TP+FP} \ P_+ &amp;amp; \frac{FN}{FN+TN} \ P_- \ \frac{FP}{TP+FP} \ P_+ &amp;amp; \frac{TN}{FN+TN} \ P_- \end{array} \right)$$&lt;/p&gt;
&lt;p&gt;In our dataset we have approximately 14% churn rate.  We can check that in the two extreams what will happen.   &lt;/p&gt;
&lt;p&gt;If our model predicts that everyone will churn, our confusion matrix and rate look like:&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  N_+ &amp;amp; N_- \ 0 &amp;amp; 0 \end{array} \right) =&amp;gt; \left( \begin{array}{cc}  1 &amp;amp; 1 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;Since our $P_+ = .14$ and our $P_- = 0.86$, we have the error rate that looks like:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  .14 &amp;amp; .86 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;The element wise multipication with our cost matrix gives:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  11.2 &amp;amp; -17.2 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;We sum all the elements of this matrix together to get the expected profit:&lt;/p&gt;
&lt;p&gt;$$E[\mbox{Profit}] = 11.2 - 17.2 + 0 + 0 = -6$$&lt;/p&gt;
&lt;p&gt;This is obviously a bad strategy.  If we look at the other extreme and guess no one will churn we get the following results&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  0 &amp;amp; 0 \ N+ &amp;amp; N- \end{array} \right) =&amp;gt; \left( \begin{array}{cc}  0 &amp;amp; 0 \ 1 &amp;amp; 1 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0 &amp;amp; 0 \  .14 &amp;amp; .86  \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;The element wise multipication with our cost matrix gives:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0 &amp;amp; 0 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;$$E[\mbox{Profit}] = 0 + 0 + 0 + 0 = 0$$ &lt;/p&gt;
&lt;p&gt;So we have two extreme predictions.  One where we lose money, and one where we do not make a profit.   The idea is that if we can make smart prediction about who will churn and who will not churn, we can target our spending in the right places.   That will allow us to maximize our profit.   The smarter the predictions in terms of this problem and population, the better the results.  &lt;/p&gt;
&lt;h2&gt;Smart Classifiers&lt;/h2&gt;
&lt;p&gt;We are going to make somem smart predictors.  By smart I mean genertic, untuned, machine learning algorithms.   We will try a Logistic Regression, Support Vector Machine, Random Forest, Gradient Boosting, and AdaBoost methods and compare their results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="n"&gt;smart_classifiers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because sklearn's confusion matrix calculation is in a different format that I was expecting, we built a helper function to calculate the rate and format it into the form we were expencting&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def confusion_rates(matrix):
    new = np.zeros(matrix.shape)
    new[0,0] = matrix[1,1]
    new[1,1] = matrix[0,0]
    new[1,0] = matrix[1,0]
    new[0,1] = matrix[0,1]
    return new.astype(float)/np.sum(new,axis=0)

confusion_rates(np.array([[94,6],[96,4]]))




array([[ 0.04,  0.06],
       [ 0.96,  0.94]])
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Profit Curves Theory&lt;/h2&gt;
&lt;p&gt;The idea behind the profit curves we are producing is that each classifier is fitted to training data.   Then test data is given to the classifier, the classifier predicts if it is a positive example or negative example.   The classifieres we are testing also estimate how strongly the algorithm believes each instance is a positive example (or negative example).    &lt;/p&gt;
&lt;p&gt;We can now take the strength of these believes and asked the question:  If we only take the test example with the strongest belief as a positive example, and guess that all other test examples as negative example, how profitable is this. We should get something close to zero in our churn example.&lt;/p&gt;
&lt;p&gt;We can then ask the question for the two strongest predictions, then the three strongest predictions, and so on.  We do this until we just guess everything is a positive example.   That would lead us back to the -6 dollar profits in our churn model.  &lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;$$truth = [1, \ 1, \ 0, \ 0, \ 1]$$
$$ \mbox{model 1} = [0.9, \ 0.4, \ 0.2, \ 0.8, \ 0.7] $$
$$\mbox{model 2} = [0.8, \ 0.7, \ 0.2, \ 0.3, \ 0.6] $$&lt;/p&gt;
&lt;p&gt;Model 1 and Model 2 have the same strongest predictor, so we would precict the following for our first question:&lt;/p&gt;
&lt;p&gt;$$ \mbox{model 1} = [1, \ 0, \ 0, \ 0, \ 0]$$
$$ \mbox{model 2} = [1, \ 0, \ 0, \ 0, \ 0]$$&lt;/p&gt;
&lt;p&gt;Leading to the confusion matrix:&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  1 &amp;amp; 0 \ 2 &amp;amp; 1 \end{array} \right) =&amp;gt; \left( \begin{array}{cc}  0.33 &amp;amp; 0 \ 0.67 &amp;amp; 1 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;We have the proportion of positive examples $P_+ = 0.6$ and $P_- = 0.4$.  That leads to the classification rates of:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0.2 &amp;amp; 0 \ 0.4 &amp;amp; 0.4 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;Using the cost matrix from the Data Science for Business School of 4 dollars profit for true positives and -5 dollars profit for false positives we get a profit matrix of:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0.8 &amp;amp; 0 \ 0 &amp;amp; 0 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;For if we only take the strongest predictor we expect an average profit of 0.8 per costomer for both models.&lt;/p&gt;
&lt;p&gt;$$E(\mbox{Profit}) = 0.8 + 0 + 0 + 0 = 0.8$$&lt;/p&gt;
&lt;p&gt;We can now ask the second question of what is the expected profit if we take two two strongest predictors.  In this case we have the following predecitions:&lt;/p&gt;
&lt;p&gt;$$ \mbox{model 1} = [1, \ 0, \ 0, \ 1, \ 0]$$
$$ \mbox{model 2} = [1, \ 1, \ 0, \ 0, \ 0]$$&lt;/p&gt;
&lt;p&gt;Now the two models make different predictions!&lt;/p&gt;
&lt;p&gt;The confusion matrix for model 1 (left) and model 2 (right) right are next. &lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  1 &amp;amp; 1 \ 2 &amp;amp; 1 \end{array} \right) \ ,  \ \left( \begin{array}{cc}  2 &amp;amp; 0 \ 1 &amp;amp; 2 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;The rate matrixes become:&lt;/p&gt;
&lt;p&gt;$$ \left( \begin{array}{cc}  0.33 &amp;amp; .5 \ .67 &amp;amp; .5 \end{array} \right) \ , \ \left( \begin{array}{cc}  0.67 &amp;amp; 0 \ 0.33 &amp;amp; 1 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;The population proportions have not changed. The proportion of positive examples $P_+ = 0.6$ and $P_- = 0.4$. &lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0.2 &amp;amp; 0.2 \ 0.4 &amp;amp; 0.2 \end{array} \right) \ , \ \left( \begin{array}{cc}  0.4 &amp;amp; 0 \ 0.2 &amp;amp; 0.4 \end{array} \right) $$&lt;/p&gt;
&lt;p&gt;That then makes the final profit matrix look like:&lt;/p&gt;
&lt;p&gt;$$\left( \begin{array}{cc}  0.8 &amp;amp; -1 \ 0 &amp;amp; 0 \end{array} \right) \ , \ \left( \begin{array}{cc}  1.6 &amp;amp; 0 \ 0 &amp;amp; 0 \end{array} \right)  $$&lt;/p&gt;
&lt;p&gt;Now there is a clear difference in the models if we take the two strongest predictors.&lt;/p&gt;
&lt;p&gt;$$E(\mbox{Profit Model 1}) = 0.8 + -1 + 0 + 0 = -0.2$$&lt;/p&gt;
&lt;p&gt;$$E(\mbox{Profit Model 2}) = 1.6 + 0 + 0 + 0 = 1.6$$&lt;/p&gt;
&lt;p&gt;Model 2 is much better model if we limit ourselves to the two strongest predictions.  To find the most profitable model, we should conditinue to do these calcuations.  Becausse they are so repetative, its time for some automation&lt;/p&gt;
&lt;h2&gt;Plotting Profit Curves&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;profit_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;classifiers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s-Atom"&gt;#split&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="s-Atom"&gt;into&lt;/span&gt; &lt;span class="s-Atom"&gt;a&lt;/span&gt; &lt;span class="s-Atom"&gt;training&lt;/span&gt; &lt;span class="s-Atom"&gt;and&lt;/span&gt; &lt;span class="s-Atom"&gt;test&lt;/span&gt; &lt;span class="s-Atom"&gt;set&lt;/span&gt;
    &lt;span class="s-Atom"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.35&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s-Atom"&gt;#get&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;true&lt;/span&gt; &lt;span class="s-Atom"&gt;proportions&lt;/span&gt; &lt;span class="s-Atom"&gt;in&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;test&lt;/span&gt; &lt;span class="s-Atom"&gt;set&lt;/span&gt;
    &lt;span class="s-Atom"&gt;p_pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s-Atom"&gt;p_neg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s-Atom"&gt;p_pos&lt;/span&gt;

    &lt;span class="s-Atom"&gt;#scale&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;training&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s-Atom"&gt;important&lt;/span&gt; &lt;span class="s-Atom"&gt;for&lt;/span&gt; &lt;span class="s-Atom"&gt;some&lt;/span&gt; &lt;span class="s-Atom"&gt;classifiers&lt;/span&gt; &lt;span class="s-Atom"&gt;and&lt;/span&gt; &lt;span class="s-Atom"&gt;to&lt;/span&gt; &lt;span class="s-Atom"&gt;get&lt;/span&gt; &lt;span class="s-Atom"&gt;consistent&lt;/span&gt; &lt;span class="s-Atom"&gt;results&lt;/span&gt;
    &lt;span class="s-Atom"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="s-Atom"&gt;xtrn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s-Atom"&gt;xtst&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s-Atom"&gt;for&lt;/span&gt; &lt;span class="s-Atom"&gt;c&lt;/span&gt; &lt;span class="s-Atom"&gt;in&lt;/span&gt; &lt;span class="nn"&gt;classifiers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s-Atom"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;xtrn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;xtst&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="s-Atom"&gt;#&lt;/span&gt;&lt;span class="nv"&gt;Get&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;indexes&lt;/span&gt; &lt;span class="s-Atom"&gt;of&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="s-Atom"&gt;most&lt;/span&gt; &lt;span class="s-Atom"&gt;likely&lt;/span&gt; &lt;span class="s-Atom"&gt;to&lt;/span&gt; &lt;span class="s-Atom"&gt;be&lt;/span&gt; &lt;span class="s-Atom"&gt;positive&lt;/span&gt;
        &lt;span class="s-Atom"&gt;indicies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s-Atom"&gt;::-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="s-Atom"&gt;costs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="s-Atom"&gt;x_axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="s-Atom"&gt;#&lt;/span&gt;&lt;span class="nv"&gt;For&lt;/span&gt; &lt;span class="s-Atom"&gt;each&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="s-Atom"&gt;point&lt;/span&gt;
        &lt;span class="s-Atom"&gt;for&lt;/span&gt; &lt;span class="s-Atom"&gt;i&lt;/span&gt; &lt;span class="s-Atom"&gt;in&lt;/span&gt; &lt;span class="nn"&gt;indicies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

            &lt;span class="s-Atom"&gt;#predict&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;all&lt;/span&gt; &lt;span class="s-Atom"&gt;probabilities&lt;/span&gt; &lt;span class="s-Atom"&gt;above&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;ith&lt;/span&gt; &lt;span class="s-Atom"&gt;strongest&lt;/span&gt; &lt;span class="s-Atom"&gt;predicters&lt;/span&gt; &lt;span class="o"&gt;is&lt;/span&gt; &lt;span class="s-Atom"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;else&lt;/span&gt; &lt;span class="s-Atom"&gt;negative&lt;/span&gt;
            &lt;span class="s-Atom"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="s-Atom"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s-Atom"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]).&lt;/span&gt;&lt;span class="nf"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="s-Atom"&gt;#calculate&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;confusion&lt;/span&gt; &lt;span class="s-Atom"&gt;matrix&lt;/span&gt; &lt;span class="s-Atom"&gt;for&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;predictions&lt;/span&gt;
            &lt;span class="s-Atom"&gt;matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;confusion_rates&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="s-Atom"&gt;#matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;nan_to_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="s-Atom"&gt;#calculate&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;cost&lt;/span&gt; &lt;span class="s-Atom"&gt;matrix&lt;/span&gt; &lt;span class="s-Atom"&gt;through&lt;/span&gt; &lt;span class="s-Atom"&gt;element&lt;/span&gt; &lt;span class="s-Atom"&gt;wise&lt;/span&gt; &lt;span class="s-Atom"&gt;product&lt;/span&gt; &lt;span class="s-Atom"&gt;and&lt;/span&gt; &lt;span class="s-Atom"&gt;sum&lt;/span&gt;
            &lt;span class="s-Atom"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s-Atom"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="s-Atom"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="s-Atom"&gt;p_pos&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s-Atom"&gt;p_neg&lt;/span&gt;&lt;span class="p"&gt;]])))&lt;/span&gt;

            &lt;span class="s-Atom"&gt;#append&lt;/span&gt; &lt;span class="s-Atom"&gt;the&lt;/span&gt; &lt;span class="s-Atom"&gt;cost&lt;/span&gt; &lt;span class="s-Atom"&gt;and&lt;/span&gt; &lt;span class="s-Atom"&gt;proportion&lt;/span&gt; &lt;span class="s-Atom"&gt;of&lt;/span&gt; &lt;span class="s-Atom"&gt;test&lt;/span&gt; &lt;span class="s-Atom"&gt;predictions&lt;/span&gt; &lt;span class="s-Atom"&gt;we&lt;/span&gt; &lt;span class="s-Atom"&gt;set&lt;/span&gt; &lt;span class="s-Atom"&gt;positive&lt;/span&gt;
            &lt;span class="s-Atom"&gt;x_axis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="s-Atom"&gt;costs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="s-Atom"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s-Atom"&gt;indicies&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;costs&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s-Atom"&gt;::-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]]&lt;/span&gt;

        &lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;x_axis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;costs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s-Atom"&gt;c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;__&lt;/span&gt;&lt;span class="s-Atom"&gt;class__&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;__&lt;/span&gt;&lt;span class="s-Atom"&gt;name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;



&lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;figsize=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nf"&gt;profit_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;smart_classifiers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;profit_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;&amp;#39;Expected Profit Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;&amp;#39;% Test Predictions Set Positive&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D5/output_14_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;The plots we have generated match our intuition we developed.  If we do not guess anyone churns (left), then we expect the profit to be zero.   If we guess everyone churns (right), we expect the profit to be -6.  The acutal value is different be the actual proportion in the test set is different from 14%.  Inbetween we are using smart classifier to predict who will churn and who will not churn.  Even the worst model leads to some profitability, which is promising for our cell phone company.   &lt;/p&gt;
&lt;p&gt;The best model, in this trial, is the GradientBoostingClassifier, followed closely by the Support Vector Machine and Randome Forest.  The peak profitability for all the models seem to be taking the 18% of strongest predictors in the models.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.35)

#Prediction Probabilities on the test set
probs = GradientBoostingClassifier().fit(x_train,y_train).predict_proba(x_test)[:,1]

#class probabilities ~ [.14,.86]
class_prob = np.array([sum(y_test).astype(float)/len(y_test), sum(y_test==0).astype(float)/len(y_test) ])

#get the cutoff for the tope 18%, 100-18 - 82
percentile = np.percentile(probs,82)
print &amp;quot;If probabibility is larger than this predict Churn:&amp;quot;, percentile
y_pred = (probs &amp;gt;= percentile).astype(int)
rates = confusion_rates(confusion_matrix(y_test,y_pred))

print &amp;quot;Predicted Profit Rate:&amp;quot;, np.sum(rates*class_prob*profit_matrix)

print &amp;quot;% Costomers Affected/Targed, %of Correct Costomers:&amp;quot;, np.sum((rates*class_prob)[0]),(rates*class_prob)[0,0]

If probabibility is larger than this predict Churn: 0.184275129582
Predicted Profit Rate: 6.51242502142
% Costomers Affected/Targed, %of Correct Costomers: 0.179948586118 0.101113967438
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For our gradient model we will predict a costomer to be likely churn if their prediciton probability is above 0.18, and we have expected profit rate of $6.5/prediction.   We estimate that we will target approximately 18% of our costomers using this model, with 10% of them being the group we want to target.&lt;/p&gt;
&lt;p&gt;If we have an unlimited budget, or a budget larger than the cost of targeting every costomer, then we would choose this model.  &lt;/p&gt;
&lt;p&gt;If we have a limited budget that want to be profitable (near term) but are willing to not persue maximal profit to reach the largest number of customers and reduce churn, then we will want a differnet strategy.  Because the cost of targeting a customer is fixed, we will want to optimize precision by moving to the left of the graph.  In our case of the cell phone churn, the the top models move in lock-step.   &lt;/p&gt;
&lt;p&gt;We would just increase the threshold for our GradientBoostedClassifier.  If we increase the threshold to be the top 10% of predictions instead of the top 18%, we get reduce profitability but increase targeting rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;#Prediction Probabilities on the test set
probs = GradientBoostingClassifier().fit(x_train,y_train).predict_proba(x_test)[:,1]

#class probabilities ~ [.14,.86]
class_prob = np.array([sum(y_test).astype(float)/len(y_test), sum(y_test==0).astype(float)/len(y_test) ])

#get the cutoff for the tope 18%, 100-18 - 82
percentile = np.percentile(probs,92)
print &amp;quot;If probabibility is larger than this predict Churn:&amp;quot;, percentile
y_pred = (probs &amp;gt;= percentile).astype(int)
rates = confusion_rates(confusion_matrix(y_test,y_pred))

print &amp;quot;Predicted Profit Rate:&amp;quot;, np.sum(rates*class_prob*profit_matrix)

print &amp;quot;% Costomers Affected/Targed, %of Correct Costomers:&amp;quot;, np.sum((rates*class_prob)[0]),(rates*class_prob)[0,0]

If probabibility is larger than this predict Churn: 0.5623562992
Predicted Profit Rate: 5.8440445587
% Costomers Affected/Targed, %of Correct Costomers: 0.0805484147386 0.0745501285347
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, our expected profit drops to 5.8 from 6.5, and our threshold is increased to 0.56.   What we like to see is now we are only targeting 8% of our customers, but 7.5% of them are the ones we want to target.  That takes our precision to approximately 93% from 56%.  We are not getting the most profit, but we are spending our money in the most targeted way in this campaign.  If we know what percentage of our customers we can target with our budget, we can move down the curve and clacluate our expected results.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;10.1/17.99, 0.07455/0.08055




(0.5614230127848805, 0.9255121042830541)
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="Profit Curves"></category></entry><entry><title>Galvanize - Week 04 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-04/" rel="alternate"></link><updated>2015-06-25T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-25:galvanize/galvanize-data-science-04-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 4 - Day 4&lt;/h2&gt;
&lt;p&gt;Our quiz today had to do with the birthday problem and another problem that involved two hunting hounds.  The question was if there are two hunting hounds that successfully track with a probability p, is the strategy of following both hounds if they go in the same direction on a fork in the round, otherwise randomly guessing, better then just following 1 hound?  &lt;/p&gt;
&lt;p&gt;The probability of both hounds being successful and matching is $p^2$ and the probability of both hounds matching and being unsuccessful is $(1-p)^2$.   The probability of not matching is $2p(1-p)$, and half of each time the hunter will randomly pick correct.  The exepected odds of success is $p^2 + p(1-p) = p^2 + p - p^2 = p$, the same as following one hound.   When I first read the problem I was not expecting that solution.   I like when I see something I was not expected!&lt;/p&gt;
&lt;h2&gt;Morning Boosting&lt;/h2&gt;
&lt;p&gt;This morning we discussed boosting, and our morning sprint was to predict the bosting house prices using boosting on regression classifieres in &lt;a href="http://scikit-learn.org/stable/"&gt;sklearn&lt;/a&gt;.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeRegressor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestRegressor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdaBoostRegressor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r2_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble.partial_dependence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_partial_dependence&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;# House Prices&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="c"&gt;# The other 13 features&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="c"&gt;#train and test set&lt;/span&gt;
&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_1_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;For this sprint we really are not concerned with the distributions, but I still like to plot.   We see the house prices targets vary from 5 to 50.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pd.Series(y).describe()




count    506.000000
mean      22.532806
std        9.197104
min        5.000000
25%       17.025000
50%       21.200000
75%       25.000000
max       50.000000
dtype: float64




print &amp;quot;MSE From Average: &amp;quot;, np.sum(np.power(y-y.mean(),2))/len(y)

MSE From Average:  84.4195561562
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have some baselines, we can now start to train our regressors and compare thier performance.  For this first trail I am going to make a Random Forest, GradientBoostingRegressor, and a AdaBoostRegressor.   I will be comparing the cross validated MSE and $r^2$ on the training set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;gdbr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                 &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ls&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                 &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                 &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;abr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdaBoostRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeRegressor&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                        &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;r2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;r2_score&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;gdbr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gdbr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gdbr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;abr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;



&lt;span class="n"&gt;RandomForestRegressor&lt;/span&gt; &lt;span class="mf"&gt;11.6404925079&lt;/span&gt; &lt;span class="mf"&gt;0.848311635198&lt;/span&gt;
&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt; &lt;span class="mf"&gt;9.78445216743&lt;/span&gt; &lt;span class="mf"&gt;0.871926100237&lt;/span&gt;
&lt;span class="n"&gt;AdaBoostRegressor&lt;/span&gt; &lt;span class="mf"&gt;11.757802439&lt;/span&gt; &lt;span class="mf"&gt;0.846590661377&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The 10 fold cross validation on the training set gives MSE of order 10, much smaller than the naive estimate of 80.   All three of these models are doing well on the training set.   I would not pick one model over until I have tested them on the hold out set.  We can plot the performance of these models as we trained them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mse_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;staged_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;mse_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;staged_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; {} - Learning Rate &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mse_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mse_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Iterations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Mean Square Error&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gdbr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_8_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the Gradient Boosting Regressor we see that as we add more weak learners/iterations, the training and test error drop together.  The Training error is still dropping, but the test error has leveled off around 8.  This result is also affected by the learning rate.  If we change it we get different results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,10))
stage_score_plot(GradientBoostingRegressor(learning_rate=.1,loss=&amp;#39;ls&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
stage_score_plot(GradientBoostingRegressor(learning_rate=1,loss=&amp;#39;ls&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_10_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The higher learning rate leads to over fitting.   The training error goes to zero almost immediately, but the error on the test set is very high.&lt;/p&gt;
&lt;p&gt;We can also lower the learning rate.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(16,8))
plt.subplot(1,2,1)
stage_score_plot(GradientBoostingRegressor(learning_rate=.1,loss=&amp;#39;ls&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
stage_score_plot(GradientBoostingRegressor(learning_rate=.01,loss=&amp;#39;ls&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
plt.legend()
plt.subplot(1,2,2)
stage_score_plot(GradientBoostingRegressor(learning_rate=.1,loss=&amp;#39;ls&amp;#39;, n_estimators=1000, random_state=1), x_trn, y_trn, x_test, y_test)
stage_score_plot(GradientBoostingRegressor(learning_rate=.01,loss=&amp;#39;ls&amp;#39;, n_estimators=1000, random_state=1), x_trn, y_trn, x_test, y_test)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_12_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case we see that the test error levels off to the same place for these two rates, but the lower learning rate takes more iterations to get there.&lt;/p&gt;
&lt;p&gt;We can also compare the results of the gradient boosting to the random forest algorithm.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,10))
stage_score_plot(GradientBoostingRegressor(learning_rate=.1,loss=&amp;#39;ls&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
plt.axhline(y=mean_squared_error(rf.fit(x_trn,y_trn).predict(x_test),y_test),color=&amp;#39;orange&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Random Forest Test&amp;#39;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_15_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The random forest does not have the stage predict function that allows you to retroactively calculate the predictions at each stage of the training.   The end result is that for the same number of estimators/iterations, the Gradient Boosting Regressor does better on the Boston dataset.  &lt;/p&gt;
&lt;p&gt;We can also look at the AdaBoostingRegessor because it does have stage predict.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,10))
plt.subplot(1,2,1)
stage_score_plot(AdaBoostRegressor(learning_rate=1,loss=&amp;#39;linear&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
stage_score_plot(AdaBoostRegressor(learning_rate=.1,loss=&amp;#39;linear&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
#stage_score_plot(AdaBoostRegressor(learning_rate=.01,loss=&amp;#39;linear&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
plt.axhline(y=mean_squared_error(rf.fit(x_trn,y_trn).predict(x_test),y_test),color=&amp;#39;orange&amp;#39;,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Random Forest&amp;#39;)
plt.legend()
plt.subplot(1,2,2)
stage_score_plot(AdaBoostRegressor(learning_rate=1,loss=&amp;#39;linear&amp;#39;, n_estimators=1000, random_state=1), x_trn, y_trn, x_test, y_test)
stage_score_plot(AdaBoostRegressor(learning_rate=.1,loss=&amp;#39;linear&amp;#39;, n_estimators=1000, random_state=1), x_trn, y_trn, x_test, y_test)
#stage_score_plot(AdaBoostRegressor(learning_rate=.01,loss=&amp;#39;linear&amp;#39;, n_estimators=100, random_state=1), x_trn, y_trn, x_test, y_test)
plt.axhline(y=mean_squared_error(rf.fit(x_trn,y_trn).predict(x_test),y_test),color=&amp;#39;orange&amp;#39;,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Random Forest&amp;#39;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_17_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case the AdaBoostRegressor does not do better than RandomForest on the test set.  Even allowing for more iterations (which takes a fair amoutn of time to fit).   We are using the naive parameters to fit the model.  We should really search for the best parameters in each model.&lt;/p&gt;
&lt;h2&gt;Grid Search&lt;/h2&gt;
&lt;p&gt;The goal of grid searching is to fit the model using different parameters, and choose the result that has the best cross validated score.   This is not guaranteed to give the best results, but currently I do not know a better way to tune a model.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;random_forest_grid = {&amp;#39;max_depth&amp;#39;: [3, None],
                      &amp;#39;max_features&amp;#39;: [&amp;#39;sqrt&amp;#39;, &amp;#39;log2&amp;#39;, None],
                      &amp;#39;min_samples_split&amp;#39;: [1, 2, 4],
                      &amp;#39;min_samples_leaf&amp;#39;: [1, 2, 4],
                      &amp;#39;bootstrap&amp;#39;: [True, False],
                      &amp;#39;n_estimators&amp;#39;: [40, 80, 160, 320],
                      &amp;#39;random_state&amp;#39;: [1]}

rf_gridsearch = GridSearchCV(RandomForestRegressor(),
                             random_forest_grid,
                             n_jobs=-1,
                             verbose=True,
                             scoring=&amp;#39;mean_squared_error&amp;#39;)
rf_gridsearch.fit(x_trn, y_trn)

print &amp;quot;best parameters:&amp;quot;, rf_gridsearch.best_params_
print &amp;quot;best score:&amp;quot;,rf_gridsearch.best_score_
best_rf_model = rf_gridsearch.best_estimator_


Fitting 3 folds for each of 432 candidates, totalling 1296 fits


[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s
[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    1.9s
[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    8.2s
[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   19.9s
[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   42.4s
[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  1.2min
[Parallel(n_jobs=-1)]: Done 1296 out of 1296 | elapsed:  1.3min finished


best parameters: {&amp;#39;bootstrap&amp;#39;: True, &amp;#39;min_samples_leaf&amp;#39;: 1, &amp;#39;n_estimators&amp;#39;: 40, &amp;#39;min_samples_split&amp;#39;: 1, &amp;#39;random_state&amp;#39;: 1, &amp;#39;max_features&amp;#39;: &amp;#39;sqrt&amp;#39;, &amp;#39;max_depth&amp;#39;: None}
best score: -14.0843113088
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see the MSE is negative, but that is an artifact of the fit used by sklearn.  The MSE is just the absolute value of that parameter.   We can use the best model from the search and get a feel for the results on the test set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mean_squared_error(best_rf_model.predict(x_test),y_test),mean_squared_error(RandomForestRegressor().fit(x_trn,y_trn).predict(x_test),y_test)




(12.737843504901965, 9.5049568627450984)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our tuned random forest did worse on the test set than our untuned model.  We would need to estimate the uncertainty of the MSE of both classifiers to get a feel for if this is a statisically significant difference. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;gb_grid = {&amp;#39;learning_rate&amp;#39;: [1,0.1,0.01],
                      &amp;#39;max_depth&amp;#39;: [2,4,6],
                      &amp;#39;min_samples_leaf&amp;#39;: [1, 2, 4],
                      &amp;#39;n_estimators&amp;#39;: [20, 40, 80, 160],
                      &amp;#39;max_features&amp;#39;: [&amp;#39;sqrt&amp;#39;,&amp;#39;log2&amp;#39;,None],
                      &amp;#39;random_state&amp;#39;: [1]}

gb_gridsearch = GridSearchCV(GradientBoostingRegressor(),
                             gb_grid,
                             n_jobs=-1,
                             verbose=True,
                             scoring=&amp;#39;mean_squared_error&amp;#39;)
gb_gridsearch.fit(x_trn, y_trn)

print &amp;quot;best parameters:&amp;quot;, gb_gridsearch.best_params_
print &amp;quot;best score:&amp;quot;, gb_gridsearch.best_score_
best_gb_model = gb_gridsearch.best_estimator_


Fitting 3 folds for each of 324 candidates, totalling 972 fits


[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s
[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    0.3s
[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    1.8s
[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:    4.7s
[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   10.7s
[Parallel(n_jobs=-1)]: Done 972 out of 972 | elapsed:   13.9s finished


best parameters: {&amp;#39;learning_rate&amp;#39;: 0.1, &amp;#39;min_samples_leaf&amp;#39;: 4, &amp;#39;n_estimators&amp;#39;: 160, &amp;#39;random_state&amp;#39;: 1, &amp;#39;max_features&amp;#39;: &amp;#39;sqrt&amp;#39;, &amp;#39;max_depth&amp;#39;: 4}
best score: -13.2445739497



mean_squared_error(best_gb_model.predict(x_test),y_test),mean_squared_error(GradientBoostingRegressor().fit(x_trn,y_trn).predict(x_test),y_test)




(10.363886409362564, 6.8593273937564954)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see a similar result in the Gradient Boosting Regressor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ada_grid = {&amp;#39;base_estimator&amp;#39;: [best_gb_model,best_rf_model],
            &amp;#39;learning_rate&amp;#39;: [1,0.1,0.01],
            &amp;#39;n_estimators&amp;#39;: [20, 40, 80, 160],
            &amp;#39;random_state&amp;#39;: [1]}

ada_gridsearch = GridSearchCV(AdaBoostRegressor(),
                             ada_grid,
                             n_jobs=-1,
                             verbose=True,
                             scoring=&amp;#39;mean_squared_error&amp;#39;)
ada_gridsearch.fit(x_trn, y_trn)

print &amp;quot;best parameters:&amp;quot;, ada_gridsearch.best_params_
print &amp;quot;best score:&amp;quot;, ada_gridsearch.best_score_
best_ada_model = ada_gridsearch.best_estimator_


Fitting 3 folds for each of 24 candidates, totalling 72 fits


[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.6s
[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  1.0min
[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  1.6min finished


best parameters: {&amp;#39;n_estimators&amp;#39;: 20, &amp;#39;base_estimator&amp;#39;: GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss=&amp;#39;ls&amp;#39;,
             max_depth=4, max_features=&amp;#39;sqrt&amp;#39;, max_leaf_nodes=None,
             min_samples_leaf=4, min_samples_split=2,
             min_weight_fraction_leaf=0.0, n_estimators=160,
             random_state=1, subsample=1.0, verbose=0, warm_start=False), &amp;#39;random_state&amp;#39;: 1, &amp;#39;learning_rate&amp;#39;: 0.1}
best score: -12.8839320317



mean_squared_error(best_ada_model.predict(x_test),y_test),mean_squared_error(AdaBoostRegressor().fit(x_trn,y_trn).predict(x_test),y_test)




(11.267204239234372, 12.438897032159721)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The AdaBoostRegressor did improve over the default values, but in the end they all gave around the same MSE on the test set using the Bosting Housing Data.  &lt;/p&gt;
&lt;h1&gt;Afternoon - AdaBoost&lt;/h1&gt;
&lt;p&gt;We started out the afternoon buliding our own AdaBoost Classification Algorithm, then we explored using sklearn's implementation to explore partial dependency plots.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.base&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;AdaBoostBinaryClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;    - n_estimator (int)&lt;/span&gt;
&lt;span class="sd"&gt;      * The number of estimators to use in boosting&lt;/span&gt;
&lt;span class="sd"&gt;      * Default: 50&lt;/span&gt;

&lt;span class="sd"&gt;    - learning_rate (float)&lt;/span&gt;
&lt;span class="sd"&gt;      * Determines how fast the error would shrink&lt;/span&gt;
&lt;span class="sd"&gt;      * Lower learning rate means more accurate decision boundary,&lt;/span&gt;
&lt;span class="sd"&gt;        but slower to converge&lt;/span&gt;
&lt;span class="sd"&gt;      * Default: 1&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;base_estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_estimators&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;

        &lt;span class="c"&gt;# Will be filled-in in the fit() step&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimators_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator_weight_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_estimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - x: 2d numpy array, feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;        - y: numpy array, labels&lt;/span&gt;

&lt;span class="sd"&gt;        Build the estimators for the AdaBoost estimator.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_estimator&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_boost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimators_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator_weight_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="c"&gt;#print temp&lt;/span&gt;
        &lt;span class="c"&gt;#return temp&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_boost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - x: 2d numpy array, feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;        - y: numpy array, labels&lt;/span&gt;
&lt;span class="sd"&gt;        - sample_weight: numpy array&lt;/span&gt;

&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - estimator: DecisionTreeClassifier&lt;/span&gt;
&lt;span class="sd"&gt;        - sample_weight: numpy array (updated weights)&lt;/span&gt;
&lt;span class="sd"&gt;        - estimator_weight: float (weight of estimator)&lt;/span&gt;

&lt;span class="sd"&gt;        Go through one iteration of the AdaBoost algorithm. Build one estimator.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;base_estimator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ypred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;err_m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;log&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;err_m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;err_m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


        &lt;span class="n"&gt;yp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample_weight&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - x: 2d numpy array, feature matrix&lt;/span&gt;

&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - labels: numpy array of predictions (0 or 1)&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimators_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator_weight_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c"&gt;#pred += self.estimator_weight_[i]*estimator.predict(x)&lt;/span&gt;

        &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;



    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - x: 2d numpy array, feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;        - y: numpy array, labels&lt;/span&gt;

&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;        - score: float (accuracy score between 0 and 1)&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above is our AdaBoost Classifory class.  We will be using it on spam data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;&amp;#39;boosting/data/spam.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;delimiter=&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="s-Atom"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="s-Atom"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;test_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="s-Atom"&gt;my_ada&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;AdaBoostBinaryClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;my_ada&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;my_ada&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;Accuracy&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="p"&gt;...,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="mf"&gt;0.917463075586&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our out of the box score is around 92% accuracy.   We will be exploring feature importance using sklearn's implementation, so I will read in the naems from the clipboard.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df = pd.read_clipboard()
names = df.values[:,0]
names




array([&amp;#39;word_freq_make:&amp;#39;, &amp;#39;word_freq_address:&amp;#39;, &amp;#39;word_freq_all:&amp;#39;,
       &amp;#39;word_freq_3d:&amp;#39;, &amp;#39;word_freq_our:&amp;#39;, &amp;#39;word_freq_over:&amp;#39;,
       &amp;#39;word_freq_remove:&amp;#39;, &amp;#39;word_freq_internet:&amp;#39;, &amp;#39;word_freq_order:&amp;#39;,
       &amp;#39;word_freq_mail:&amp;#39;, &amp;#39;word_freq_receive:&amp;#39;, &amp;#39;word_freq_will:&amp;#39;,
       &amp;#39;word_freq_people:&amp;#39;, &amp;#39;word_freq_report:&amp;#39;, &amp;#39;word_freq_addresses:&amp;#39;,
       &amp;#39;word_freq_free:&amp;#39;, &amp;#39;word_freq_business:&amp;#39;, &amp;#39;word_freq_email:&amp;#39;,
       &amp;#39;word_freq_you:&amp;#39;, &amp;#39;word_freq_credit:&amp;#39;, &amp;#39;word_freq_your:&amp;#39;,
       &amp;#39;word_freq_font:&amp;#39;, &amp;#39;word_freq_000:&amp;#39;, &amp;#39;word_freq_money:&amp;#39;,
       &amp;#39;word_freq_hp:&amp;#39;, &amp;#39;word_freq_hpl:&amp;#39;, &amp;#39;word_freq_george:&amp;#39;,
       &amp;#39;word_freq_650:&amp;#39;, &amp;#39;word_freq_lab:&amp;#39;, &amp;#39;word_freq_labs:&amp;#39;,
       &amp;#39;word_freq_telnet:&amp;#39;, &amp;#39;word_freq_857:&amp;#39;, &amp;#39;word_freq_data:&amp;#39;,
       &amp;#39;word_freq_415:&amp;#39;, &amp;#39;word_freq_85:&amp;#39;, &amp;#39;word_freq_technology:&amp;#39;,
       &amp;#39;word_freq_1999:&amp;#39;, &amp;#39;word_freq_parts:&amp;#39;, &amp;#39;word_freq_pm:&amp;#39;,
       &amp;#39;word_freq_direct:&amp;#39;, &amp;#39;word_freq_cs:&amp;#39;, &amp;#39;word_freq_meeting:&amp;#39;,
       &amp;#39;word_freq_original:&amp;#39;, &amp;#39;word_freq_project:&amp;#39;, &amp;#39;word_freq_re:&amp;#39;,
       &amp;#39;word_freq_edu:&amp;#39;, &amp;#39;word_freq_table:&amp;#39;, &amp;#39;word_freq_conference:&amp;#39;,
       &amp;#39;char_freq_;:&amp;#39;, &amp;#39;char_freq_(:&amp;#39;, &amp;#39;char_freq_[:&amp;#39;, &amp;#39;char_freq_!:&amp;#39;,
       &amp;#39;char_freq_$:&amp;#39;, &amp;#39;char_freq_#:&amp;#39;, &amp;#39;capital_run_length_average:&amp;#39;,
       &amp;#39;capital_run_length_longest:&amp;#39;, &amp;#39;capital_run_length_total:&amp;#39;], dtype=object)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To explore this we want to get a feel for the misclassification error of databoost.  We redefined our plot function to score misclassification instead of MSE.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;acc_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;staged_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;acc_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;staged_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; {} - Learning Rate &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;acc_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;acc_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Iterations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Misclassification&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stage_score_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_36_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The top plot shows AdaBoost vs GradientBoosting with differnt max depths set.   The Ada performas better on the test set.  As we increase the depth of the trees used in the GradientBoosting, we see in the bottom plot the over fitting is abundant.   We need to have week learners to get optimal results with this method.  A strong learner will still overfit the data when boosted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;

&lt;span class="n"&gt;gb_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;n_estimators&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sqrt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;log2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;random_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;

&lt;span class="n"&gt;gb_gridsearch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                             &lt;span class="n"&gt;gb_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gb_gridsearch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;best parameters:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gb_gridsearch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;best score:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gb_gridsearch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_score_&lt;/span&gt;

&lt;span class="n"&gt;best_gb_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gb_gridsearch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Fitting&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;folds&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;108&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;totalling&lt;/span&gt; &lt;span class="mi"&gt;324&lt;/span&gt; &lt;span class="n"&gt;fits&lt;/span&gt;


&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt; &lt;span class="n"&gt;Done&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;jobs&lt;/span&gt;       &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;    &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt; &lt;span class="n"&gt;Done&lt;/span&gt;  &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="n"&gt;jobs&lt;/span&gt;       &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;    &lt;span class="mf"&gt;8.1&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt; &lt;span class="n"&gt;Done&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="n"&gt;jobs&lt;/span&gt;       &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt; &lt;span class="n"&gt;Done&lt;/span&gt; &lt;span class="mi"&gt;318&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;324&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt; &lt;span class="n"&gt;remaining&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;    &lt;span class="mf"&gt;1.7&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt; &lt;span class="n"&gt;Done&lt;/span&gt; &lt;span class="mi"&gt;324&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;324&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.6&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt; &lt;span class="n"&gt;finished&lt;/span&gt;


&lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;log2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;n_estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;random_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.952463768116&lt;/span&gt;





&lt;span class="mf"&gt;0.95221546481320596&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is a good improvement over the previous model.   With GradientBoosting, we cal now explore which features are the most important features for identifying spam.&lt;/p&gt;
&lt;h2&gt;Feature Importance&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;indexes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_importances_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;x_ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_importances_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;barh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_ind&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_importances_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;center&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_ind&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_ind&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_ind&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_40_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that use of excessive capitalizaiton and expclaimations are important for predicting spam.   The use of 'cs' or 'telnet' are not.   The use of a partial dependency plot allows us to get a feel for how changing the values of these features affect the outcome of the predictions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble.partial_dependence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_partial_dependence&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble.partial_dependence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial_dependence&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial_dependence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid_resolution&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c"&gt;#plot_partial_dependence(best_gb_model, train_x,indexes[:12],feature_names=names,n_jobs=3, grid_resolution=50,ax=ax)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bbox_to_anchor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.005&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;As we increase the freq_lab or freq_address, we have a incrasing and decreasing accuracy of spam classification.  The parital dependency on these features is high.  Lab and telenet are also more than most, which is interesting because telent is low feature importance. &lt;/p&gt;
&lt;p&gt;It is possible to make 2D plots and 3D plots of partial dependancies.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble.partial_dependence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial_dependence&lt;/span&gt;

&lt;span class="n"&gt;couple_of_tuples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;plot_partial_dependence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_gb_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;couple_of_tuples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid_resolution&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D4/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;From these plots we can see that word-frequency has co-dependency with freq data and freq_parts &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="Boosting"></category><category term="AdaBoost"></category><category term="GradientBoosting"></category><category term="machines"></category></entry><entry><title>Galvanize - Week 04 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-03/" rel="alternate"></link><updated>2015-06-24T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-24:galvanize/galvanize-data-science-04-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 4 - Day 3&lt;/h2&gt;
&lt;p&gt;Our quiz toda was about making change.  Given a sufficient amount US coins what is the minimum number of coins needed to give change for a specificied amount?   &lt;/p&gt;
&lt;p&gt;We were suppose to build a function to do this.  My solution was to sort the list of coins from largest to smallest.   We then make the maximum amount of change using the largest denomination, then continue this until we have given change back.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def find_change(coins,value):
    coins.sort(reverse=True)
    n = 0
    v = value
    for x in coins:
        m =  v / x
        n += m
        v = v - m*x       
    return n

coins = [1,5,10,25]
print &amp;quot;Correct Answer 4, Your Answer: &amp;quot;, find_change(coins,100)
print &amp;quot;Correct Answer 8, Your Answer: &amp;quot;,find_change(coins,74)

Correct Answer 4, Your Answer:  4
Correct Answer 8, Your Answer:  8
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After the quiz we had a lecture on Support Vector Machines.  The afternoon lecture was on kernel tricks for SVMs.   &lt;/p&gt;
&lt;h2&gt;Morning: Maximal Margin Classifier&lt;/h2&gt;
&lt;p&gt;We learned that a support vectore machine is a maximum margin classifier, trying to construct a hyper plane that maximize the margin of linearly seperable data.   &lt;/p&gt;
&lt;p&gt;To help get a feel between SVMs and other classifiers we looked up some made up data about the number of hours emailing and number of hours spent at the gym for a people labeled as a data scientist or not a data scientist.&lt;/p&gt;
&lt;p&gt;This dataset is special in that it is linearly seperable and easily displayed in two dimensions&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/data_scientist.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Not Data Scientist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Data Scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Hours Emailing&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Hours At Gym&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/(output_3_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Margin of Logistic Regression Boundary&lt;/h2&gt;
&lt;p&gt;We learned previously that logistic regression minimizes the log-loss function:&lt;/p&gt;
&lt;p&gt;$$ - \ \sum_{i=1}^m [ \ y_i \ log(h_\theta (x_i)) \ + \ (1-y_i) \ log(1-h_\theta (x_i)) \ ]$$&lt;/p&gt;
&lt;p&gt;Where $$h_\theta(x) = \frac{1}{1+e^{x\theta}}$$.  &lt;/p&gt;
&lt;p&gt;This is different that explicitly maximizing the margin of a decision boundary.  &lt;/p&gt;
&lt;p&gt;We can fit a logistic regression model on our data that has 100% accuracy.  I have plotted the theta dotted the data on the x-axis, and the data scientist status on the y-axis.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data_scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Not Data Scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data_scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Data Scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;zp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;yp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;zp&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Logistic Fit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_5_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Fun Fact!&lt;/h3&gt;
&lt;p&gt;I origianlly fit this without regulariaiton, but the algorithm was converging before finding the optimal solution.  I added a regularization term to make a better (100%) fit to the classification.  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Write a function to compute and plot the decision boundary. Remember &lt;code&gt;y&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt; at the decision boundary when
   the probability of a positive class is &lt;code&gt;0.5&lt;/code&gt;. You should also define a range over one of your features (&lt;code&gt;gym_hours&lt;/code&gt;
   for example) and compute the &lt;code&gt;email_hours&lt;/code&gt; at the decision boundary.&lt;/p&gt;
&lt;p&gt;x = np.linspace(0,50,100)
y = np.linspace(0,50,100)
xx, yy = np.meshgrid(x,y)
Z = lin.predict(np.c_[xx.ravel(), yy.ravel()])
zz = Z.reshape(xx.shape)&lt;/p&gt;
&lt;h1&gt;zz = 1/(1+np.exp(xx&lt;em&gt;lin.coef_[0,0]+yy&lt;/em&gt;lin.coef_[0,1]+lin.intercept_))&lt;/h1&gt;
&lt;p&gt;extent = [0,50,0,50]
plt.figure(figsize=(14,8))
plt.pcolormesh(xx, yy, zz, cmap='winter',alpha=0.1, ) #plt.cm.Paired)&lt;/p&gt;
&lt;p&gt;ax = plt.subplot(1,1,1)&lt;/p&gt;
&lt;h1&gt;b = (0.5 - lin.intercept_)/lin.coef_[0,1]&lt;/h1&gt;
&lt;h1&gt;m = -lin.coef_[0,0]/lin.coef_[0,1]&lt;/h1&gt;
&lt;h1&gt;plt.plot(x,m*x+b,linestyle='--',color='black')&lt;/h1&gt;
&lt;p&gt;df[df.data_scientist==0].plot(kind='scatter',x='email_hours',y='gym_hours',color='steelblue',s=100,label='Not Data Scientist',alpha=0.8,ax=ax)
df[df.data_scientist==1].plot(kind='scatter',x='email_hours',y='gym_hours',color='seagreen',s=100,label='Data Scientist',alpha=0.8, ax=ax)
plt.xlim([0,50])
plt.ylim([0,50])
plt.xlabel('Hours Emailing')
plt.ylabel('Hours At Gym')
plt.legend(loc=2)
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_8_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The distance each point is from the margin is given by the following equations &lt;/p&gt;
&lt;p&gt;$$\mbox{distance} = \frac{\beta_0+\beta x^T}{||\beta||}$$&lt;/p&gt;
&lt;p&gt;A line perpendicular to a line will always have a slope that is $\frac{1}{\mbox{slope}}.  We can use this equation and make our plot to illustrate the distance from the margin using the size property.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def distance(x,slopes,intercept):
    return (intercept+x.dot(slopes))/np.sqrt(intercept**2+slopes.T.dot(slopes))

df[&amp;#39;s&amp;#39;] = np.abs(250*distance(df[[&amp;#39;email_hours&amp;#39;,&amp;#39;gym_hours&amp;#39;]].values,lin.coef_.T,lin.intercept_))
x = np.linspace(0,50,100)
y = np.linspace(0,50,100)
xx, yy = np.meshgrid(x,y)
Z = lin.predict(np.c_[xx.ravel(), yy.ravel()])
zz = Z.reshape(xx.shape)
#zz = 1/(1+np.exp(xx*lin.coef_[0,0]+yy*lin.coef_[0,1]+lin.intercept_)) 
extent = [0,50,0,50]
plt.figure(figsize=(14,8))
plt.pcolormesh(xx, yy, zz, cmap=&amp;#39;winter&amp;#39;,alpha=0.1, ) #plt.cm.Paired)

ax = plt.subplot(1,1,1)
#b = (0.5 - lin.intercept_)/lin.coef_[0,1]
#m = -lin.coef_[0,0]/lin.coef_[0,1]
#plt.plot(x,m*x+b,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;black&amp;#39;)
df[df.data_scientist==0].plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;email_hours&amp;#39;,
                              y=&amp;#39;gym_hours&amp;#39;,color=&amp;#39;steelblue&amp;#39;,
                              s=df[df.data_scientist==0].s,
                              label=&amp;#39;Not Data Scientist&amp;#39;,
                              alpha=0.8,ax=ax)
df[df.data_scientist==1].plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;email_hours&amp;#39;,
                              y=&amp;#39;gym_hours&amp;#39;,color=&amp;#39;seagreen&amp;#39;,
                              s=df[df.data_scientist==1].s,
                              label=&amp;#39;Data Scientist&amp;#39;,alpha=0.8, 
                              ax=ax)
plt.xlim([0,50])
plt.ylim([0,50])
plt.xlabel(&amp;#39;Hours Emailing&amp;#39;)
plt.ylabel(&amp;#39;Hours At Gym&amp;#39;)
plt.legend(loc=2)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_11_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Margin of Support Vector Machines&lt;/h2&gt;
&lt;p&gt;We learned that the SVM is a maximal margin classifier which in theory would have a larger margin than Logistic Regression. We will go through the same process that we just did for logistic regression.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;

&lt;span class="n"&gt;svc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="mf"&gt;1.0&lt;/span&gt;




&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;zz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()],&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;zz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;span class="n"&gt;zz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;winter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;#plt.cm.Paired)&lt;/span&gt;


&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Not Data Scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;email_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gym_hours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_scientist&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Data Scientist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                              &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Hours Emailing&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Hours At Gym&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_14_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The SVM classifier seems to match my intuition for what the optimal boundary should be, and the logistic regression did not capture that.  If we had a new data point at 10 hours of emailing and 15 hours at the gym, I would expect that person to be a data scientist because it is closer to the cluster of data scientists.  The logistic regression fit we did previously would have classifed it as a non-data scientist.  This is despit it is so far from the cluster.&lt;/p&gt;
&lt;p&gt;Just because this is my intuition does not mean that its correct.   There are problems that this intuition is incorrect.   That is why you might have some insight into the problem because picking a classifier.  That is also why you cross validate and test on a unseen test set.   &lt;/p&gt;
&lt;h2&gt;Scaling&lt;/h2&gt;
&lt;p&gt;We just worked a problem where the scaling of the two variables are the same, but if they are not this will mess with the results the svm will produce.   The distance measurements change with units.  It is standard practice to scale variables before fitting an SVM on a data set.   If we do not, we can get different results.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/non_sep.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scaler&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;svc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;named_steps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;xvals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;yvals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;xvals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;span class="n"&gt;zz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;zz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;BrBG&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;burlywood&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;0&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_16_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case the data is not linearly seperable.  You will have noticed I change the colors from the previous spot to values that contrast better.  That way you can see when values are misclassified more easily.  The overlap is not very much in this instance, so we can still git fair accuracy.   A 5-Fold cross validation shows above 90% accuracy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="mf"&gt;0.93000000000000005&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Tuning SVMs&lt;/h2&gt;
&lt;p&gt;The SVM has a C parameter that acts like $\frac{1}{\lambda}$ for regularization in Lasso and Ridge Regression.  Changing this values allows allowing error.  We get get a feel for how it affect the accuracy of a prediction by scanning through a number of values of this tuning paramter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;cv_score = []
Cs = np.logspace(-3, 1, 100)
for c in Cs:
    pipeline = Pipeline([(&amp;#39;scaler&amp;#39;, StandardScaler()),
                    (&amp;#39;svc&amp;#39;, SVC(kernel=&amp;#39;linear&amp;#39;, C=c))])
    cv_score.append(cross_val_score(pipeline, x, y, scoring=&amp;#39;accuracy&amp;#39;, cv=10).mean())

plt.figure(figsize=(14,8))
plt.plot(Cs,cv_score,lw=2,color=&amp;#39;seagreen&amp;#39;)
plt.xlabel(&amp;quot;Tuning Paramter&amp;quot;)
plt.ylabel(&amp;quot;Accuracy&amp;quot;)
plt.xscale(&amp;quot;log&amp;quot;)
plt.ylim([0.9,.95])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_20_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that by changing the tuning parameter that we change the accuracy of the SVM, but there are similar accuracies for different C values.  To show what the algorithm is doing we can plot the decision boundaries for two very different C's on the non-seperable dataset we have started investigating.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pipeline1 = Pipeline([(&amp;#39;scaler&amp;#39;, StandardScaler()),
                    (&amp;#39;svc&amp;#39;, SVC(kernel=&amp;#39;linear&amp;#39;,C=.01))])
pipeline1.fit(x, y)
pipeline2= Pipeline([(&amp;#39;scaler&amp;#39;, StandardScaler()),
                    (&amp;#39;svc&amp;#39;, SVC(kernel=&amp;#39;linear&amp;#39;,C=10))])
pipeline2.fit(x, y)

mask = y==1
xx,yy = np.meshgrid(np.linspace(-5,5,100),np.linspace(-5,5,100))
Z1 = pipeline1.predict(np.c_[xx.ravel(),yy.ravel()])
zz1 = Z1.reshape(xx.shape)

Z2 = pipeline2.predict(np.c_[xx.ravel(),yy.ravel()])
zz2 = Z2.reshape(xx.shape)

plt.figure(figsize=(14,8))
plt.pcolormesh(xx,yy,zz1,cmap=&amp;#39;BrBG&amp;#39;,alpha=0.2)
plt.pcolormesh(xx,yy,zz2,cmap=&amp;#39;binary&amp;#39;,alpha=0.2)
plt.plot(x[mask,0],x[mask,1],color=&amp;#39;seagreen&amp;#39;,marker=&amp;#39;o&amp;#39;, markersize=10,lw=0,label=&amp;#39;1\&amp;#39;s&amp;#39;)
plt.plot(x[~mask,0],x[~mask,1],color=&amp;#39;burlywood&amp;#39;,marker=&amp;#39;o&amp;#39;,markersize=10,lw=0,label=&amp;#39;0\&amp;#39;s&amp;#39;)
plt.legend()
plt.xlim([-5,5])
plt.ylim([-5,5])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_22_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that the two regions have different slopes.  I mapped a binary (black/white) colormap over the large C fit.   This makes the background darker.  The smaller C has the hyperplane seperator with a more negative slope than the larger C.   By changing the hyperparameter, we are ultimately move the hyperplane that is attempting to fit the data.&lt;/p&gt;
&lt;h2&gt;Kernels&lt;/h2&gt;
&lt;p&gt;SVM's can accept a kernal argument that effectively maps the data into a higher dimension, potentially making the data linearably seperable when it otherwise might not be.  We can illustrate this with a simple example using random data.  We will have some data that is seperable, but not linearably seperable.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x1r = 2*np.random.random((200,1))-1
x2r = 2*np.random.random((200,1))-1

x3r = x1r*x1r+x2r*x2r

yr = np.sqrt(x1r*x1r+x2r*x2r) &amp;lt; .5

mask = yr==0
plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
plt.plot(x1r[mask],x2r[mask],&amp;#39;ro&amp;#39;)
plt.plot(x1r[~mask],x2r[~mask],&amp;#39;bo&amp;#39;)
plt.subplot(1,2,2)
plt.plot(x1r[mask],x3r[mask],&amp;#39;ro&amp;#39;)
plt.plot(x1r[~mask],x3r[~mask],&amp;#39;bo&amp;#39;)
plt.axhline(y=0.25)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_24_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;By transforming the data into a third dimention we take a seperable data and make in seperable by a hyperplane.  How we can train an SVM on this data, and find the decision boundary.  Two common kernals to fit data two is a gaussian kernal and a polynomial kernal.  This allow for making non-linear decision surfaces.  Lets first look at the RBF kernel.&lt;/p&gt;
&lt;h2&gt;RBF Kernel&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def plot_surface(kernel,C=1,degree=3,gamma=1,*args):
    pipeline = Pipeline([(&amp;#39;scaler&amp;#39;, StandardScaler()),
                    (&amp;#39;svc&amp;#39;, SVC(kernel=kernel,C=C,degree=degree,gamma=1,*args))])
    pipeline.fit(x,y)
    svc_rbf = pipeline.named_steps[&amp;#39;svc&amp;#39;]


    mask = y==1
    xx,yy = np.meshgrid(np.linspace(-5,5,100),np.linspace(-5,5,100))
    Z = pipeline.predict(np.c_[xx.ravel(),yy.ravel()])
    zz = Z.reshape(xx.shape)


    plt.figure(figsize=(14,8))
    plt.pcolormesh(xx,yy,zz,cmap=&amp;#39;BrBG&amp;#39;,alpha=0.3)
    plt.plot(x[mask,0],x[mask,1],color=&amp;#39;seagreen&amp;#39;,marker=&amp;#39;o&amp;#39;, markersize=10,lw=0,label=&amp;#39;1\&amp;#39;s&amp;#39;)
    plt.plot(x[~mask,0],x[~mask,1],color=&amp;#39;burlywood&amp;#39;,marker=&amp;#39;o&amp;#39;,markersize=10,lw=0,label=&amp;#39;0\&amp;#39;s&amp;#39;)
    plt.legend()
    plt.xlim([-5,5])
    plt.ylim([-5,5])
    plt.title(&amp;quot;SVM with &amp;quot; + kernel + &amp;quot; Kernel, C = &amp;quot;+str(C))
    plt.show()

plot_surface(&amp;#39;rbf&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_26_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The decision surface of this SVM is clearly not linear in the data, but it is linear in a hyperspace the data is projected into.   We can look at how the accuracy changes as we turn the model.  As well as how the deciion surfaces changes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;cv_score = []
Cs = np.logspace(-3, 1, 100)
for c in Cs:
    pipeline = Pipeline([(&amp;#39;scaler&amp;#39;, StandardScaler()),
                    (&amp;#39;svc&amp;#39;, SVC(kernel=&amp;#39;rbf&amp;#39;, C=c))])
    cv_score.append(cross_val_score(pipeline, x, y, scoring=&amp;#39;accuracy&amp;#39;, cv=10).mean())

plt.figure(figsize=(14,8))
plt.plot(Cs,cv_score,lw=2,color=&amp;#39;seagreen&amp;#39;)
plt.xlabel(&amp;quot;Tuning Paramter&amp;quot;)
plt.ylabel(&amp;quot;Accuracy&amp;quot;)
plt.xscale(&amp;quot;log&amp;quot;)
plt.ylim([0.89,.95])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_28_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_surface(&amp;#39;rbf&amp;#39;,C=100)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_29_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_surface(&amp;#39;rbf&amp;#39;,C=1000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_30_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that as we increase the turning parameter, which is lower the regulization, we get very curvy decision surfaces.&lt;/p&gt;
&lt;h2&gt;Polynomial&lt;/h2&gt;
&lt;p&gt;The polynomial kernal allows for some curvature to the decision surface, but usally less than that fit by the RBF kernel.   We can see a degree polynomial curve below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_surface(&amp;#39;poly&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_32_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can look at the surface as we change the turning parameter and the degree of the kernel.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_surface(&amp;#39;rbf&amp;#39;,C=100,degree=3)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_34_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_surface(&amp;#39;rbf&amp;#39;,C=1,degree=5)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_35_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that the hypersurface changes its form adn has more curvature as we increase the size of these parameters.  &lt;/p&gt;
&lt;h2&gt;Grid Search&lt;/h2&gt;
&lt;p&gt;Ideally we will want to find the best model to fit the data we are given.  This is difficult to do by hand, so we can use a grid search strategy to find the best model on our training data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc__degree&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc__C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;scaler&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;poly&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Best Accuracy:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_score_&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Best Parameters:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;

&lt;span class="n"&gt;Best&lt;/span&gt; &lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.94&lt;/span&gt;
&lt;span class="n"&gt;Best&lt;/span&gt; &lt;span class="n"&gt;Parameters&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc__degree&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;svc__C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.058570208180566671&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;



&lt;span class="n"&gt;plot_surface&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;poly&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc__C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svc__degree&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_38_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Multi-Classification&lt;/h2&gt;
&lt;p&gt;We can also use SVM's, and other classifiers, with datesets that have multiple classifications.   An example is the digits dataset that has hand written digits as images and we are attempting to classify them.&lt;/p&gt;
&lt;p&gt;The two methods is 1 vs all, which makes a classifier for each classification.  Then each data point is scored by each classifier.  The highest scored predictor is then classified as a positive example in that classifier.
In 1 vs 1, a classifier is made for each pair of data, and then there is a vote.  The classification with the most votes win.   We will be using both of these with a SVM on the digit data.&lt;/p&gt;
&lt;p&gt;Because we are going to be using linear kernals, there is a optimize algorithm in sklearn that we will be using.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.multiclass&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OneVsRestClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;OneVsOneClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1797&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;LinVsAll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OneVsRestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;LinVsOne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OneVsOneClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_40_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The data are 8x8 pixle images of numbers that are hand written.  There are 10 classifications available in tis dataset.  We will train both multiclassification methods on the dataset and check the test accuracy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x_train, x_test, y_train, y_test = train_test_split(images,data.target)

LinVsAll.fit(x_train,y_train)
y_pred = LinVsAll.predict(x_test)

print &amp;quot;One Vs All&amp;quot;
print &amp;quot;Accuracy: &amp;quot;, accuracy_score(y_test,y_pred)
print &amp;quot;Recall: &amp;quot;, recall_score(y_test,y_pred,average=&amp;#39;weighted&amp;#39;)
print &amp;quot;Precision: &amp;quot;, precision_score(y_test,y_pred,average=&amp;#39;weighted&amp;#39;)


print &amp;quot;&amp;quot;
print &amp;quot;One Vs One&amp;quot;
LinVsOne.fit(x_train,y_train)
y_pred = LinVsOne.predict(x_test)

print &amp;quot;Accuracy: &amp;quot;, accuracy_score(y_test,y_pred)
print &amp;quot;Recall: &amp;quot;, recall_score(y_test,y_pred,average=&amp;#39;weighted&amp;#39;)
print &amp;quot;Precision: &amp;quot;, precision_score(y_test,y_pred,average=&amp;#39;weighted&amp;#39;)

One Vs All
Accuracy:  0.942222222222
Recall:  0.942222222222
Precision:  0.942982349293

One Vs One
Accuracy:  0.975555555556
Recall:  0.975555555556
Precision:  0.976581128748
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this case we have the One vs One method being more accurate.  I suspect that we find some values of 7,9, and 4 that are pretty similar, and the voting helps seperate them while the best score does not. &lt;/p&gt;
&lt;h2&gt;Real World Data&lt;/h2&gt;
&lt;p&gt;We have some biological data that we are asked to predict if a given sample comes from stool or tissue.  This problem is interesting to me because the data is not structured in a way for us to answer it.  We have to restructure the data set!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df_b = pd.read_csv(&amp;#39;data/bio.csv&amp;#39;).drop(&amp;#39;Group&amp;#39;,axis=1)
df_b.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Patient&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;th&gt;Stool&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Firmicutes&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Firmicutes&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Firmicutes&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Firmicutes&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Firmicutes&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df_b = df_b.pivot(&amp;#39;Patient&amp;#39;,&amp;#39;Taxon&amp;#39;)
df_b
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th colspan="5" halign="left"&gt;Tissue&lt;/th&gt;
      &lt;th colspan="5" halign="left"&gt;Stool&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Patient&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1590&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;195&lt;/td&gt;
      &lt;td&gt;2469&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1821&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;839&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;661&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;4414&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;568&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
      &lt;td&gt;12044&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;1102&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;2310&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;4829&lt;/td&gt;
      &lt;td&gt;718&lt;/td&gt;
      &lt;td&gt;527&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
      &lt;td&gt;377&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;717&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;547&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;260&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;395&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;651&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;2651&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;767&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;548&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1195&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3196&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
      &lt;td&gt;6857&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;381&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;795&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;4255&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;359&lt;/td&gt;
      &lt;td&gt;4361&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;666&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2950&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;3994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2473&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1541&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;2314&lt;/td&gt;
      &lt;td&gt;223&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;816&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;310&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;281&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;1307&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;2377&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df_b = df_b.stack(level=0)
df_b
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Patient&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;1&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;1590&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;195&lt;/td&gt;
      &lt;td&gt;2469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1821&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;2&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;839&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;661&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;3&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;4414&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;4&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;568&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
      &lt;td&gt;12044&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;5&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;1102&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;2310&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;6&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;4829&lt;/td&gt;
      &lt;td&gt;718&lt;/td&gt;
      &lt;td&gt;527&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;377&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;717&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;547&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;7&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;260&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;395&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;651&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;8&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;2651&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;767&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;9&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;548&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1195&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3196&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;10&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
      &lt;td&gt;6857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;381&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;795&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;11&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;4255&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;359&lt;/td&gt;
      &lt;td&gt;4361&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;666&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;12&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2950&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;3994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;13&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2473&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1541&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;2314&lt;/td&gt;
      &lt;td&gt;223&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;816&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan="2" valign="top"&gt;14&lt;/th&gt;
      &lt;th&gt;Tissue&lt;/th&gt;
      &lt;td&gt;310&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;281&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;1307&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stool&lt;/th&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;2377&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df_b = df_b.reset_index()
df_b
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Patient&lt;/th&gt;
      &lt;th&gt;level_1&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;1590&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;195&lt;/td&gt;
      &lt;td&gt;2469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1821&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;839&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;661&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;4414&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;568&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
      &lt;td&gt;12044&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;1102&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;2310&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;4829&lt;/td&gt;
      &lt;td&gt;718&lt;/td&gt;
      &lt;td&gt;527&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;377&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;717&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;547&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;260&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;395&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;651&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;2651&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;767&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;548&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1195&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3196&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
      &lt;td&gt;6857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;381&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;795&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;20&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;4255&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;21&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;359&lt;/td&gt;
      &lt;td&gt;4361&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;666&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2950&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;3994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2473&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1541&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;2314&lt;/td&gt;
      &lt;td&gt;223&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;816&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;26&lt;/th&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;310&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;281&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;1307&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27&lt;/th&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;2377&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df_b[&amp;#39;Location&amp;#39;] = np.where(df_b.level_1 == &amp;#39;Tissue&amp;#39;,1,0)
df_b
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Patient&lt;/th&gt;
      &lt;th&gt;level_1&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
      &lt;th&gt;Location&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;1590&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;195&lt;/td&gt;
      &lt;td&gt;2469&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1821&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;839&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;661&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;4414&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;568&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
      &lt;td&gt;12044&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;1102&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;2310&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;4829&lt;/td&gt;
      &lt;td&gt;718&lt;/td&gt;
      &lt;td&gt;527&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;377&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;717&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;547&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;260&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;395&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;651&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;2651&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;767&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;548&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1195&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3196&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
      &lt;td&gt;6857&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;381&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;795&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;20&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;4255&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;21&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;359&lt;/td&gt;
      &lt;td&gt;4361&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;666&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2950&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;3994&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2473&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1541&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;2314&lt;/td&gt;
      &lt;td&gt;223&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;816&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;26&lt;/th&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;Tissue&lt;/td&gt;
      &lt;td&gt;310&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;281&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;1307&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27&lt;/th&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;Stool&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;2377&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df_b = df_b.drop([&amp;#39;level_1&amp;#39;, &amp;#39;Patient&amp;#39;], axis=1)
df_b
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;Taxon&lt;/th&gt;
      &lt;th&gt;Actinobacteria&lt;/th&gt;
      &lt;th&gt;Bacteroidetes&lt;/th&gt;
      &lt;th&gt;Firmicutes&lt;/th&gt;
      &lt;th&gt;Other&lt;/th&gt;
      &lt;th&gt;Proteobacteria&lt;/th&gt;
      &lt;th&gt;Location&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1590&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;195&lt;/td&gt;
      &lt;td&gt;2469&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4182&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1821&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1174&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;839&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;703&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;661&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;4414&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3946&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;568&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;831&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
      &lt;td&gt;12044&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8605&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;1102&lt;/td&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;693&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;2310&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;678&lt;/td&gt;
      &lt;td&gt;4829&lt;/td&gt;
      &lt;td&gt;718&lt;/td&gt;
      &lt;td&gt;527&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;377&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;717&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;547&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;260&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;395&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;651&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;2651&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;767&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;548&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1195&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3196&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
      &lt;td&gt;6857&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;381&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;795&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;20&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;4255&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;21&lt;/th&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;359&lt;/td&gt;
      &lt;td&gt;4361&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;666&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22&lt;/th&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2950&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;3994&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2473&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1541&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;2314&lt;/td&gt;
      &lt;td&gt;223&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;816&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;26&lt;/th&gt;
      &lt;td&gt;310&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;281&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;1307&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27&lt;/th&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;2377&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;And now we have our data!!!!  We can split into a train test set and see how it performs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;y = df_b[&amp;#39;Location&amp;#39;].values
x = df_b.drop(&amp;#39;Location&amp;#39;, axis=1).values
x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=.2)
svc_lin = LinearSVC()
parameters = {&amp;#39;C&amp;#39;:np.logspace(-3,3,1000)}
clf = GridSearchCV(svc_lin, parameters, scoring=&amp;#39;accuracy&amp;#39;, cv=10)
clf.fit(x_train,y_train)
print &amp;quot;Best Accuracy:&amp;quot;, clf.best_score_
print &amp;quot;Test Accuracy:&amp;quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.863636363636
Test Accuracy: 0.166666666667
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our linear SVC did not generalize to the test set at all.  Lets try a different model!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;svc_lin = SVC(kernel=&amp;#39;rbf&amp;#39;)
parameters = {&amp;#39;C&amp;#39;:np.logspace(-3,3,100),&amp;#39;gamma&amp;#39;:np.logspace(-8,2,11)}
clf = GridSearchCV(svc_lin, parameters, scoring=&amp;#39;accuracy&amp;#39;, cv=10)
clf.fit(x_train,y_train)
print &amp;quot;Best Accuracy:&amp;quot;, clf.best_score_
print &amp;quot;Test Accuracy:&amp;quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.818181818182
Test Accuracy: 0.666666666667
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The RBF kernal generalized much better to the test set.  We are doing better then guessing, but I am wondering if a logistic regressor will do better.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lin = LogisticRegression()
parameters = {&amp;#39;C&amp;#39;:np.logspace(-3,3,1000)}
clf = GridSearchCV(lin, parameters, scoring=&amp;#39;accuracy&amp;#39;, cv=10)
clf.fit(x_train,y_train)
print &amp;quot;Best Accuracy:&amp;quot;, clf.best_score_
print &amp;quot;Test Accuracy:&amp;quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.818181818182
Test Accuracy: 0.333333333333
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is not the best model by far.   Before we come to a close, we will see if the SVC does better at predict than a random forest.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;n_estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Best Accuracy:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_score_&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Test Accuracy:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;Best&lt;/span&gt; &lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.818181818182&lt;/span&gt;
&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.833333333333&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And as expected, the random forest goes for the best generalization.   Of course, that what ensemble methods are suppose to do.  We're suppose to learn about them in depth tomorrow.  Until then....&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="SVM"></category><category term="support vector machines"></category></entry><entry><title>Galvanize - Week 04 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-02/" rel="alternate"></link><updated>2015-06-23T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-23:galvanize/galvanize-data-science-04-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 4 - Day 2&lt;/h2&gt;
&lt;p&gt;Today's quiz a an online interview questions that involves picking a random value from a stream in order 1 memory use. 
After that was a lecture on Bootstrap Aggregate (Bagging) ML methods, focused on decisions trees, then the random forest algorithm.   We implemented a random forest using our decision trees we made from yesterday.&lt;/p&gt;
&lt;h2&gt;Random Forest Class&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;DecisionTree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTree&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;RandomForest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;A Random Forest class&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="p"&gt;,):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;           num_trees:  number of trees to create in the forest:&lt;/span&gt;
&lt;span class="sd"&gt;        num_features:  the number of features to consider when choosing the&lt;/span&gt;
&lt;span class="sd"&gt;                           best split for each node of the decision trees&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_trees&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_trees&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        X:  two dimensional numpy array representing feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;                for test data&lt;/span&gt;
&lt;span class="sd"&gt;        y:  numpy array representing labels for test data&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_forest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_forest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        Return a list of num_trees DecisionTrees.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;forest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;size_feature&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;size_feature&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

            &lt;span class="c"&gt;#features = np.random.choice(range(len(X[0,:])),size=size_feature,replace=False)&lt;/span&gt;
            &lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;indexes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;,:],&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;forest&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        Return a numpy array of the labels predicted for the given test data.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="n"&gt;final_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;return_counts&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;final_predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;final_predictions&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        Return the accuracy of the Random Forest for the given test data and&lt;/span&gt;
&lt;span class="sd"&gt;        labels.&lt;/span&gt;

&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Checking Our Random Forest&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;DecisionTree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTree&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../data/playgolf.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Result&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTree&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;

&lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;overcast&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="n"&gt;Play&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt; &lt;span class="n"&gt;overcast&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;71&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="n"&gt;Play&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;71&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="n"&gt;Play&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="n"&gt;Don&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t Play&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our Decision tree is still functioning.  Since the random forest depends on that I wanted to just show that it functions.  The idea is that if we have a weak classifier we can use it a large number of them to vote on a decisions and find, on average, a better answer.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;RandomForest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForest&lt;/span&gt;

&lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;score:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y_predict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;


&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Play&amp;#39;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Don&amp;#39;t Play&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Don&amp;#39;t Play&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The random forest, unsurprisingly, does not do very well on the golf data set.  It is very small, and the rules are complicated.   Lets try it on some congress data.&lt;/p&gt;
&lt;h2&gt;Comparision with Sklearn&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;congress&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../data/congressional_voting.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;congress&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;congress&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;republican&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;democrat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;?&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;congress&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;congress&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;cv_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;
&lt;span class="n"&gt;skcv_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;

&lt;span class="n"&gt;kf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;n_folds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trees&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;skrf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;skrf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;cv_score&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;10.&lt;/span&gt;
    &lt;span class="n"&gt;skcv_score&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;skrf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;10.&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;                       CV Accuracy    Test Accuracy&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My RF Scores:         &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My Sklearn RF Scores: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skcv_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;skrf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


                       &lt;span class="n"&gt;CV&lt;/span&gt; &lt;span class="n"&gt;Accuracy&lt;/span&gt;    &lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Accuracy&lt;/span&gt;
&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;RF&lt;/span&gt; &lt;span class="n"&gt;Scores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;          &lt;span class="mf"&gt;0.958441558442&lt;/span&gt; &lt;span class="mf"&gt;0.94495412844&lt;/span&gt;
&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;Sklearn&lt;/span&gt; &lt;span class="n"&gt;RF&lt;/span&gt; &lt;span class="n"&gt;Scores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.958658008658&lt;/span&gt; &lt;span class="mf"&gt;0.95871559633&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sklearn's random forest classifier and my random forest implementation give similar results in cross validation and on the test set.  On small datasets, it works in similar time.   &lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;The afternoon paired sprint involved using and exploring sklearn's random forest classifier on a cell phone plan dataset attempting to predict churn.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.neighbors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../data/churn.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Account Length&lt;/th&gt;
      &lt;th&gt;Area Code&lt;/th&gt;
      &lt;th&gt;Phone&lt;/th&gt;
      &lt;th&gt;Int'l Plan&lt;/th&gt;
      &lt;th&gt;VMail Plan&lt;/th&gt;
      &lt;th&gt;VMail Message&lt;/th&gt;
      &lt;th&gt;Day Mins&lt;/th&gt;
      &lt;th&gt;Day Calls&lt;/th&gt;
      &lt;th&gt;Day Charge&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;Eve Calls&lt;/th&gt;
      &lt;th&gt;Eve Charge&lt;/th&gt;
      &lt;th&gt;Night Mins&lt;/th&gt;
      &lt;th&gt;Night Calls&lt;/th&gt;
      &lt;th&gt;Night Charge&lt;/th&gt;
      &lt;th&gt;Intl Mins&lt;/th&gt;
      &lt;th&gt;Intl Calls&lt;/th&gt;
      &lt;th&gt;Intl Charge&lt;/th&gt;
      &lt;th&gt;CustServ Calls&lt;/th&gt;
      &lt;th&gt;Churn?&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;KS&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;415&lt;/td&gt;
      &lt;td&gt;382-4657&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;265.1&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;45.07&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;16.78&lt;/td&gt;
      &lt;td&gt;244.7&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;11.01&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;OH&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;415&lt;/td&gt;
      &lt;td&gt;371-7191&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;161.6&lt;/td&gt;
      &lt;td&gt;123&lt;/td&gt;
      &lt;td&gt;27.47&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;16.62&lt;/td&gt;
      &lt;td&gt;254.4&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;11.45&lt;/td&gt;
      &lt;td&gt;13.7&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;NJ&lt;/td&gt;
      &lt;td&gt;137&lt;/td&gt;
      &lt;td&gt;415&lt;/td&gt;
      &lt;td&gt;358-1921&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;243.4&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
      &lt;td&gt;41.38&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;10.30&lt;/td&gt;
      &lt;td&gt;162.6&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;7.32&lt;/td&gt;
      &lt;td&gt;12.2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3.29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;OH&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;375-9999&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;299.4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;50.90&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;5.26&lt;/td&gt;
      &lt;td&gt;196.9&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;8.86&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;1.78&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;OK&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;415&lt;/td&gt;
      &lt;td&gt;330-6626&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;no&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;166.7&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;28.34&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt;12.61&lt;/td&gt;
      &lt;td&gt;186.9&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt;8.41&lt;/td&gt;
      &lt;td&gt;10.1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;False.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 21 columns&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;for i in df.columns:
    print i, df[i].nunique()

State 51
Account Length 212
Area Code 3
Phone 3333
Int&amp;#39;l Plan 2
VMail Plan 2
VMail Message 46
Day Mins 1667
Day Calls 119
Day Charge 1667
Eve Mins 1611
Eve Calls 123
Eve Charge 1440
Night Mins 1591
Night Calls 120
Night Charge 933
Intl Mins 162
Intl Calls 21
Intl Charge 162
CustServ Calls 10
Churn? 2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to clean up the data a little.   Replace boolean type values with 1 or 0, and drop some information that will not work with the classifier. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df[&amp;#39;Int\&amp;#39;l Plan&amp;#39;]=np.where(df[&amp;#39;Int\&amp;#39;l Plan&amp;#39;]==&amp;#39;yes&amp;#39;,1,0)
df[&amp;#39;VMail Plan&amp;#39;]=np.where(df[&amp;#39;VMail Plan&amp;#39;]==&amp;#39;yes&amp;#39;,1,0)
df[&amp;#39;Churn?&amp;#39;]=np.where(df[&amp;#39;Churn?&amp;#39;]==&amp;#39;True.&amp;#39;,1,0)
df = df[[u&amp;#39;Account Length&amp;#39;, u&amp;#39;Int\&amp;#39;l Plan&amp;#39;, u&amp;#39;VMail Plan&amp;#39;, u&amp;#39;VMail Message&amp;#39;, u&amp;#39;Day Mins&amp;#39;, u&amp;#39;Day Calls&amp;#39;, u&amp;#39;Day Charge&amp;#39;, u&amp;#39;Eve Mins&amp;#39;, u&amp;#39;Eve Calls&amp;#39;, u&amp;#39;Eve Charge&amp;#39;, u&amp;#39;Night Mins&amp;#39;, u&amp;#39;Night Calls&amp;#39;, u&amp;#39;Night Charge&amp;#39;, u&amp;#39;Intl Mins&amp;#39;, u&amp;#39;Intl Calls&amp;#39;, u&amp;#39;Intl Charge&amp;#39;, u&amp;#39;CustServ Calls&amp;#39;, u&amp;#39;Churn?&amp;#39;]]
df.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Account Length&lt;/th&gt;
      &lt;th&gt;Int'l Plan&lt;/th&gt;
      &lt;th&gt;VMail Plan&lt;/th&gt;
      &lt;th&gt;VMail Message&lt;/th&gt;
      &lt;th&gt;Day Mins&lt;/th&gt;
      &lt;th&gt;Day Calls&lt;/th&gt;
      &lt;th&gt;Day Charge&lt;/th&gt;
      &lt;th&gt;Eve Mins&lt;/th&gt;
      &lt;th&gt;Eve Calls&lt;/th&gt;
      &lt;th&gt;Eve Charge&lt;/th&gt;
      &lt;th&gt;Night Mins&lt;/th&gt;
      &lt;th&gt;Night Calls&lt;/th&gt;
      &lt;th&gt;Night Charge&lt;/th&gt;
      &lt;th&gt;Intl Mins&lt;/th&gt;
      &lt;th&gt;Intl Calls&lt;/th&gt;
      &lt;th&gt;Intl Charge&lt;/th&gt;
      &lt;th&gt;CustServ Calls&lt;/th&gt;
      &lt;th&gt;Churn?&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;265.1&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;45.07&lt;/td&gt;
      &lt;td&gt;197.4&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;16.78&lt;/td&gt;
      &lt;td&gt;244.7&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;11.01&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;161.6&lt;/td&gt;
      &lt;td&gt;123&lt;/td&gt;
      &lt;td&gt;27.47&lt;/td&gt;
      &lt;td&gt;195.5&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;16.62&lt;/td&gt;
      &lt;td&gt;254.4&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;11.45&lt;/td&gt;
      &lt;td&gt;13.7&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;137&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;243.4&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
      &lt;td&gt;41.38&lt;/td&gt;
      &lt;td&gt;121.2&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;10.30&lt;/td&gt;
      &lt;td&gt;162.6&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;7.32&lt;/td&gt;
      &lt;td&gt;12.2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3.29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;299.4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;50.90&lt;/td&gt;
      &lt;td&gt;61.9&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;5.26&lt;/td&gt;
      &lt;td&gt;196.9&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;8.86&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;1.78&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;166.7&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;28.34&lt;/td&gt;
      &lt;td&gt;148.3&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt;12.61&lt;/td&gt;
      &lt;td&gt;186.9&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt;8.41&lt;/td&gt;
      &lt;td&gt;10.1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2.73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;df.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 3333 entries, 0 to 3332
Data columns (total 18 columns):
Account Length    3333 non-null int64
Int&amp;#39;l Plan        3333 non-null int64
VMail Plan        3333 non-null int64
VMail Message     3333 non-null int64
Day Mins          3333 non-null float64
Day Calls         3333 non-null int64
Day Charge        3333 non-null float64
Eve Mins          3333 non-null float64
Eve Calls         3333 non-null int64
Eve Charge        3333 non-null float64
Night Mins        3333 non-null float64
Night Calls       3333 non-null int64
Night Charge      3333 non-null float64
Intl Mins         3333 non-null float64
Intl Calls        3333 non-null int64
Intl Charge       3333 non-null float64
CustServ Calls    3333 non-null int64
Churn?            3333 non-null int64
dtypes: float64(8), int64(10)
memory usage: 494.7 KB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So now we have a clean dataset with only ints and floats. Lets prepare our test and training&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;y = df[&amp;#39;Churn?&amp;#39;].values
x = df.drop(&amp;#39;Churn?&amp;#39;,axis=1).values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)
model = RandomForestClassifier()
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
print &amp;quot;Base Accuracy: &amp;quot;, model.score(x_test,y_test)
print &amp;quot;Guessing No Churn:&amp;quot;, np.sum(y_test==0).astype(float)/len(y_test)

Base Accuracy:  0.939
Guessing No Churn: 0.862
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The random forest is giving a better than guessing result!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print confusion_matrix(y_test, model.predict(x_test))

[[853   9]
 [ 52  86]]



print &amp;quot;Recall: &amp;quot;, recall_score(y_test,y_predict)
print &amp;quot;Precision: &amp;quot;, precision_score(y_test,y_predict)

Recall:  0.623188405797
Precision:  0.905263157895
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The model is having the worst problem predicting that people who do churn will churn.   Only ~60% of those that churn were predicted to do so.  There is room for imporovement in the model here.&lt;/p&gt;
&lt;p&gt;Sklearn's random forest classifiery has an out of bag error estimate as well as a estimator of feature importance.  We are going to use these next.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;model_oob = RandomForestClassifier(oob_score=True)
model_oob.fit(x_train, y_train)
oob_score = model_oob.score(x_test, y_test)
print &amp;#39;Accuracy (old/new): &amp;#39;, model.score(x_test,y_test), oob_score

print &amp;quot;OBB Estimate: &amp;quot;, model_oob.oob_score_

y_predict_oob = model_oob.predict(x_test)

print &amp;#39;Precision (old, new)&amp;#39;, precision_score(y_test, y_predict), precision_score(y_test, y_predict_oob)
print &amp;#39;Recall (old, new)&amp;#39;, recall_score(y_test, y_predict), recall_score(y_test, y_predict_oob)

Accuracy (old/new):  0.939 0.943
OBB Estimate:  0.921560222889
Precision (old, new) 0.905263157895 0.917525773196
Recall (old, new) 0.623188405797 0.644927536232
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The difference between the old and new model is not statistically significant.  They are just the natural variation in this model.   The OBB estimate is very close to the accuracy on the test set.  That is promissing.  Lets see if we can find the most important features:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;features = model.feature_importances_
print df.columns[model.feature_importances_ &amp;gt;= sorted(features)[-5]]
features = model_oob.feature_importances_
print df.columns[model_oob.feature_importances_ &amp;gt;= sorted(features)[-5]]

Index([u&amp;#39;Int&amp;#39;l Plan&amp;#39;, u&amp;#39;Day Mins&amp;#39;, u&amp;#39;Day Charge&amp;#39;, u&amp;#39;Eve Charge&amp;#39;, u&amp;#39;CustServ Calls&amp;#39;], dtype=&amp;#39;object&amp;#39;)
Index([u&amp;#39;Int&amp;#39;l Plan&amp;#39;, u&amp;#39;Day Mins&amp;#39;, u&amp;#39;Day Charge&amp;#39;, u&amp;#39;Eve Charge&amp;#39;, u&amp;#39;CustServ Calls&amp;#39;], dtype=&amp;#39;object&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see that the two models share 4 out of 5 of the same most important features.   They differ on Eve. Charge and Int'l Plan.  &lt;/p&gt;
&lt;p&gt;Before we start selecting featuers, we want to make sure that we are seeing the best model. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def tree_accuracy(num_trees):    
    model = RandomForestClassifier(oob_score=True, n_estimators=num_trees)
    model.fit(x_train, y_train)
    accuracy = model.score(x_test, y_test)
    return accuracy

treevalues1 = range(1, 1000, 100)
values1 = []
for v in treevalues1:
    values1.append(tree_accuracy(v))

treevalues2 = range(1, 200, 10)
values2 = []
for v in treevalues2:
    values2.append(tree_accuracy(v))
plt.figure(figsize=(14,8))
plt.subplot(1,2,1)
plt.plot(treevalues1, values1,lw=2,color=&amp;#39;seagreen&amp;#39;)
plt.subplot(1,2,2)
plt.plot(treevalues2, values2,lw=2,color=&amp;#39;steelblue&amp;#39;)
plt.xlabel(&amp;#39;Number of Trees&amp;#39;)
plt.ylabel(&amp;#39;accuracy&amp;#39;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_23_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see after about 50 estimators the accuracy does not significantly improve.   We also see if there is a limit on the number of features to consider at each node.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def tree_accuracy(num_features, num_trees=50):    
    model = RandomForestClassifier(oob_score=True, n_estimators=num_trees, max_features=num_features)
    model.fit(x_train, y_train)
    accuracy = model.score(x_test, y_test)
    return accuracy

treevalues1 = range(1, 17, 1)
values1 = []
for v in treevalues1:
    values1.append(tree_accuracy(v))

plt.figure(figsize=(14,8))
plt.plot(treevalues1, values1,lw=2,color=&amp;#39;seagreen&amp;#39;)

plt.xlabel(&amp;#39;Number of Features&amp;#39;)
plt.ylabel(&amp;#39;accuracy&amp;#39;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_25_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can consider 4 features for each node and about 5 estimators while improving the results.&lt;/p&gt;
&lt;p&gt;I am wondering how this compares to other models we have covered.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,8))
for model in [LogisticRegression(),DecisionTreeClassifier(),KNeighborsClassifier(),RandomForestClassifier(n_estimators=50,max_features=5)]:
    model.fit(x_train,y_train)
    print &amp;quot;MODEL: &amp;quot;, model.__class__.__name__
    y_pred = model.predict(x_test)
    y_prob = model.predict_proba(x_test)[:,1]
    print &amp;quot;Accuracy&amp;quot;, accuracy_score(y_pred,y_test)
    print &amp;quot;Recall&amp;quot;, recall_score(y_pred,y_test)
    print &amp;quot;Precision&amp;quot;, precision_score(y_pred,y_test)
    print &amp;quot;&amp;quot;
    fpr,trp,thres = roc_curve(y_test,y_prob)
    plt.plot(fpr,trp,label=model.__class__.__name__)
plt.xlabel(&amp;quot;False Positive Rate&amp;quot;)
plt.ylabel(&amp;quot;True Positive Rate&amp;quot;)
plt.legend(loc=4)
plt.show()

MODEL:  LogisticRegression
Accuracy 0.872
Recall 0.647058823529
Precision 0.159420289855

MODEL:  DecisionTreeClassifier
Accuracy 0.907
Recall 0.659574468085
Precision 0.673913043478

MODEL:  KNeighborsClassifier
Accuracy 0.886
Recall 0.714285714286
Precision 0.289855072464

MODEL:  RandomForestClassifier
Accuracy 0.96
Recall 0.929824561404
Precision 0.768115942029
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_27_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The random forest out performs the other models.   We have a relativley high true positve for a small false positive rate.   If we wanted to avoid false positive predictions of churn, the random forest can be turned to have a tre positive rate between 70 and 80%.   &lt;/p&gt;
&lt;p&gt;Now lets try to find the most important features in the data set.  We are going through each tree in our model and getting the feature importance of that model.  We then will average over all the importance estimates.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;izip&lt;/span&gt;


&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tree&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimators_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;findex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_importances_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ordered_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;findex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_importances_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;findex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;izip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ordered_features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;feature_means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;feature_stds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vals&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vals&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;feature_means&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;feature_stds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;feature_means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_means&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;indexes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_means&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;feature_means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;feature_means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;feature_stds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_stds&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;999&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;indexes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c"&gt;# the x locations for the groups&lt;/span&gt;
&lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.55&lt;/span&gt;       &lt;span class="c"&gt;# the width of the bars&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;feature_means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yerr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_stds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Mean Importance&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Feature Importance&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xticklabels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;45&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_29_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;So we see that the most important features, on average, are day charge, day mins, custserv calls, int'l plan, eve charge, and int'l calls. &lt;/p&gt;
&lt;p&gt;Lets retrain the dataset on this subseted data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df2 = df[[u&amp;#39;Int\&amp;#39;l Plan&amp;#39;, u&amp;#39;Day Mins&amp;#39;, u&amp;#39;Day Charge&amp;#39;, u&amp;#39;Eve Mins&amp;#39;, u&amp;#39;Eve Charge&amp;#39;, u&amp;#39;CustServ Calls&amp;#39;]]
x = df2.values
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Int'l Plan&lt;/th&gt;
      &lt;th&gt;Day Mins&lt;/th&gt;
      &lt;th&gt;Day Charge&lt;/th&gt;
      &lt;th&gt;Eve Mins&lt;/th&gt;
      &lt;th&gt;Eve Charge&lt;/th&gt;
      &lt;th&gt;CustServ Calls&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;265.1&lt;/td&gt;
      &lt;td&gt;45.07&lt;/td&gt;
      &lt;td&gt;197.4&lt;/td&gt;
      &lt;td&gt;16.78&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;161.6&lt;/td&gt;
      &lt;td&gt;27.47&lt;/td&gt;
      &lt;td&gt;195.5&lt;/td&gt;
      &lt;td&gt;16.62&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;243.4&lt;/td&gt;
      &lt;td&gt;41.38&lt;/td&gt;
      &lt;td&gt;121.2&lt;/td&gt;
      &lt;td&gt;10.30&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;299.4&lt;/td&gt;
      &lt;td&gt;50.90&lt;/td&gt;
      &lt;td&gt;61.9&lt;/td&gt;
      &lt;td&gt;5.26&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;166.7&lt;/td&gt;
      &lt;td&gt;28.34&lt;/td&gt;
      &lt;td&gt;148.3&lt;/td&gt;
      &lt;td&gt;12.61&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)
model = RandomForestClassifier(n_estimators=100,max_features=5)
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
print &amp;quot;Base Accuracy: &amp;quot;, model.score(x_test,y_test)
print &amp;quot;Guessing No Churn:&amp;quot;, np.sum(y_test==0).astype(float)/len(y_test)
print &amp;quot;Recall: &amp;quot;, recall_score(y_test,y_predict)
print &amp;quot;Precision: &amp;quot;, precision_score(y_test,y_predict)
print confusion_matrix(y_test, model.predict(x_test))

Base Accuracy:  0.944
Guessing No Churn: 0.857
Recall:  0.671328671329
Precision:  0.914285714286
[[848   9]
 [ 47  96]]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The similar model gives similar results as the more complicated model, lending credence to the idea these are the most influencial factors.   If a cell phone company wants to reduce churn then they need to deal with the number of mins and charges for customer plans.   This is where they will reduce churn.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="Random Forests"></category><category term="sklearn"></category></entry><entry><title>Galvanize - Week 04 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-01/" rel="alternate"></link><updated>2015-06-02T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-22:galvanize/galvanize-data-science-04-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 4 - Day 1&lt;/h2&gt;
&lt;p&gt;Our quiz involved creating an SQL table that will determine churn for an fake web adervertising data.  Then with this new table, and a table of predictions, we had to come up with SQL queries that would calculate accuracy, precision, recall, and specificity.  &lt;/p&gt;
&lt;h3&gt;Tables&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;advertisers
    id
    name
    city
    state
    business_type

campaigns
    advertiser_id
    campaign_id
    start_date
    duration
    daily_budget

predicted_churn
    advertiser_id
    churn
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Table Query&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;CREATE TABLE churn 
AS (SELECT a.id, 
        a.name, 
        a.city, 
        a.state, 
        a.business_type, 
        (DATEDIFF(day,
            GETDATE(),
            c.start_date)
            +c.duration &amp;gt; 14) AS churn
    FROM advertisers a 
    JOIN (SELECT c.* 
            FROM campaigns c 
            JOIN (SELECT advertiser_id, 
                    MAX(start_date) as last_campaign_date 
                    GROUPBY advertiser_id) cc 
            ON c.advertiser_id = cc.advertiser
            AND c.start_date = cc.last_campaign_date) c
    ON a.id = c.advertiser_id
    );
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Metric Query&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT CAST((TP+TN) AS FLOAT)/(TP+TN+FP+FN) as accuracy,
            CAST(TP AS FLOAT)/(TP+FN) as recall,
            CAST(TP AS FLOAT)/(TP+FP) as precision,
            CAST(TN AS FLOAT)/(TN+FP) as specificity
FROM (SELECT COUNT( CASE WHERE c.churn=1 AND pc.churn=1
                          THEN 1 ELSE 0) as TP,
             COUNT( CASE WHERE c.churn=1 AND pc.churn=0
                          THEN 1 ELSE 0) as FN,
             COUNT( CASE WHERE c.churn=0 AND pc.churn=1
                          THEN 1 ELSE 0) as FP,
             COUNT( CASE WHERE c.churn=0 AND pc.churn=0
                          THEN 1 ELSE 0) as TN,
      FROM churn c JOIN predicted_churn)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;k Nearest Neighbors&lt;/h2&gt;
&lt;p&gt;Our morning individual sprit was to implement a kNN class that can take differnt similarity functions as a measure of nearest.  For good for bad, mine was by var the tursted solution, but that is because I perfer to think interms of matrix operations.  Because of my use of numpy, mine was also the fastest!&lt;/p&gt;
&lt;p&gt;We will start with loading some data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_classification&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_classification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_redundant&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_informative&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;n_clusters_per_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;euclidean_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - row: 1d numpy array k x 1&lt;/span&gt;
&lt;span class="sd"&gt;            - arr: 2d numpy array m x n&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - 1d numpy array m x 1&lt;/span&gt;

&lt;span class="sd"&gt;        Calculates the euclidean distance of row from each row in arr.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;euclidean_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - row: 1d numpy array k x 1&lt;/span&gt;
&lt;span class="sd"&gt;            - arr: 2d numpy array m x n&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - 1d numpy array m x 1&lt;/span&gt;

&lt;span class="sd"&gt;        Calculates the euclidean distance of row from each row in arr.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cosine_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - row: 1d numpy array k x 1&lt;/span&gt;
&lt;span class="sd"&gt;            - arr: 2d numpy array m x n&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - 1d numpy array m x 1&lt;/span&gt;

&lt;span class="sd"&gt;        Calculates the cosign similarity of row from each row in arr.&lt;/span&gt;
&lt;span class="sd"&gt;        cosign similarity = 1 - a.dot(b)/(|a||b|)&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - k: int &amp;gt; 0&lt;/span&gt;
&lt;span class="sd"&gt;            - similarity: function(1d numpy array,2d numpy array) returns 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - None&lt;/span&gt;

&lt;span class="sd"&gt;        Instantiates the KNN class&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;similarity&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array of labels&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - None&lt;/span&gt;

&lt;span class="sd"&gt;        Stores training data&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - 1d numpy array&lt;/span&gt;

&lt;span class="sd"&gt;        Calculate the distances of each row in X from each row in the training data.  &lt;/span&gt;
&lt;span class="sd"&gt;        Returns the max vote from k nearest points.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;distances&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;bincount&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]])),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;distances&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;

&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - numpy float&lt;/span&gt;

&lt;span class="sd"&gt;        Accurcy of kNN on training data&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;euclidean_distance&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="mf"&gt;1.0&lt;/span&gt;




&lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cosine_distance&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;kNN on Iris Data&lt;/h2&gt;
&lt;p&gt;I will apply my KNN class on the Iris dataset from sklearn, and compare my results to the sklearn results.  I will do this for both my metrics of euclidean and cosign.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.colors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;
&lt;span class="n"&gt;cmap_light&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FFAAAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAFFAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAAAFF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cmap_bold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FF0000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#00FF00&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#0000FF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;n_neighbors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;XI&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
&lt;span class="n"&gt;yI&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;euclidean_distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cosine_distance&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
    &lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_light&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_bold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Bryan &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; KNN 3-Class classification (k = &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s"&gt;)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.neighbors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sKNN&lt;/span&gt;
    &lt;span class="n"&gt;sknn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sKNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sknn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
    &lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sknn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_light&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_bold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Sklearn KNN 3-Class classification (k = &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s"&gt;)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D1/output_7_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D1/output_7_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Best K&lt;/h2&gt;
&lt;p&gt;We can use cross validation to try to find the best K to optimize for a metric.  Because the Iris data set has 3 labels, we can not use precision or recall.   We can use accuracy.   With a 20 Fold Cross Validation, we can estimate the accuracy for different K values.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;
&lt;span class="n"&gt;mean_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mean_precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mean_recall&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;k_range&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;k_range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;kf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;n_folds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;euclidean_distance&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;XI&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;recall&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;mean_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;mean_precision&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;mean_recall&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recall&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k_range&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Accuracy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yI&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k_range&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean_precision&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Precision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k_range&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean_recall&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Recall&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW04D1/output_9_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;It seems like any k &amp;gt; 5 and k &amp;lt; 50 will do well this data set.  Really this should be done on a hold out set for a final estimate of the model.&lt;/p&gt;
&lt;h2&gt;Recusion&lt;/h2&gt;
&lt;p&gt;We had an optional assignment to work on Recursive methods to preare for our afternoon spring involving decision trees.  We had to make trees and print trees using the following tree class.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;TreeNode&lt;/span&gt;(&lt;span class="n"&gt;object&lt;/span&gt;):
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;__init__&lt;/span&gt;(&lt;span class="k"&gt;self&lt;/span&gt;, &lt;span class="nb"&gt;value&lt;/span&gt;, &lt;span class="n"&gt;left&lt;/span&gt;=&lt;span class="n"&gt;None&lt;/span&gt;, &lt;span class="n"&gt;right&lt;/span&gt;=&lt;span class="n"&gt;None&lt;/span&gt;):
        &lt;span class="k"&gt;self&lt;/span&gt;.&lt;span class="nb"&gt;value&lt;/span&gt; = &lt;span class="nb"&gt;value&lt;/span&gt;
        &lt;span class="k"&gt;self&lt;/span&gt;.&lt;span class="n"&gt;left&lt;/span&gt; = &lt;span class="n"&gt;left&lt;/span&gt;
        &lt;span class="k"&gt;self&lt;/span&gt;.&lt;span class="n"&gt;right&lt;/span&gt; = &lt;span class="n"&gt;right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The print method I developed was just to us a "|_" style to represent branches of the tree.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def print_all(tree,level=1):
    response = &amp;quot;|_&amp;quot; + str(tree.value)
    if tree.left != None:
        response += &amp;#39;\n&amp;#39; +  &amp;quot; &amp;quot;*2*level +print_all(tree.left,level+1)
    if tree.right != None:
        response += &amp;quot;\n&amp;quot; +  &amp;quot; &amp;quot;*2*level +print_all(tree.right,level+1)
    return response

Tree = TreeNode(10,TreeNode(5,TreeNode(4),TreeNode(5)),TreeNode(10))
print print_all(Tree)

|_10
  |_5
    |_4
    |_5
  |_10
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We were asked to also find a method to find a value of a tree by summing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def sum_tree(root):
    if root.left == None:
        if root.right==None:
            return root.value
        else:
            return root.value+sum_tree(root.right)
    elif root.right == None:
        return root.value + sum_tree(root.left)
    else:
        return sum_tree(root.left) + sum_tree(root.right) + root.value

sum_tree(Tree)




34
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We were also asked to come up with a pay to construct a tree that would give all possible out comes of n coin flips:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def build_coinflip_tree(n,root=TreeNode(&amp;quot;&amp;quot;)):
    if n == 0:
        return root
    root.left = build_coinflip_tree(n-1,root=TreeNode(&amp;quot;H&amp;quot;))
    root.right = build_coinflip_tree(n-1,root=TreeNode(&amp;quot;T&amp;quot;))
    return root

print print_all(build_coinflip_tree(3))

|_
  |_H
    |_H
      |_H
      |_T
    |_T
      |_H
      |_T
  |_T
    |_H
      |_H
      |_T
    |_T
      |_H
      |_T
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;The afternoon paired spring involved creating and appending pythong classes to implement a Decision tree with pre and post purning.   We started with the simple golf dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;DecisionTree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTree&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../data/playgolf.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Outlook&lt;/th&gt;
      &lt;th&gt;Temperature&lt;/th&gt;
      &lt;th&gt;Humidity&lt;/th&gt;
      &lt;th&gt;Windy&lt;/th&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;sunny&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Don't Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;sunny&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Don't Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;overcast&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;78&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Don't Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;overcast&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;sunny&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Don't Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;sunny&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;sunny&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;overcast&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;overcast&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;Play&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;Don't Play&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;First we will show the results of the Decision Tree we made:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;y = df.Result.values
x = df.drop(&amp;#39;Result&amp;#39;,axis=1).values
dt = DecisionTree()
dt.fit(x, y,df.drop(&amp;#39;Result&amp;#39;,axis=1).columns)
print dt

Outlook
  |-&amp;gt; overcast:
  |     Play
  |-&amp;gt; no overcast:
  |     Temperature
  |     |-&amp;gt; &amp;lt; 80:
  |     |     Temperature
  |     |     |-&amp;gt; &amp;lt; 75:
  |     |     |     Temperature
  |     |     |     |-&amp;gt; &amp;lt; 71:
  |     |     |     |     Temperature
  |     |     |     |     |-&amp;gt; &amp;lt; 68:
  |     |     |     |     |     Don&amp;#39;t Play
  |     |     |     |     |-&amp;gt; &amp;gt;= 68:
  |     |     |     |     |     Play
  |     |     |     |-&amp;gt; &amp;gt;= 71:
  |     |     |     |     Don&amp;#39;t Play
  |     |     |-&amp;gt; &amp;gt;= 75:
  |     |     |     Play
  |     |-&amp;gt; &amp;gt;= 80:
  |     |     Don&amp;#39;t Play
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the decision tree can split on the same variable multiple times, and we can change the spliting criteria.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dt = DecisionTree(impurity_criterion=&amp;#39;gini&amp;#39;)
dt.fit(x, y,df.drop(&amp;#39;Result&amp;#39;,axis=1).columns)
print dt

Outlook
  |-&amp;gt; overcast:
  |     Play
  |-&amp;gt; no overcast:
  |     Temperature
  |     |-&amp;gt; &amp;lt; 80:
  |     |     Temperature
  |     |     |-&amp;gt; &amp;lt; 68:
  |     |     |     Don&amp;#39;t Play
  |     |     |-&amp;gt; &amp;gt;= 68:
  |     |     |     Temperature
  |     |     |     |-&amp;gt; &amp;lt; 71:
  |     |     |     |     Play
  |     |     |     |-&amp;gt; &amp;gt;= 71:
  |     |     |     |     Temperature
  |     |     |     |     |-&amp;gt; &amp;lt; 75:
  |     |     |     |     |     Don&amp;#39;t Play
  |     |     |     |     |-&amp;gt; &amp;gt;= 75:
  |     |     |     |     |     Play
  |     |-&amp;gt; &amp;gt;= 80:
  |     |     Don&amp;#39;t Play



dt = DecisionTree(depth=2)
dt.fit(x, y,df.drop(&amp;#39;Result&amp;#39;,axis=1).columns)
print dt

Outlook
  |-&amp;gt; overcast:
  |     Play
  |-&amp;gt; no overcast:
  |     Temperature
  |     |-&amp;gt; &amp;lt; 80:
  |     |     Play
  |     |-&amp;gt; &amp;gt;= 80:
  |     |     Don&amp;#39;t Play



dt = DecisionTree(leaf_size=7)
dt.fit(x, y,df.drop(&amp;#39;Result&amp;#39;,axis=1).columns)
print dt

Outlook
  |-&amp;gt; overcast:
  |     Play
  |-&amp;gt; no overcast:
  |     Temperature
  |     |-&amp;gt; &amp;lt; 80:
  |     |     Temperature
  |     |     |-&amp;gt; &amp;lt; 75:
  |     |     |     Play
  |     |     |-&amp;gt; &amp;gt;= 75:
  |     |     |     Play
  |     |-&amp;gt; &amp;gt;= 80:
  |     |     Don&amp;#39;t Play
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Post Pruning&lt;/h2&gt;
&lt;p&gt;We also implementing a post pruning procedure that takes a test set of data and labels, then prunes the tree to non decrease accuracy while reducing the size of the tree.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;X_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X_tst&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_tst&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_trn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_trn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy On Test Set:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_tst&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y_tst&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_tst&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_tst&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy On Test Set:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_tst&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y_tst&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;3&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;5.6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;5.6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;4.9&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;4.9&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt; &lt;span class="n"&gt;On&lt;/span&gt; &lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.906666666667&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;3&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|-&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt; &lt;span class="n"&gt;On&lt;/span&gt; &lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.92&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Decision Tree Code&lt;/h2&gt;
&lt;p&gt;The decision tree class is built on a TreeNode class, similar to the recusion part of the lesson.   We altered the original to allow tracking of parent nodes for up and down tree transversals.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TreeNode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    A node class for a decision tree.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c"&gt;# (int)    index of feature to split on&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c"&gt;# value of the feature to split on&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;  &lt;span class="c"&gt;# (bool) whether or not node is split on&lt;/span&gt;
                                 &lt;span class="c"&gt;# categorial feature&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;    &lt;span class="c"&gt;# (string) name of feature (or name of class in the&lt;/span&gt;
                            &lt;span class="c"&gt;#          case of a list)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;    &lt;span class="c"&gt;# (TreeNode) left child&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;   &lt;span class="c"&gt;# (TreeNode) right child&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;   &lt;span class="c"&gt;# (bool)   true if node is a leaf, false otherwise&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c"&gt;# (Counter) only necessary for leaf node:&lt;/span&gt;
                                  &lt;span class="c"&gt;#           key is class name and value is&lt;/span&gt;
                                  &lt;span class="c"&gt;#           count of the count of data points&lt;/span&gt;
                                  &lt;span class="c"&gt;#           that terminate at this leaf&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - x: 1d numpy array (single data point)&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - y: predicted label&lt;/span&gt;

&lt;span class="sd"&gt;        Return the predicted label for a single data point.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="n"&gt;col_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;col_value&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c"&gt;### REPLACE WITH YOUR CODE&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;col_value&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c"&gt;### REPLACE WITH YOUR CODE&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X_test: 2d np array&lt;/span&gt;
&lt;span class="sd"&gt;            - y_test: 1d np array&lt;/span&gt;
&lt;span class="sd"&gt;            - parent: Boolean&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;

&lt;span class="sd"&gt;        Prunes node if both children are leaves.  &lt;/span&gt;
&lt;span class="sd"&gt;        If not, call prune on children.&lt;/span&gt;
&lt;span class="sd"&gt;        If prune successful, call prune on parent node. &lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                
                &lt;span class="n"&gt;leftX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;
                &lt;span class="n"&gt;rightX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;
                &lt;span class="n"&gt;lefty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;righty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leftX&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;left_y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;leftX&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;left_y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rightX&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;right_y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;rightX&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;right_y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
                &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left_y_pred&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;lefty&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;right_y_pred&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;righty&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

                &lt;span class="n"&gt;new_counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_counter&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="n"&gt;most_common&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="n"&gt;node_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;node_acc&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_counter&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;most_common&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,:],&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,:],&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;



    &lt;span class="c"&gt;# This is for visualizing your tree. You don&amp;#39;t need to look into this code.&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;as_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - level: int (amount to indent)&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - prefix: str (to start the line with)&lt;/span&gt;

&lt;span class="sd"&gt;        Return a string representation of the tree rooted at this node.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;indent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;  |   &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;  |-&amp;gt; &amp;quot;&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;indent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;  |   &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;level&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;  &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;left_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;right_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;no &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;left_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;lt; &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;right_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;gt;= &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left_key&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_key&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__repr__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_string&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;



&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DecisionTree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    A decision tree class.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leaf_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stop_percentage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;impurity_criterion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;entropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        Initialize an empty DecisionTree.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c"&gt;# root Node&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c"&gt;# string names of features (for interpreting&lt;/span&gt;
                                   &lt;span class="c"&gt;# the tree)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c"&gt;# Boolean array of whether variable is&lt;/span&gt;
                                 &lt;span class="c"&gt;# categorical (or continuous)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impurity_criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_entropy&lt;/span&gt; \
                                  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;impurity_criterion&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;entropy&amp;#39;&lt;/span&gt; \
                                  &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_gini&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leaf_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;leaf_size&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;leaf_size&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e10&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_percentage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;stop_percentage&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_percentage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stop_percentage&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_percentage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error_threshold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;error_threshold&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;error_threshold&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - feature_names: numpy array of strings&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT: None&lt;/span&gt;

&lt;span class="sd"&gt;        Build the decision tree.&lt;/span&gt;
&lt;span class="sd"&gt;        X is a 2 dimensional array with each column being a feature and each&lt;/span&gt;
&lt;span class="sd"&gt;        row a data point.&lt;/span&gt;
&lt;span class="sd"&gt;        y is a 1 dimensional array with each value being the corresponding&lt;/span&gt;
&lt;span class="sd"&gt;        label.&lt;/span&gt;
&lt;span class="sd"&gt;        feature_names is an optional list containing the names of each of the&lt;/span&gt;
&lt;span class="sd"&gt;        features.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;

        &lt;span class="c"&gt;# Create True/False array of whether the variable is categorical&lt;/span&gt;
        &lt;span class="n"&gt;is_categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; \
                                   &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; \
                                   &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;unicode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_categorical&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_tree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_tree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - TreeNode&lt;/span&gt;

&lt;span class="sd"&gt;        Recursively build the decision tree. Return the root node.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TreeNode&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;splits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_choose_split_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;splits&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_tree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;

            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_tree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - float&lt;/span&gt;

&lt;span class="sd"&gt;        Return the entropy of the array y.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gini&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - float&lt;/span&gt;

&lt;span class="sd"&gt;        Return the gini impurity of the array y.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_make_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - split_index: int (index of feature)&lt;/span&gt;
&lt;span class="sd"&gt;            - split_value: int/float/bool/str (value of feature)&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X1: 2d numpy array (feature matrix for subset 1)&lt;/span&gt;
&lt;span class="sd"&gt;            - y1: 1d numpy array (labels for subset 1)&lt;/span&gt;
&lt;span class="sd"&gt;            - X2: 2d numpy array (feature matrix for subset 2)&lt;/span&gt;
&lt;span class="sd"&gt;            - y2: 1d numpy array (labels for subset 2)&lt;/span&gt;

&lt;span class="sd"&gt;        Return the two subsets of the dataset achieved by the given feature and&lt;/span&gt;
&lt;span class="sd"&gt;        value to split on.&lt;/span&gt;

&lt;span class="sd"&gt;        Call the method like this:&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;gt;&amp;gt;&amp;gt; X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)&lt;/span&gt;

&lt;span class="sd"&gt;        X1, y1 is a subset of the data.&lt;/span&gt;
&lt;span class="sd"&gt;        X2, y2 is the other subset of the data.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,:],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,:],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;condition&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_information_gain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y1: 1d numpy array (labels for subset 1)&lt;/span&gt;
&lt;span class="sd"&gt;            - y2: 1d numpy array (labels for subset 2)&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - float&lt;/span&gt;

&lt;span class="sd"&gt;        Return the information gain of making the given split.&lt;/span&gt;

&lt;span class="sd"&gt;        Use self.impurity_criterion(y) rather than calling _entropy or _gini&lt;/span&gt;
&lt;span class="sd"&gt;        directly.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impurity_criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impurity_criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; \
                &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impurity_criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_choose_split_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - index: int (index of feature)&lt;/span&gt;
&lt;span class="sd"&gt;            - value: int/float/bool/str (value of feature)&lt;/span&gt;
&lt;span class="sd"&gt;            - splits: (2d array, 1d array, 2d array, 1d array)&lt;/span&gt;

&lt;span class="sd"&gt;        Determine which feature and value to split on. Return the index and&lt;/span&gt;
&lt;span class="sd"&gt;        value of the optimal split along with the split of the dataset.&lt;/span&gt;

&lt;span class="sd"&gt;        Return None, None, None if there is no split which improves information&lt;/span&gt;
&lt;span class="sd"&gt;        gain.&lt;/span&gt;

&lt;span class="sd"&gt;        Call the method like this:&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;gt;&amp;gt;&amp;gt; index, value, splits = self._choose_split_index(X, y)&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;gt;&amp;gt;&amp;gt; X1, y1, X2, y2 = splits&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf_size&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaf_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;max_gain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1e10&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])):&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_make_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;gain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_information_gain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;gain&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_gain&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;max_gain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gain&lt;/span&gt;
                    &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;split_value&lt;/span&gt;
                    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;max_gain&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error_threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_make_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        INPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - X: 2d numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        OUTPUT:&lt;/span&gt;
&lt;span class="sd"&gt;            - y: 1d numpy array&lt;/span&gt;

&lt;span class="sd"&gt;        Return an array of predictions for the feature matrix X.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_one&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__str__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        Return string representation of the Decision Tree.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="kNN"></category><category term="K Nearest Neighbors"></category><category term="Decision Trees"></category></entry><entry><title>Galvanize - Week 03 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-05/" rel="alternate"></link><updated>2015-06-19T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-19:galvanize/galvanize-data-science-03-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 3 - Day 5&lt;/h2&gt;
&lt;p&gt;Today we had our checkin survey as a quiz, then we had a lecture on gradient decent.  We covered examples using linear and logistic regression.  The assignment was an all day paired sprint. &lt;/p&gt;
&lt;h2&gt;Gradient Descent&lt;/h2&gt;
&lt;p&gt;We will be implementing logistic regression using the gradient descent algorithm.  The goal in to include regulation and stocastic gradient decent.  We will start by testing it on data that will allow for simple solutions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.optimize&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;op&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/testdata.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;xp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;
&lt;span class="n"&gt;xn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;go&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Positive&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xn&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;xn&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Negative&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;First Feature&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Second Feature&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D5/output_1_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This data set is very nice because the positive and negative examples are linearly separable.   I'll have to remember this dataset when I try to implement support vector machines.  I can guess the sigmoid function that will fit this data.&lt;/p&gt;
&lt;p&gt;$$y = \frac{1}{1+e^{-2x_1}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x  = np.linspace(-6,8,100)
y1 = 1/(1+np.exp(-2.*(x)))
plt.plot(X[:,0],y,&amp;#39;ro&amp;#39;)
plt.plot(x,y1,&amp;#39;b-&amp;#39;)
plt.ylim([-0.1,1.1])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D5/output_3_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Cost function&lt;/h2&gt;
&lt;p&gt;In order to be able to evaluate if our gradient descent algorithm is working correctly, we will need to be able to calculate the cost.  The cost function we will be using is the &lt;em&gt;log likelihood&lt;/em&gt;. Our goal will be to &lt;em&gt;maximize&lt;/em&gt; this value, so we will actually be implementing gradient &lt;em&gt;ascent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;$$ \mathcal{l}(\Theta) = Log(\mathcal{L}(\Theta))= \Sigma_{i} ( \ y_i  \ log( \ h(x_i|\Theta) \ ) + (1-y_i) \ log( \ 1-h(x_i|\Theta) \ ) \ )$$&lt;/p&gt;
&lt;p&gt;where the hypothesis fucntion is &lt;/p&gt;
&lt;p&gt;$$ h(x|\Theta) = \frac{1}{1 \ + \ e^{\Theta x}} $$&lt;/p&gt;
&lt;p&gt;We will be using the gradient ascent potion of using &lt;/p&gt;
&lt;p&gt;$$\Theta_i = \Theta_i + \alpha \frac{\partial}{\partial\Theta_i} \mathcal{l}(\Theta)$$&lt;/p&gt;
&lt;p&gt;Where alpha is the learning rate for the update.   We can show that &lt;/p&gt;
&lt;p&gt;$$\frac{\partial}{\partial\Theta_i} \mathcal{l}(\Theta) = \Sigma_{i} ( \ y_i \ - \ h(x_i|\Theta) \ ) \ x_j $$&lt;/p&gt;
&lt;p&gt;We also implimented feature scaling, the options to fit the intercept, and Ridge (l2) penalizations.  This is done by subtracking a term from the cost function.&lt;/p&gt;
&lt;p&gt;$$ \mathcal{l}(\Theta) = Log(\mathcal{L}(\Theta))= \Sigma_{i} ( \ y_i  \ log( \ h(x_i|\Theta) \ ) + (1-y_i) \ log( \ 1-h(x_i|\Theta) \ ) \ ) - \lambda \Theta^2$$&lt;/p&gt;
&lt;p&gt;This changes the update function to&lt;/p&gt;
&lt;p&gt;$$\frac{\partial}{\partial\theta_i} \mathcal{l}(\Theta) = \Sigma_{i} ( \ y_i \ - \ h(x_i|\Theta) \ ) \ x_j - 2 \ \lambda \theta_i$$&lt;/p&gt;
&lt;p&gt;The larger the parameter lambda, the stronger the pull for the coefficients to be zero.&lt;/p&gt;
&lt;p&gt;The last method we implmeneted was a stocastic gradient decent. Where we shuffled the data and take a set for each data point.  We then reshuffle the data and take another stuff.   This, in practice, is faster than hill climbing.   We did not implement this with regularization.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;y&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;lamb&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;fit_intercept&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tol&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;    
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_std&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;add_intercept&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;row_hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;


    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;X&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;X&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nx"&gt;thresh&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="nx"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_std&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="nx"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;add_intercept&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nx"&gt;thresh&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;llh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;llh&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;    
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;


    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;stoch_log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;xt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;xt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;row_hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)).&lt;/span&gt;&lt;span class="nx"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="nx"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt;
            &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;temp&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;stoch_gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="nx"&gt;rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="k"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="k"&gt;rows&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;STOCK LIKI:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;stoch_log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Shapes:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;stoch_log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;
                &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;stoch_log_likelihood_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nx"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;Sprint&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TEMP:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;
            &lt;span class="nx"&gt;lik_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt;
            &lt;span class="nx"&gt;previous_likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;temp&lt;/span&gt;

            &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="k"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="k"&gt;rows&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;add_intercept&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;ones&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nx"&gt;ones&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;ones&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;ones&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nx"&gt;ones&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;X&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;ones&lt;/span&gt;


    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_std&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;x_std&lt;/span&gt; 
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;




&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="nx"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;t2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;T2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;t1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;t2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;x1&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;t1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y1&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;t2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;x1&lt;/span&gt;
        &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;theta&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;y1&lt;/span&gt;
        &lt;span class="nx"&gt;Z&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;j&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;CS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;T2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;clabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;CS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;inline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;First Feature&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Second Feature&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Controur of Log Likelihood&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;text&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Text&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="mh"&gt;0x10a552c50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D5/output_6_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.pcolor(Z)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D5/output_7_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;These plots are showing how seperable the data is.   We could fit this data with a number of logisitic function perfectly.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x  = np.linspace(-6,8,100)
y1 = 1/(1+np.exp(-2.*(x)))
y2 = 1/(1+np.exp(-10.*(x)))
y3 = 1/(1+np.exp(-100.*(x)))
plt.plot(X[:,0],y,&amp;#39;ro&amp;#39;)
plt.plot(x,y1,&amp;#39;b-&amp;#39;)
plt.plot(x,y2,&amp;#39;b-&amp;#39;)
plt.plot(x,y3,&amp;#39;b-&amp;#39;)
plt.ylim([-0.1,1.1])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D5/output_9_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;So for any logistic function with a postive constant with a first feature will fit the data we have.  &lt;/p&gt;
&lt;h2&gt;Compare to Sklearn&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.01688462&lt;/span&gt;  &lt;span class="mf"&gt;1.27894634&lt;/span&gt;  &lt;span class="mf"&gt;0.00995178&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;



&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;

&lt;span class="n"&gt;lin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.04652751&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;1.2318561&lt;/span&gt;   &lt;span class="mf"&gt;0.02251709&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;


&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Library&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;449&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DataConversionWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;passed&lt;/span&gt; &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Please&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
  &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;column_or_1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;warn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This give similar results to sklearn.   The sklearn package does seem to be faster, but I also believe it is using a c package called liblinear for its optimization, while we are doing it in pure python.&lt;/p&gt;
&lt;h2&gt;Checking Scaling&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.17710665&lt;/span&gt;  &lt;span class="mf"&gt;1.17554584&lt;/span&gt;  &lt;span class="mf"&gt;0.05796619&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;



&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.01688462&lt;/span&gt;  &lt;span class="mf"&gt;1.27894634&lt;/span&gt;  &lt;span class="mf"&gt;0.00995178&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.227847971687&lt;/span&gt;



&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.09595542&lt;/span&gt;  &lt;span class="mf"&gt;0.92911705&lt;/span&gt;  &lt;span class="mf"&gt;0.0366225&lt;/span&gt; &lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Graduate Student Data&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/grad.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skiprows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;yg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;xg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.000001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.84699307&lt;/span&gt;  &lt;span class="mf"&gt;0.00229446&lt;/span&gt;  &lt;span class="mf"&gt;0.76299305&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.54843734&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;





&lt;span class="mf"&gt;0.70499999999999996&lt;/span&gt;




&lt;span class="n"&gt;log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;yg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xg&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;yg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.18847876&lt;/span&gt;  &lt;span class="mf"&gt;0.00191577&lt;/span&gt;  &lt;span class="mf"&gt;0.21564289&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.59842009&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mf"&gt;0.715&lt;/span&gt;


&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Library&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;449&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DataConversionWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;passed&lt;/span&gt; &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Please&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
  &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;column_or_1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;warn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Regularization&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.09595542&lt;/span&gt;  &lt;span class="mf"&gt;0.92911705&lt;/span&gt;  &lt;span class="mf"&gt;0.0366225&lt;/span&gt; &lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;



&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.02568882&lt;/span&gt;  &lt;span class="mf"&gt;0.53308379&lt;/span&gt;  &lt;span class="mf"&gt;0.01324245&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;



&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.00178464&lt;/span&gt;  &lt;span class="mf"&gt;0.22617375&lt;/span&gt;  &lt;span class="mf"&gt;0.00074595&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;



&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;  &lt;span class="mf"&gt;3.04588371e-06&lt;/span&gt;   &lt;span class="mf"&gt;4.27234980e-02&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.65650480e-04&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Stochastic Gradient Descent&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nf"&gt;%timeit&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stoch_gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;69.314718056&lt;/span&gt;
&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;slowest&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;68.69&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt; &lt;span class="n"&gt;longer&lt;/span&gt; &lt;span class="n"&gt;than&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;fastest&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="n"&gt;could&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;intermediate&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;being&lt;/span&gt; &lt;span class="n"&gt;cached&lt;/span&gt; 
&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.29&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000261667070057&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;1.40603903&lt;/span&gt;  &lt;span class="mf"&gt;2.90379792&lt;/span&gt;  &lt;span class="mf"&gt;0.22544191&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;



&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regression_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;fit_intercept&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nf"&gt;%timeit&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradient_ascent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_likelihood&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_coeff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;69.314718056&lt;/span&gt;
&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;slowest&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;1034.52&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt; &lt;span class="n"&gt;longer&lt;/span&gt; &lt;span class="n"&gt;than&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;fastest&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="n"&gt;could&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;intermediate&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;being&lt;/span&gt; &lt;span class="n"&gt;cached&lt;/span&gt; 
&lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;129&lt;/span&gt; &lt;span class="err"&gt;µ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000393662821043&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;1.31662671&lt;/span&gt;  &lt;span class="mf"&gt;2.79854521&lt;/span&gt;  &lt;span class="mf"&gt;0.25109632&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Newton's Method for a single variable&lt;/h2&gt;
&lt;p&gt;We were told to use newton's method for root finding on the following function:&lt;/p&gt;
&lt;p&gt;$$f(x) = 6 \ x^2 + 3 \ x - 10$$&lt;/p&gt;
&lt;p&gt;This function has two roots: -1.565 and 1.0650.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.00002&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;newton_roots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tolerance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-3&lt;/span&gt;
    &lt;span class="n"&gt;xo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;toll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;toll&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;tolerance&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;xo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xo&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xo&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;xo&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;toll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xo&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1e6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xo&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;

&lt;span class="n"&gt;cnt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newton_roots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;

&lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="mf"&gt;1.065&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;54&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.565&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see that we get the two roots about equal number of times.  &lt;/p&gt;</summary><category term="data-science"></category><category term="gradient decent"></category><category term="stocastic gradient decent"></category><category term="logistic regression"></category></entry><entry><title>Galvanize - Week 03 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-04/" rel="alternate"></link><updated>2015-06-18T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-18:galvanize/galvanize-data-science-03-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 3 - Day 4&lt;/h2&gt;
&lt;p&gt;Today we had a 2 hour assessment on everything we covered.  There were programming style problem and 1 math style problems.  The topics were on everything we have covered up to now.  &lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;The lectures today were on Logistic Regresion, Odds, and ROC Curves.   &lt;/p&gt;
&lt;h2&gt;ROC Curve&lt;/h2&gt;
&lt;p&gt;We were told that one of the best ways to evaluate how a classifier performs is an &lt;a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;ROC curve&lt;/a&gt;.   They display the change in the false and true positive rates as paramters in the model change.  In the case of logistic regression, its the threshold use to classify a data point based on the predicted probability. &lt;/p&gt;
&lt;p&gt;Recall that the &lt;em&gt;true positive rate&lt;/em&gt; is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; number of true positives     number correctly predicted positive
-------------------------- = -------------------------------------
 number of positive cases           number of positive cases
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the &lt;em&gt;false positive rate&lt;/em&gt; is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; number of false positives     number incorrectly predicted positive
--------------------------- = ---------------------------------------
  number of negative cases           number of negative cases
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_classification&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;TPR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;FPR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thresh&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="n"&gt;thresh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;TPR&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;FPR&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;TPR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TPR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;FPR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FPR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;TPR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;FPR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;thresholds&lt;/span&gt;


&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_classification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_redundant&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_informative&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;n_clusters_per_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ROC plot of fake data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_1_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;roc&lt;/span&gt;
&lt;span class="n"&gt;fpr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tpr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;threshs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Bryans ROC&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sklearn ROC&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ROC plot of fake data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_2_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The results between the two curves is negligable on a fake dataset.  We are now going to do this with FICO data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/loanf.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Interest.Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;FICO.Score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Loan.Length&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Loan.Amount&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;


&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.calibration&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CalibratedClassifierCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegressionCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;

&lt;span class="n"&gt;a_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;cal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CalibratedClassifierCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isotonic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ROC plot of Loan Data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AUC: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Recall: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Precision: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_5_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; AUC:  0.763176276353
Accuracy:  0.769333333333
Recall:  0.744094488189
Precision:  0.636363636364
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The the model is not a great model, but it does start off identifying 3 out of 4 true positives and has 6 our of 10 of those predicted to be positive actually being positive.  &lt;/p&gt;
&lt;h2&gt;Graduate School Admissions&lt;/h2&gt;
&lt;p&gt;The data we will be using net is admission data on Grad school acceptances.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;admit&lt;/code&gt;: whether or not the applicant was admitted to grad. school&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gpa&lt;/code&gt;: undergraduate GPA&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GRE&lt;/code&gt;: score of GRE test&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rank&lt;/code&gt;: prestige of undergraduate school (1 is highest prestige, ala Harvard)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.&lt;/p&gt;
&lt;p&gt;Before we get to predictions, we should do some data exploration.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Load in the dataset into pandas: &lt;code&gt;data/grad.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;grad = pd.read_csv('data/grad.csv')
grad.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;admit&lt;/th&gt;
      &lt;th&gt;gre&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;rank&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;380&lt;/td&gt;
      &lt;td&gt;3.61&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;660&lt;/td&gt;
      &lt;td&gt;3.67&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;800&lt;/td&gt;
      &lt;td&gt;4.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;640&lt;/td&gt;
      &lt;td&gt;3.19&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;520&lt;/td&gt;
      &lt;td&gt;2.93&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;grad.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 400 entries, 0 to 399
Data columns (total 4 columns):
admit    400 non-null int64
gre      400 non-null int64
gpa      400 non-null float64
rank     400 non-null int64
dtypes: float64(1), int64(3)
memory usage: 15.6 KB



grad.describe()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;admit&lt;/th&gt;
      &lt;th&gt;gre&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;rank&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.00000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;0.317500&lt;/td&gt;
      &lt;td&gt;587.700000&lt;/td&gt;
      &lt;td&gt;3.389900&lt;/td&gt;
      &lt;td&gt;2.48500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;0.466087&lt;/td&gt;
      &lt;td&gt;115.516536&lt;/td&gt;
      &lt;td&gt;0.380567&lt;/td&gt;
      &lt;td&gt;0.94446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;220.000000&lt;/td&gt;
      &lt;td&gt;2.260000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;520.000000&lt;/td&gt;
      &lt;td&gt;3.130000&lt;/td&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;580.000000&lt;/td&gt;
      &lt;td&gt;3.395000&lt;/td&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;660.000000&lt;/td&gt;
      &lt;td&gt;3.670000&lt;/td&gt;
      &lt;td&gt;3.00000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;800.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;4.00000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;temp = pd.crosstab(grad[&amp;#39;admit&amp;#39;],grad[&amp;#39;rank&amp;#39;])
temp
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;rank&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;admit&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;93&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;temp.transpose().plot(kind=&amp;#39;bar&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x10a7a8a90&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_11_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that if a person is applying to grad school from a rank 1 school, they are more likely than not to be accepeted.   We also see that this ratio drops as the rank of the current school is lower.   We also see that most of the dat is from ranke 2 and rank 3 scores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(temp/temp.sum()).transpose().plot(kind=&amp;#39;bar&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x10f591a50&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_13_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the rations instead of hte counts highlight that change.   There is an increase in chances of getting accepted if the person is coming from a better school.   &lt;/p&gt;
&lt;p&gt;Lets look at the GRE and GPA distributions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;grad.gre.hist()




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x10f854990&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_15_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;grad.gpa.hist()




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x10f8f2a10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_16_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Both of these are skewed left, but we do have a cut off on the GPA not being above 4.  We see a spike there, and this is commonly seen in any data with arbitary cutoffs.   If there was no max GPA, we would expenct that some people would have 5's,6's, and so on.  They might be rare, but they are there.  The cap compresses all these overacheivers to 4.0.  &lt;/p&gt;
&lt;h2&gt;Fitting Grad School Admissions&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.discrete.discrete_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;admit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gpa&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
         &lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.574302&lt;/span&gt;
         &lt;span class="n"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;Logit Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;       &lt;td&gt;admit&lt;/td&gt;      &lt;th&gt;  No. Observations:  &lt;/th&gt;  &lt;td&gt;   400&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;               &lt;td&gt;Logit&lt;/td&gt;      &lt;th&gt;  Df Residuals:      &lt;/th&gt;  &lt;td&gt;   396&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;               &lt;td&gt;MLE&lt;/td&gt;       &lt;th&gt;  Df Model:          &lt;/th&gt;  &lt;td&gt;     3&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;          &lt;td&gt;Fri, 19 Jun 2015&lt;/td&gt; &lt;th&gt;  Pseudo R-squ.:     &lt;/th&gt;  &lt;td&gt;0.08107&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;              &lt;td&gt;07:23:59&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -229.72&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;converged:&lt;/th&gt;           &lt;td&gt;True&lt;/td&gt;       &lt;th&gt;  LL-Null:           &lt;/th&gt; &lt;td&gt; -249.99&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt; &lt;/th&gt;                      &lt;td&gt; &lt;/td&gt;        &lt;th&gt;  LLR p-value:       &lt;/th&gt; &lt;td&gt;8.207e-09&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt;   -3.4495&lt;/td&gt; &lt;td&gt;    1.133&lt;/td&gt; &lt;td&gt;   -3.045&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt;   -5.670    -1.229&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gre&lt;/th&gt;   &lt;td&gt;    0.0023&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    2.101&lt;/td&gt; &lt;td&gt; 0.036&lt;/td&gt; &lt;td&gt;    0.000     0.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gpa&lt;/th&gt;   &lt;td&gt;    0.7770&lt;/td&gt; &lt;td&gt;    0.327&lt;/td&gt; &lt;td&gt;    2.373&lt;/td&gt; &lt;td&gt; 0.018&lt;/td&gt; &lt;td&gt;    0.135     1.419&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rank&lt;/th&gt;  &lt;td&gt;   -0.5600&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;   -4.405&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.809    -0.311&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We see that the model is not very good.  The Pseudo R-square, which as to do with the deviance or negative log likelihood, is very low.  The coefficients are interested in that they do predict more likely admision for higher gre, higher gpa, and better schools.   &lt;/p&gt;
&lt;p&gt;I think I want to plot the over of the predictions to show how poor the model is.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;xp = X.values[(y.values==1)[:,0],:]
yp = model.fittedvalues.values[(y.values==1)[:,0]]
xn = X.values[(y.values==0)[:,0],:]
yn = model.fittedvalues.values[(y.values==0)[:,0]]
zp = xp.dot(model.params.values)
zn = xn.dot(model.params.values)
plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(zp,np.exp(yp)/(1+np.exp(yp)),&amp;#39;go&amp;#39;,alpha=0.5)
plt.xlabel(&amp;quot;Logodds for Datapoint&amp;quot;)
plt.ylabel(&amp;quot;Prob for Student Admited&amp;quot;)
plt.subplot(1,2,2)
plt.plot(zn,np.exp(yn)/(1+np.exp(yn)),&amp;#39;ro&amp;#39;,alpha=0.5)
plt.xlabel(&amp;quot;Logodds for Datapoint&amp;quot;)
plt.ylabel(&amp;quot;Prob for Student Not Admited&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_20_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This model does not do a great job of predicting admissions, but we can attemp to use Sklearn's machinery to get a better fit, and also measure the model metrics simply.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegressionCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;roc_auc_score&lt;/span&gt;


&lt;span class="n"&gt;a_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gpa&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;admit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CalibratedClassifierCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isotonic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegressionCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ROC plot of College Data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AUC: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Recall: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Precision: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;b_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_22_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.613445378151&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.666666666667&lt;/span&gt;
&lt;span class="n"&gt;Recall&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.485714285714&lt;/span&gt;
&lt;span class="n"&gt;Precision&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.435897435897&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The area under the ROC curve is not far from 0.5 (random guessing).  The model only predicts 50% of the students admitted to college of being admitted, and only 43% of those predicted to be admitted were actually admitted.   I do not think the College Board will be breaking down our doors for this model.&lt;/p&gt;
&lt;p&gt;In one way we treated the Rank as a continuous variable, and could try treating it like a categorical variable instead.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;grad1 = pd.get_dummies(grad, columns=[&amp;#39;rank&amp;#39;])
a_train, a_test, b_train, b_test = train_test_split(grad1.drop([&amp;#39;admit&amp;#39;,&amp;#39;rank_4&amp;#39;],axis=1).values, grad1.admit.values, test_size=0.30, random_state=42)

lin = LogisticRegression()
cal2=CalibratedClassifierCV(lin, method=&amp;#39;isotonic&amp;#39;, cv=20)
cal2.fit(a_train,b_train)

probs = cal2.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&amp;#39;seagreen&amp;#39;,alpha=0.8,lw=3,label=&amp;quot;Original&amp;quot;)
plt.plot(fpr1, tpr1,color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=3,label=&amp;quot;With Rank Categories&amp;quot;)
plt.xlabel(&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;)
plt.ylabel(&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;)
plt.title(&amp;quot;ROC plot of College Data&amp;quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &amp;quot;AUC: &amp;quot;, roc_auc_score(cal2.predict(a_test),b_test)
print &amp;quot;Accuracy: &amp;quot;, accuracy_score(cal2.predict(a_test),b_test)
print &amp;quot;Recall: &amp;quot;, recall_score(cal2.predict(a_test),b_test)
print &amp;quot;Precision: &amp;quot;, precision_score(cal2.predict(a_test),b_test)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_24_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.640625&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.691666666667&lt;/span&gt;
&lt;span class="n"&gt;Recall&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.53125&lt;/span&gt;
&lt;span class="n"&gt;Precision&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.435897435897&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could have an initial pass where we want a TPR &amp;gt; 60% and FPR &amp;lt; 40.  We can find the thresholds for these values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;thresholds[(tpr &amp;gt; 0.6)&amp;amp;(fpr &amp;lt; 0.4)]




array([ 0.24242424,  0.25252525,  0.26262626])
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Beta coefficients as Odds Ratio&lt;/h2&gt;
&lt;p&gt;One thing that is often lost when talking about logistic regression is the idea of the odds ratio, or rather the probabilistic interpretation of the model. For this next part we will get hands on with the odds ratio.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;odds ratio&lt;/em&gt;&lt;/strong&gt; is defined as the product of the exponential of each coefficient.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="images/odds_ratio.png" /&gt;&lt;/p&gt;
&lt;p&gt;This is the odds of being admitted over not being admitted.&lt;/p&gt;
&lt;p&gt;It tells you how much a one unit increase of a feature corresponds to the odds of being admitted to grad school. And in doing so the coefficients of the logistic regression can be interpreted similarly to the coefficients of linear regression.&lt;/p&gt;
&lt;p&gt;From our model we can look at the beta coefficients (Intercept,GRE, GPA, Rank)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = cal1.calibrated_classifiers_[-1].__dict__[&amp;#39;base_estimator&amp;#39;].coef_[0,:]
print beta
beta = np.hstack((cal1.calibrated_classifiers_[-1].__dict__[&amp;#39;base_estimator&amp;#39;].intercept_[0],beta))
beta

[ 0.00084169  0.36208945 -0.54024057]





array([ -1.19927209e+00,   8.41691796e-04,   3.62089453e-01,
        -5.40240569e-01])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The odd ratios for each beta is given by:&lt;/p&gt;
&lt;p&gt;$$\mbox{odds ratio} = e^{\beta_i}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;np.exp(beta)




array([ 0.30141353,  1.00084205,  1.43632742,  0.58260808])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This means that the base line odd ration of getting into grad school is 0.3.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Increasing your gre by 1 point increases your odds by 1.0008.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Increasing your gpa by 1 point increases your ods by 1.435.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decreasing your rank of college 1 unit decreases your odds by 0.582.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can also ask how much change would result in doubling the odds ratio:&lt;/p&gt;
&lt;p&gt;$$x_i = \frac{ln(2)}{\beta_i}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;np.log(2)/beta




array([ -5.77973244e-01,   8.23516617e+02,   1.91429818e+00,
        -1.28303430e+00])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Increasing my GRE by 824 doubles my odds ration.  Increasing my GPA by 1.91 doubles the odds ratio, and increasing the ranking by 1.28 doubles the odds ratio.  &lt;/p&gt;
&lt;h2&gt;Predicted Probabilities&lt;/h2&gt;
&lt;p&gt;Now let's actually play with our data to verify what we calculated above with the Odds Ratio.  We can look, on average, how the rank changes the odds.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;g = grad.groupby(&amp;#39;rank&amp;#39;).mean().reset_index().drop(&amp;#39;admit&amp;#39;,axis=1)
g
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;rank&lt;/th&gt;
      &lt;th&gt;gre&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;611.803279&lt;/td&gt;
      &lt;td&gt;3.453115&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;596.026490&lt;/td&gt;
      &lt;td&gt;3.361656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;574.876033&lt;/td&gt;
      &lt;td&gt;3.432893&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;570.149254&lt;/td&gt;
      &lt;td&gt;3.318358&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;p_g = cal1.predict_proba(g[[&amp;#39;gre&amp;#39;,&amp;#39;gpa&amp;#39;,&amp;#39;rank&amp;#39;]].values)[:,1]
p_g




array([ 0.68469485,  0.3519206 ,  0.17594864,  0.05175475])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can use this to calculate the odds rations for each rank&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;odds_g = p_g/(1-p_g)
final = np.vstack((np.arange(1,5),np.vstack((p_g,odds_g))))
final




array([[ 1.        ,  2.        ,  3.        ,  4.        ],
       [ 0.68469485,  0.3519206 ,  0.17594864,  0.05175475],
       [ 2.1715308 ,  0.54302082,  0.21351659,  0.0545795 ]])




predicted_odds = np.hstack((np.array(2.1715408),(odds_g*0.583)[:3]))
predicted_odds




array([ 2.1715408 ,  1.26600246,  0.31658114,  0.12448017])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The odds to drop, but they do not match the vlaues we have from the average predictions.  We can make a graph of the log odds and find the slope:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pd.DataFrame({&amp;quot;logodds&amp;quot;:np.log(odds_g),&amp;quot;rank&amp;quot;:range(1,5)}).set_index(&amp;#39;rank&amp;#39;).plot(kind=&amp;#39;bar&amp;#39;)
print -1.7/3,beta[3]

-0.566666666667 -0.540240569018
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_41_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case the slop of the logodds and the fitted coefficient match up very close.  Lets do this for the GRE and GPA&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;admit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;p_g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gpa&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;odds_g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_g&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p_g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;odds_g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;go&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;linear&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gre&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;odds_g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="mf"&gt;0.00138822870627&lt;/span&gt; &lt;span class="mf"&gt;0.000841691795892&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_43_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;They are the same scale, but differ by a factor 1.5.   This makes sense because we have a few clear outliers in the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gpa&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;admit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;p_g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cal1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gpa&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;odds_g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_g&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p_g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;odds_g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;go&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;linear&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;#linear.fit(g.gpa.values,odds_g)&lt;/span&gt;
&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;132&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;odds_g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;132&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="mf"&gt;0.506303444619&lt;/span&gt; &lt;span class="mf"&gt;0.362089452847&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_45_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This is also close, and the same order of magnitude.  But we have some clear outliers that need to be address in with this model.   The fact tha the coefficients of the model have a interpretation in terms of odds is great.  It adds a hook to reality in this abstraction. &lt;/p&gt;
&lt;h2&gt;MOOCS&lt;/h2&gt;
&lt;p&gt;This is the future!  No one goes to physical schools any more and MOOCs rule the world.&lt;/p&gt;
&lt;p&gt;Harvard and MIT have &lt;a href="http://newsoffice.mit.edu/2014/mit-and-harvard-release-de-identified-learning-data-open-online-courses"&gt;released&lt;/a&gt; a great dataset around engagement statistics for their MOOC courses. One of the biggest issues with MOOCs is engagement. We will try to predict the probability of 'engagement' of a student given all the other columns.  We will define engagement here as either: &lt;code&gt;explored == 1 OR certified == 1&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mooc = pd.read_csv(&amp;#39;data/mooc.csv&amp;#39;)
mooc[&amp;#39;engagement&amp;#39;] = mooc[&amp;#39;explored&amp;#39;]+mooc[&amp;#39;certified&amp;#39;]
mooc[&amp;#39;engagement&amp;#39;] = np.where(mooc.engagement &amp;gt; 0,1,0)
mooc.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;course_id&lt;/th&gt;
      &lt;th&gt;userid_DI&lt;/th&gt;
      &lt;th&gt;registered&lt;/th&gt;
      &lt;th&gt;viewed&lt;/th&gt;
      &lt;th&gt;explored&lt;/th&gt;
      &lt;th&gt;certified&lt;/th&gt;
      &lt;th&gt;final_cc_cname_DI&lt;/th&gt;
      &lt;th&gt;LoE_DI&lt;/th&gt;
      &lt;th&gt;YoB&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;last_event_DI&lt;/th&gt;
      &lt;th&gt;nevents&lt;/th&gt;
      &lt;th&gt;ndays_act&lt;/th&gt;
      &lt;th&gt;nplay_video&lt;/th&gt;
      &lt;th&gt;nchapters&lt;/th&gt;
      &lt;th&gt;nforum_posts&lt;/th&gt;
      &lt;th&gt;roles&lt;/th&gt;
      &lt;th&gt;incomplete_flag&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;HarvardX/CB22x/2013_Spring&lt;/td&gt;
      &lt;td&gt;MHxPC130442623&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;2013-11-17&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;HarvardX/CS50x/2012&lt;/td&gt;
      &lt;td&gt;MHxPC130442623&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;HarvardX/CB22x/2013_Spring&lt;/td&gt;
      &lt;td&gt;MHxPC130275857&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;2013-11-17&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;HarvardX/CS50x/2012&lt;/td&gt;
      &lt;td&gt;MHxPC130275857&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;HarvardX/ER22x/2013_Spring&lt;/td&gt;
      &lt;td&gt;MHxPC130275857&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 21 columns&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;mg = mooc.groupby(&amp;#39;course_id&amp;#39;)[&amp;#39;course_id&amp;#39;,&amp;#39;viewed&amp;#39;,&amp;#39;explored&amp;#39;,&amp;#39;certified&amp;#39;,&amp;#39;engagement&amp;#39;].mean()
mg = mg.sort([&amp;#39;engagement&amp;#39;])
mg
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;viewed&lt;/th&gt;
      &lt;th&gt;explored&lt;/th&gt;
      &lt;th&gt;certified&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;course_id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;HarvardX/CB22x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.543764&lt;/td&gt;
      &lt;td&gt;0.018232&lt;/td&gt;
      &lt;td&gt;0.012799&lt;/td&gt;
      &lt;td&gt;0.018299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/3.091x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.960906&lt;/td&gt;
      &lt;td&gt;0.023457&lt;/td&gt;
      &lt;td&gt;0.022479&lt;/td&gt;
      &lt;td&gt;0.028506&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;HarvardX/PH278x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.379173&lt;/td&gt;
      &lt;td&gt;0.030049&lt;/td&gt;
      &lt;td&gt;0.017954&lt;/td&gt;
      &lt;td&gt;0.031867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/6.002x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.480549&lt;/td&gt;
      &lt;td&gt;0.040612&lt;/td&gt;
      &lt;td&gt;0.026670&lt;/td&gt;
      &lt;td&gt;0.041511&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/8.MReV/2013_Summer&lt;/th&gt;
      &lt;td&gt;0.708663&lt;/td&gt;
      &lt;td&gt;0.039675&lt;/td&gt;
      &lt;td&gt;0.031339&lt;/td&gt;
      &lt;td&gt;0.042841&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/6.00x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.944850&lt;/td&gt;
      &lt;td&gt;0.046678&lt;/td&gt;
      &lt;td&gt;0.021710&lt;/td&gt;
      &lt;td&gt;0.046695&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/8.02x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.669447&lt;/td&gt;
      &lt;td&gt;0.058039&lt;/td&gt;
      &lt;td&gt;0.026475&lt;/td&gt;
      &lt;td&gt;0.058071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/3.091x/2012_Fall&lt;/th&gt;
      &lt;td&gt;0.493352&lt;/td&gt;
      &lt;td&gt;0.063032&lt;/td&gt;
      &lt;td&gt;0.044460&lt;/td&gt;
      &lt;td&gt;0.063032&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/6.00x/2012_Fall&lt;/th&gt;
      &lt;td&gt;0.620716&lt;/td&gt;
      &lt;td&gt;0.062685&lt;/td&gt;
      &lt;td&gt;0.037119&lt;/td&gt;
      &lt;td&gt;0.063074&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;HarvardX/CS50x/2012&lt;/th&gt;
      &lt;td&gt;0.625430&lt;/td&gt;
      &lt;td&gt;0.064986&lt;/td&gt;
      &lt;td&gt;0.007588&lt;/td&gt;
      &lt;td&gt;0.065016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;HarvardX/ER22x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.560238&lt;/td&gt;
      &lt;td&gt;0.061527&lt;/td&gt;
      &lt;td&gt;0.040867&lt;/td&gt;
      &lt;td&gt;0.069435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/6.002x/2012_Fall&lt;/th&gt;
      &lt;td&gt;0.637549&lt;/td&gt;
      &lt;td&gt;0.073951&lt;/td&gt;
      &lt;td&gt;0.042881&lt;/td&gt;
      &lt;td&gt;0.074343&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/7.00x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.622686&lt;/td&gt;
      &lt;td&gt;0.073826&lt;/td&gt;
      &lt;td&gt;0.039174&lt;/td&gt;
      &lt;td&gt;0.074825&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/2.01x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.682436&lt;/td&gt;
      &lt;td&gt;0.098853&lt;/td&gt;
      &lt;td&gt;0.043601&lt;/td&gt;
      &lt;td&gt;0.099029&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;HarvardX/PH207x/2012_Fall&lt;/th&gt;
      &lt;td&gt;0.583742&lt;/td&gt;
      &lt;td&gt;0.104155&lt;/td&gt;
      &lt;td&gt;0.044287&lt;/td&gt;
      &lt;td&gt;0.104179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MITx/14.73x/2013_Spring&lt;/th&gt;
      &lt;td&gt;0.588016&lt;/td&gt;
      &lt;td&gt;0.105310&lt;/td&gt;
      &lt;td&gt;0.074812&lt;/td&gt;
      &lt;td&gt;0.105633&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The goal is to attempt to predict engagement for a user based on data they do not have before they start the course.   This is only self-reported information like the Highest Level of Education, Year of Birth, Gender, and when the course started.   We can also attemp to predict it including the course id, but this will not generalize to other courses.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;m1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mooc&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;LoE_DI&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;YoB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u&amp;#39;start_time_DI&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;engagement&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LoE_DI&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LoE_DI&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Library&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;IPython&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;__main__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;SettingWithCopyWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;trying&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="nb"&gt;slice&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;a&lt;/span&gt; &lt;span class="nn"&gt;DataFrame.&lt;/span&gt;
&lt;span class="n"&gt;Try&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;row_indexer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;col_indexer&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;instead&lt;/span&gt;

&lt;span class="n"&gt;See&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;caveats&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;documentation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pydata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;stable&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;indexing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="c"&gt;#indexing-view-versus-copy&lt;/span&gt;
  &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.kernel.zmq&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kernelapp&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;LoE_DI&lt;/th&gt;
      &lt;th&gt;YoB&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gender&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gender&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Library&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;IPython&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;__main__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;SettingWithCopyWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;trying&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="nb"&gt;slice&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;a&lt;/span&gt; &lt;span class="nn"&gt;DataFrame.&lt;/span&gt;
&lt;span class="n"&gt;Try&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;row_indexer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;col_indexer&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;instead&lt;/span&gt;

&lt;span class="n"&gt;See&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;caveats&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;documentation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pydata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;stable&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;indexing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="c"&gt;#indexing-view-versus-copy&lt;/span&gt;
  &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.kernel.zmq&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kernelapp&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;LoE_DI&lt;/th&gt;
      &lt;th&gt;YoB&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;f&lt;/th&gt;
      &lt;th&gt;o&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;m1 = m1.drop([&amp;#39;LoE_DI&amp;#39;,&amp;#39;gender&amp;#39;],axis=1)
m1.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;YoB&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;f&lt;/th&gt;
      &lt;th&gt;o&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;m1.YoB = pd.cut(m1.YoB,[0,1960,1970,1980,1990,2000,2010,2020])
for x in m1.YoB.unique()[1:]:
    m1[x] = np.nan
    m1.loc[:,x] = np.where(m1.YoB==x,1,0)
m1.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;YoB&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;f&lt;/th&gt;
      &lt;th&gt;o&lt;/th&gt;
      &lt;th&gt;(2010, 2020]&lt;/th&gt;
      &lt;th&gt;(1980, 1990]&lt;/th&gt;
      &lt;th&gt;(1960, 1970]&lt;/th&gt;
      &lt;th&gt;(1970, 1980]&lt;/th&gt;
      &lt;th&gt;(1990, 2000]&lt;/th&gt;
      &lt;th&gt;(0, 1960]&lt;/th&gt;
      &lt;th&gt;(2000, 2010]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;m1 = m1.drop(&amp;#39;YoB&amp;#39;,axis=1)
m1.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;start_time_DI&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;f&lt;/th&gt;
      &lt;th&gt;o&lt;/th&gt;
      &lt;th&gt;(2010, 2020]&lt;/th&gt;
      &lt;th&gt;(1980, 1990]&lt;/th&gt;
      &lt;th&gt;(1960, 1970]&lt;/th&gt;
      &lt;th&gt;(1970, 1980]&lt;/th&gt;
      &lt;th&gt;(1990, 2000]&lt;/th&gt;
      &lt;th&gt;(0, 1960]&lt;/th&gt;
      &lt;th&gt;(2000, 2010]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2012-10-15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2012-09-17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2012-12-19&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;m1 = m1.drop([&amp;#39;start_time_DI&amp;#39;],axis=1)
m1.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;engagement&lt;/th&gt;
      &lt;th&gt;Secondary&lt;/th&gt;
      &lt;th&gt;Bachelor's&lt;/th&gt;
      &lt;th&gt;Master's&lt;/th&gt;
      &lt;th&gt;Doctorate&lt;/th&gt;
      &lt;th&gt;Less than Secondary&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;f&lt;/th&gt;
      &lt;th&gt;o&lt;/th&gt;
      &lt;th&gt;(2010, 2020]&lt;/th&gt;
      &lt;th&gt;(1980, 1990]&lt;/th&gt;
      &lt;th&gt;(1960, 1970]&lt;/th&gt;
      &lt;th&gt;(1970, 1980]&lt;/th&gt;
      &lt;th&gt;(1990, 2000]&lt;/th&gt;
      &lt;th&gt;(0, 1960]&lt;/th&gt;
      &lt;th&gt;(2000, 2010]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;m1.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 641138 entries, 0 to 641137
Data columns (total 16 columns):
engagement             641138 non-null int64
Secondary              641138 non-null int64
Bachelor&amp;#39;s             641138 non-null int64
Master&amp;#39;s               641138 non-null int64
Doctorate              641138 non-null int64
Less than Secondary    641138 non-null int64
m                      641138 non-null int64
f                      641138 non-null int64
o                      641138 non-null int64
(2010, 2020]           641138 non-null int64
(1980, 1990]           641138 non-null int64
(1960, 1970]           641138 non-null int64
(1970, 1980]           641138 non-null int64
(1990, 2000]           641138 non-null int64
(0, 1960]              641138 non-null int64
(2000, 2010]           641138 non-null int64
dtypes: int64(16)
memory usage: 83.2 MB



y = m1.engagement.values
x = m1.drop(&amp;#39;engagement&amp;#39;,axis=1).values
print y.shape,x.shape

(641138,) (641138, 15)



from sklearn.metrics import confusion_matrix

a_train, a_test, b_train, b_test = train_test_split(x,y,test_size=0.50)

mlog = LogisticRegressionCV(cv=20)
mlog.fit(a_train,b_train)

probs = mlog.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&amp;#39;seagreen&amp;#39;,alpha=0.8,lw=3)
plt.xlabel(&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;)
plt.ylabel(&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;)
plt.title(&amp;quot;MOOC&amp;quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &amp;quot;Recall: &amp;quot;, precision_score(mlog.predict(a_test),b_test)
print &amp;quot;Precision: &amp;quot;, recall_score(mlog.predict(a_test),b_test)
print &amp;quot;Accuracy: &amp;quot;, accuracy_score(mlog.predict(a_test),b_test)
confusion_matrix(mlog.predict(a_test),b_test)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_60_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Recall&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;Precision&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.937143017572&lt;/span&gt;





&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;300419&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;20150&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt;
       &lt;span class="o"&gt;[&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;      &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This model, unsurpisingly, predicts no engagement because engagement is so rare.  This is a know problem with logistic regresion.   To avoid this we can drop duplicates and refit the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;m2 = m1.drop_duplicates()
y = m2.engagement.values
x = m2.drop(&amp;#39;engagement&amp;#39;,axis=1).values
print y.shape,x.shape

(163,) (163, 15)



a_train, a_test, b_train, b_test = train_test_split(x,y,test_size=0.50)

mlog = LogisticRegressionCV(cv=20)
mlog.fit(a_train,b_train)

probs = mlog.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&amp;#39;seagreen&amp;#39;,alpha=0.8,lw=3)
plt.xlabel(&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;)
plt.ylabel(&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;)
plt.title(&amp;quot;MOOC&amp;quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &amp;quot;Recall: &amp;quot;, precision_score(mlog.predict(a_test),b_test)
print &amp;quot;Precision: &amp;quot;, recall_score(mlog.predict(a_test),b_test)
print &amp;quot;Accuracy: &amp;quot;, accuracy_score(mlog.predict(a_test),b_test)
confusion_matrix(mlog.predict(a_test),b_test)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_63_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Recall&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.121951219512&lt;/span&gt;
&lt;span class="n"&gt;Precision&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.277777777778&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.40243902439&lt;/span&gt;





&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt;
       &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]])&lt;/span&gt;




&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mlog&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;predict_proba&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;engagement&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mlog&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;engagement&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tpr1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;engagement&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;seagreen&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;False Positive Rate (1 - Specificity)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;ylabel&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;True Positive Rate (Sensitivity, Recall)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;title&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MOOC&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;xlim&lt;/span&gt;&lt;span class="o"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;ylim&lt;/span&gt;&lt;span class="o"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;legend&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;show&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Recall: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;engagement&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Precision: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;engagement&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Accuracy: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;engagement&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;engagement&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;values&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_64_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Recall&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.136417673866&lt;/span&gt;
&lt;span class="n"&gt;Precision&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.0649505324104&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.821936930895&lt;/span&gt;





&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;521467&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;34868&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt;
       &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;79295&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;5508&lt;/span&gt;&lt;span class="o"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I do not think the model is any better.   Ad at this point I think based on this data we are not able to predict engagement by self-reported features&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="Lasso Regresion"></category><category term="Ridge Regression"></category><category term="Regularization"></category></entry><entry><title>Galvanize - Week 03 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-03/" rel="alternate"></link><updated>2015-06-17T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-17:galvanize/galvanize-data-science-03-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 3 - Day 3&lt;/h2&gt;
&lt;p&gt;Today we had a miniquiz on cleaning data.  Identify missing values, incorrect format, and outliers.   It was a straight forward assignment.   I was struck with there is no clear end to data cleaning without a purpose.   It would be nice ot set a stoping criteria or variable specification so there are clear evaluation criteria.  Obviously with raw data the data has to be explored, and these criteria does not exist.   &lt;/p&gt;
&lt;h2&gt;Morning: One-fold Cross Validation&lt;/h2&gt;
&lt;p&gt;This morning we started using cross validation on the boston data set from sklearn.  The goal is not to make the best model, but just work through th eprocess of using cross validation. &lt;/p&gt;
&lt;p&gt;Descriptions for each column in &lt;code&gt;features&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Attribute Information (in order):
    - CRIM     per capita crime rate by town
    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
    - INDUS    proportion of non-retail business acres per town
    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    - NOX      nitric oxides concentration (parts per 10 million)
    - RM       average number of rooms per dwelling
    - AGE      proportion of owner-occupied units built prior to 1940
    - DIS      weighted distances to five Boston employment centres
    - RAD      index of accessibility to radial highways
    - TAX      full-value property-tax rate per $10,000
    - PTRATIO  pupil-teacher ratio by town
    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    - LSTAT    % lower status of the population
    - MEDV     Median value of owner-occupied homes in $1000's&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.tools.plotting&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;scatter_matrix&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;
&lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="c"&gt;# housing price&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are using the train, test split to make a training set of 70% of the data and a test set of 30% of the data.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;train_feature, test_feature, train_target, test_target = train_test_split(features, target, test_size=0.3)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to use sklearn's LinearRegresion on this data set.  There are likely issues with multicolinearity, but we will skip that for now.   We will train the data on the training set, and test it on the testing set.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;linear = LinearRegression()
linear.fit(train_feature, train_target)
# You can call predict to get the predicted values for training and test
train_predicted = linear.predict(train_feature)
test_predicted = linear.predict(test_feature)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I wrote a mean square function to compare to sklearn's function.   I expect the test set to have larger value then the training set because the model was not trained on the test set.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def MSE(x1,x2):
    return np.sum(np.power(x2-x1,2))/len(x1)
print &amp;quot;Train MSE: &amp;quot;, MSE(train_predicted,train_target),mean_squared_error(train_predicted,train_target)
print &amp;quot;Test MSE: &amp;quot;, MSE(test_predicted,test_target), mean_squared_error(test_predicted,test_target)


Train MSE:  21.1395296142 21.1395296142
Test MSE:  24.6555052851 24.6555052851
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;K-fold Cross Validation&lt;/h2&gt;
&lt;p&gt;In K-fold cross validation the data is split into &lt;strong&gt;k&lt;/strong&gt; groups. One group
out of the k groups will be the test set, the rest (&lt;strong&gt;k-1&lt;/strong&gt;) groups will
be the training set. In the next iteration, another group will be the test set,
and the rest will be the training set. The process repeats for k iterations (k-fold).
In each fold, a metric for accuracy (MSE in this case) will be calculated and
an overall average of that metric will be calculated over k-folds.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To do this we need to manage randomly sampling &lt;strong&gt;k&lt;/strong&gt; folds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Properly combining those &lt;strong&gt;k&lt;/strong&gt; folds into a test and training set on
   your &lt;strong&gt;on the training dataset&lt;/strong&gt;. Outside of the k-fold, there should be
   another set which will be referred to as the &lt;strong&gt;hold-out set&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train your model on your constructed training set and evaluate on the given test set&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat steps &lt;strong&gt;2&lt;/strong&gt; and &lt;strong&gt;3&lt;/strong&gt; &lt;em&gt;k&lt;/em&gt; times.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Average your results of your error metric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compare the MSE for your test set in Part 1. and your K-fold cross validated error in &lt;code&gt;4.&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;indexes = np.arange(len(train_feature))
indexes2 = indexes.copy()
linear = LinearRegression()
count = len(indexes)
mse = []
k = 10.
for i in range(int(k)):
    choices = np.random.choice(indexes2,int(count/k),replace=False)
    indexes2 = np.setdiff1d(indexes2,choices)
    train_index = np.setdiff1d(indexes,choices)
    linear.fit(train_feature[train_index], train_target[train_index])
    test_predicted = linear.predict(train_feature[choices])
    mse.append(mean_squared_error(train_target[choices],test_predicted))&lt;/p&gt;
&lt;p&gt;print "Avg MSE: ",sum(mse)/k&lt;/p&gt;
&lt;p&gt;Avg MSE:  23.9048376671&lt;/p&gt;
&lt;p&gt;def scorer(model,X,y):
    return mean_squared_error(y,model.predict(X))&lt;/p&gt;
&lt;p&gt;cross_val_score(LinearRegression(),train_feature,train_target,scoring=scorer,cv=10).mean()&lt;/p&gt;
&lt;p&gt;24.72849552718905&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I made my own k-fold validation and compared it to the sklearn method.  There is an obviously efficiency in code.  I did my own wrapper for the scoring because the string method sometimes returned negative values.   &lt;/p&gt;
&lt;p&gt;We can look at the comparison between traiing and cross validaton as we increase the sample size.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sample_sizes = np.arange(10,350,10)
train_mse = []
cross_mse = []
for size in sample_sizes:
    linear = LinearRegression()

    linear.fit(train_feature[:size],train_target[:size])
    train_predicted = linear.predict(train_feature[:size])
    train_mse.append(mean_squared_error(train_predicted[:size],train_target[:size]))
    cross_mse.append(cross_val_score(linear,train_feature[:size],train_target[:size],scoring=scorer,cv=5).mean())

print len(sample_sizes),len(train_mse)
plt.figure(figsize=(10,8))
plt.plot(sample_sizes,train_mse,color=&amp;#39;seagreen&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;Train Set MSE&amp;#39;)
plt.plot(sample_sizes,cross_mse,color=&amp;#39;steelblue&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;CV Set MSE&amp;#39;)
plt.legend()
plt.xlabel(&amp;quot;Sample Size&amp;quot;)
plt.ylabel(&amp;quot;MSE&amp;quot;)
plt.ylim([0,100])
plt.show()

34 34
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_13_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case we see that the two values start to converge, but the CV is higher.  This make sense because it is hold out values to test on.   It should be a better approximation to applying it to a test set. &lt;/p&gt;
&lt;p&gt;We can look at the difference between the test and training sets as we increase the training size.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sample_sizes = np.arange(10,350,10)
test_mse = []
train_mse = []
for size in sample_sizes:
    linear = LinearRegression()
    linear.fit(train_feature[:size],train_target[:size])
    train_predicted = linear.predict(train_feature)
    test_predicted = linear.predict(test_feature)
    train_mse.append(mean_squared_error(train_predicted,train_target))
    test_mse.append(mean_squared_error(test_predicted,test_target))

plt.figure(figsize=(10,8))
plt.plot(sample_sizes,train_mse,color=&amp;#39;seagreen&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;Train Set MSE&amp;#39;)
plt.plot(sample_sizes,test_mse,color=&amp;#39;steelblue&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;Test Set MSE&amp;#39;)
plt.legend()
plt.xlabel(&amp;quot;Sample Size&amp;quot;)
plt.ylabel(&amp;quot;MSE&amp;quot;)
plt.ylim([0,100])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_15_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;test_mse = []
train_mse = []
num_feat = range(1,len(train_feature[0,:]))
for i in num_feat:
    linear = LinearRegression()
    linear.fit(train_feature[:,:i],train_target)
    train_predicted = linear.predict(train_feature[:,:i])
    test_predicted = linear.predict(test_feature[:,:i])
    train_mse.append(mean_squared_error(train_predicted,train_target))
    test_mse.append(mean_squared_error(test_predicted,test_target))

plt.figure(figsize=(10,8))
plt.plot(num_feat,train_mse,color=&amp;#39;seagreen&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;Train Set MSE&amp;#39;)
plt.plot(num_feat,test_mse,color=&amp;#39;steelblue&amp;#39;,lw=2,alpha=0.8,label=&amp;#39;Test Set MSE&amp;#39;)
plt.legend()
plt.xlabel(&amp;quot;Number Features&amp;quot;)
plt.ylabel(&amp;quot;MSE&amp;quot;)
plt.ylim([0,100])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_16_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;As we increase features and data, our results on the training set go down and the test set values, in this case, look to converge.   &lt;/p&gt;
&lt;h2&gt;Stepwise Regression&lt;/h2&gt;
&lt;p&gt;While stepwise regression has its many &lt;a href="http://andrewgelman.com/2014/06/02/hate-stepwise-regression/"&gt;critics&lt;/a&gt;, it is a useful exercise to introduce the concept of feature selection in the context of linear regression. This extra credit exercise has two components of different difficulties. First, use the &lt;code&gt;scikit-learn&lt;/code&gt; reverse feature elimation (a greedy feature elimination algorithm) to implement something similar to sequential backward selection. The second, more difficult part is implementing sequential forward selection.&lt;/p&gt;
&lt;p&gt;We generate a random dataset that has 100 features, but only 5 of them influence the response variable.  We can use sklearn's RFE to fit and attempt to rank the features.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_friedman1&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_friedman1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RFE&lt;/span&gt;
&lt;span class="n"&gt;linear&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;rfe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RFE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rfe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rfe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ranking_&lt;/span&gt;




&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;71&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;47&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;94&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;39&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;58&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;98&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;78&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;54&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;76&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;86&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;43&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;29&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;66&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;68&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;92&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;89&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;82&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;83&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;96&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;77&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;62&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;67&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;73&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;85&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;91&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;53&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;34&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;93&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;27&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;63&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;70&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;97&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;52&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;57&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;81&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;74&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;69&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to loop through these variables and use them as a fit.   We want to see if a there is a point where adding more features will not improve the model.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;linear = LinearRegression()
rsq = []
for i in range(2,50):
    linear.fit(X[:,rfe.ranking_ &amp;lt; i],y)
    rsq.append(linear.score(X[:,rfe.ranking_ &amp;lt; i],y))
plt.plot(range(2,50),rsq,lw=3,color=&amp;#39;seagreen&amp;#39;,alpha=0.5,label=&amp;#39;R-Square&amp;#39;)
plt.xlabel(&amp;quot;Number of Variables&amp;quot;)
plt.ylabel(&amp;#39;R-Squared&amp;#39;)
plt.ylim([0,1])
plt.axvline(x=5,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;gray&amp;#39;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_21_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Instead of using RFE to do backward selection, we can create a class that implements sequential forward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model comparison criterion, adding the variable (if any) that improves the model the most, and repeating this process until none improves the model.&lt;/p&gt;
&lt;h4&gt;Reference&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://onlinecourses.science.psu.edu/stat501/node/88"&gt;Stepwise Regression Procedure&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;class ForwardRegression:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def __init__(self,X,y):
    self.nFeatures = X.shape[1]
    self.nRow = X.shape[0]
    self.X = X
    self.y = y
    self.linear = LinearRegression()
    self.scores = []
    self.best_features = []
    self.best_model = False

def next_best(self):
    if not self.best_model:
        max_rsq = 0
        best_index = -1
        for i in range(self.nFeatures):
            if i not in self.best_features:
                self.linear.fit(self.X[:,(self.best_features+[i])],self.y)
                score = self.linear.score(self.X[:,(self.best_features+[i])],self.y)
                if score &amp;gt; max_rsq:
                    max_rsq = score
                    best_index = i
        if best_index != -1:
            if len(self.scores) &amp;gt; 1:
                if (max_rsq-self.scores[-1])/self.scores[-1] &amp;gt; 0.01:
                    self.best_features.append(best_index)
                    self.scores.append(max_rsq)
                else:
                    self.best_model = True

            else:
                self.best_features.append(best_index)
                self.scores.append(max_rsq)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;fr = ForwardRegression(X,y)
for i in range(5):
    fr.next_best()
print fr.scores
print fr.best_features
print fr.best_model&lt;/p&gt;
&lt;p&gt;[0.32517954427506357, 0.49706338477081047, 0.64962046580083821, 0.74271747755945094]
[3, 1, 0, 4]
True&lt;/p&gt;
&lt;p&gt;rfe.ranking_&lt;/p&gt;
&lt;p&gt;array([  3,   2,  71,   1,   4,  47,  22,  36,  94,  61,  14,  39,  24,
        58,  20,  75,  98,  79,  41,  32,  78,  54,  76,  86,  55,  43,
        29,  66,  35,  68,  37,   8,  92,  51,  13,  89,  46,  82,  83,
        30,  96,  77,  31,  72,  90,  62,  67,  59,  73,  15,  11,  85,
        91,  17,  88,  26,  56,  28,  33,  53,  18,  10,   7,   6,  38,
        48,  50,  19,  84,  34,  93,  23,  27,  63,  70,  16,  12, 100,
        44,  80,  45,  97,  87,   9,  21,   5,  52,  60,  57,  49,  25,
        40,  95,  81,  42,  99,  74,  65,  69,  64])&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This matches with the previous result for this example.&lt;/p&gt;
&lt;h2&gt;Afternoon - Lasso and Ridge Regresson&lt;/h2&gt;
&lt;p&gt;The goal fo this exercise is to get a feel for the shrinkage that these models do for coefficients.   We will start with the Ridge Regression in sklearn on the diabetes dataset.  We did a basic fit to start. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_diabetes&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Lasso&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ridge&lt;/span&gt;

&lt;span class="n"&gt;diabetes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_diabetes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;diabetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;diabetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;XT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;diabetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;yT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;diabetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Ridge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;MSE: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;MSE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;4251.12234379&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we will look at how the parameters change as we increasee alpha.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;

&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;alphas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;X_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Ridge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_28_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Given a large enough alpha/lambda, the coefficients go to zero   This is a property of the model, and we want to try to use this paramter to find the best model.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;k = X.shape[1]
alphas = np.logspace(-3,0,1000)
mse_train = []
mse_test = []
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Ridge(alpha=a, normalize=True).fit(X_data, y)
    mse_train.append(mean_squared_error(y,fit.predict(X_data)))
    mse_test.append(mean_squared_error(yT,fit.predict(scaler.transform(XT))))
plt.figure(figsize=(14,6))
plt.plot(alphas, mse_train)
plt.plot(alphas, mse_test,color=&amp;#39;green&amp;#39;)
plt.show()


print &amp;quot;Best Alpha: &amp;quot;, alphas[mse_test.index(min(mse_test))]
alpha_ridge = alphas[mse_test.index(min(mse_test))]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_30_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Best Alpha:  0.252582002696
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we are going to do the same for Lasso Regresion&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;k = X.shape[1]
alphas = np.logspace(-2, 2)
params = np.zeros((len(alphas), k))
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Lasso(alpha=a, normalize=True).fit(X_data, y)
    params[i] = fit.coef_

plt.figure(figsize=(14,6))
for param in params.T:
    plt.plot(alphas, param)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_32_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Lasso drops the paramters to zero much more quickly than the Ridge Method.  This is because of the absolut value in the prior.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;k = X.shape[1]
alphas = np.logspace(-3,0,1000)
mse_train = []
mse_test = []
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Lasso(alpha=a, normalize=True).fit(X_data, y)
    mse_train.append(mean_squared_error(y,fit.predict(X_data)))
    mse_test.append(mean_squared_error(yT,fit.predict(scaler.transform(XT))))
plt.figure(figsize=(14,6))
plt.plot(alphas, mse_train)
plt.plot(alphas, mse_test,color=&amp;#39;green&amp;#39;)
plt.show()

print &amp;quot;Best Alpha: &amp;quot;, alphas[mse_test.index(min(mse_test))]
alpha_lasso = alphas[mse_test.index(min(mse_test))]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D3/output_34_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Best Alpha:  0.286059553518
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to compare the 3 models.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ridge = Ridge(alpha=alpha_ridge)
ridge.fit(X_data,y)

lasso = Lasso(alpha=alpha_lasso)
lasso.fit(X_data,y)

ols = LinearRegression()
ols.fit(X_data,y)

print &amp;quot;Ridge MSE:&amp;quot;,mean_squared_error(yT,ridge.predict(scaler.transform(XT)))
print &amp;quot;Lasso MSE:&amp;quot;,mean_squared_error(yT,lasso.predict(scaler.transform(XT)))
print &amp;quot;OLS MSE:&amp;quot;,mean_squared_error(yT,ols.predict(scaler.transform(XT)))
print &amp;quot; &amp;quot;
print &amp;quot;Ridge,Lasso,OLS&amp;quot; 
for i in range(len(ridge.coef_)):
    print round(ridge.coef_[i],2),round(lasso.coef_[i],2),round(ols.coef_[i],2)

Ridge MSE: 3184.60043415
Lasso MSE: 3160.04592755
OLS MSE: 3190.98282981

Ridge,Lasso,OLS
-3.28 -2.95 -3.29
-17.46 -17.04 -17.51
20.52 20.39 20.55
14.68 14.28 14.7
2.45 -0.0 3.85
-15.54 -12.27 -16.78
-12.38 -11.89 -13.07
5.94 4.43 5.84
26.36 27.43 25.93
4.47 4.34 4.44
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Lasso model on the lower MSE on the test set, and also dropped one of the variables in the model.   Over all its coefficients are lower, and this is consistent with what we saw in the previous plots.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;r2_score&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ridge MSE:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r2_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ridge&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XT&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Lasso MSE:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r2_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lasso&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XT&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;OLS MSE:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r2_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ols&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XT&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="n"&gt;Ridge&lt;/span&gt; &lt;span class="n"&gt;MSE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.473709418009&lt;/span&gt;
&lt;span class="n"&gt;Lasso&lt;/span&gt; &lt;span class="n"&gt;MSE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.477767322867&lt;/span&gt;
&lt;span class="n"&gt;OLS&lt;/span&gt; &lt;span class="n"&gt;MSE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.472654656259&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is clear that this is far from the final story...&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="Lasso Regresion"></category><category term="Ridge Regression"></category><category term="Regularization"></category></entry><entry><title>Galvanize - Week 03 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-02/" rel="alternate"></link><updated>2015-06-16T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-16:galvanize/galvanize-data-science-03-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 3 - Day 2&lt;/h2&gt;
&lt;p&gt;Today we did more linear regression, and started evaluating applying linear models to regression problems.   It was a combination of informative and frustration because I learned a lot, but did not develope an intuition for the process.   I suppsoe that will come with time.  &lt;/p&gt;
&lt;h2&gt;Mini-Quiz&lt;/h2&gt;
&lt;p&gt;Today's miniquiz took all of 5 minutes.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Write a python function to find the value in a list that's closest to a given value.&lt;/p&gt;
&lt;p&gt;e.g. &lt;code&gt;closest([10, 17, 2, 29, 16], 14&lt;/code&gt; should return 16.&lt;/p&gt;
&lt;p&gt;a = [10,17,2,29,16]
import math
def closest(array,val):
    diff = [math.fabs(x-val) for x in array]
    index = diff.index(min(diff))
    return array[index]&lt;/p&gt;
&lt;p&gt;closest(a,14)&lt;/p&gt;
&lt;p&gt;16&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead let's start with a numpy array. How can we do the same thing in one line using numpy magic.&lt;/p&gt;
&lt;p&gt;import numpy as np&lt;/p&gt;
&lt;p&gt;a = np.array(a)
a[np.abs(a-14).argmin()]&lt;/p&gt;
&lt;p&gt;16&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;My favorite numpy trick is &lt;a href="http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays"&gt;masking&lt;/a&gt;. Say you have a feature matrix &lt;code&gt;X&lt;/code&gt; (2d numpy array) and with labels &lt;code&gt;y&lt;/code&gt; (1d numpy array). I would like to get a feature matrix of only the positive cases, i.e. get the rows from &lt;code&gt;X&lt;/code&gt; where &lt;code&gt;y&lt;/code&gt; is positive.&lt;/p&gt;
&lt;p&gt;How can you do this in one line?&lt;/p&gt;
&lt;p&gt;Create example &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to verify your code.&lt;/p&gt;
&lt;p&gt;x = np.random.rand(100,10)
y = np.random.randint(2,size=(100,))&lt;/p&gt;
&lt;p&gt;print x.shape, x[y&amp;gt;0,:].shape&lt;/p&gt;
&lt;p&gt;(100, 10) (47, 10)&lt;/p&gt;
&lt;p&gt;print set(y[y&amp;gt;0].tolist())&lt;/p&gt;
&lt;p&gt;set([1])&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Morning - Regression&lt;/h2&gt;
&lt;p&gt;The linear regression model makes a number of assumptions about the data, including &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Homoscedasticity of residuals&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normal distribution of residuals&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of multicollinearity among features&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Independence of the observations (For example, independence assumption violated if data is a time series)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the results of the regression model depend on these statistical assumptions, the 
results of the regression model are only correct if our assumptions hold (at least approximately).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This morning we will be exploring two datasets: &lt;code&gt;prestige&lt;/code&gt; and &lt;code&gt;ccard&lt;/code&gt;. Below is a description of the 2 datasets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;prestige&lt;/code&gt; &lt;em&gt;(From yesterday afternoon)&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Prediction of the prestige of a job&lt;/li&gt;
&lt;li&gt;Dependent variable: &lt;code&gt;prestige&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Independent variables: &lt;code&gt;income&lt;/code&gt;, &lt;code&gt;education&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Code to load data set into dataframe:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;python
  import statsmodels.api as sm
  prestige = sm.datasets.get_rdataset("Duncan", "car", cache=True).data&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ccard&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;Prediction of the average credit card expenditure&lt;/li&gt;
&lt;li&gt;Dependent variable: &lt;code&gt;AVGEXP&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Independent variables: &lt;code&gt;AGE&lt;/code&gt;, &lt;code&gt;INCOME&lt;/code&gt;, &lt;code&gt;INCOMESQ&lt;/code&gt; (&lt;code&gt;INCOME^2&lt;/code&gt;), &lt;code&gt;OWNRENT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Code to load data set into dataframe:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;python
  credit_card = sm.datasets.ccard.load_pandas().data&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Prestige Regression&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.tools.plotting&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;scatter_matrix&lt;/span&gt;
&lt;span class="n"&gt;prestige&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_rdataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Duncan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cache&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;credit_card&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ccard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_pandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;scatter_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prestige&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kp"&gt;diagonal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;kde&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Figure&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="mh"&gt;0x106186450&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_9_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
prestige.boxplot()
plt.show()

/Library/Python/2.7/site-packages/pandas/tools/plotting.py:2633: FutureWarning: 
The default value for &amp;#39;return_type&amp;#39; will change to &amp;#39;axes&amp;#39; in a future release.
 To use the future behavior now, set return_type=&amp;#39;axes&amp;#39;.
 To keep the previous behavior and silence this warning, set return_type=&amp;#39;dict&amp;#39;.
  warnings.warn(msg, FutureWarning)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_10_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;y = prestige[[&amp;#39;prestige&amp;#39;]]
x = sm.add_constant(prestige[[&amp;#39;income&amp;#39;,&amp;#39;education&amp;#39;]])
model1 = sm.OLS(y,x).fit()
model1.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;        &lt;td&gt;prestige&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   101.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;8.65e-17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;07:50:23&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -178.98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    45&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   364.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    42&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   369.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt;   -6.0647&lt;/td&gt; &lt;td&gt;    4.272&lt;/td&gt; &lt;td&gt;   -1.420&lt;/td&gt; &lt;td&gt; 0.163&lt;/td&gt; &lt;td&gt;  -14.686     2.556&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    0.5987&lt;/td&gt; &lt;td&gt;    0.120&lt;/td&gt; &lt;td&gt;    5.003&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.357     0.840&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;    0.5458&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    5.555&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.348     0.744&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 1.279&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.528&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   0.520&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.155&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.426&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    163.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;So far we have looked at the data distributions, and the scatter matrix shows relationships between prestige and both income and education.  We have some covariance between income and educations, that could affect the assumption of linear regression that the variables in the model are independant.   We can look at a studentized resid plot and a QQ plot to examine the model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def stud_plot(model):
    stud = model.resid/model.resid.std()
    fitv = model.fittedvalues
    plt.figure(figsize=(10,4))
    plt.plot(fitv,stud,marker=&amp;#39;o&amp;#39;,lw=0,color=&amp;#39;steelblue&amp;#39;,alpha=0.8)
    plt.axhline(2,color=&amp;#39;gray&amp;#39;,linestyle=&amp;#39;--&amp;#39;,)
    plt.axhline(-2,color=&amp;#39;gray&amp;#39;,linestyle=&amp;#39;--&amp;#39;)
    plt.xlabel(&amp;quot;Fitted Values&amp;quot;)
    plt.ylabel(&amp;quot;Studentized Residual&amp;quot;)
    plt.ylim([-5,5])
    plt.show()

def qq_plot(model):
    plt.figure(figsize=(10,4))
    stud = model.resid/model.resid.std()
    sc.probplot(model.resid,plot=plt)
    plt.title(&amp;quot;QQ Plot&amp;quot;)
    plt.show()

stud_plot(model1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_13_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;qq_plot(model1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_14_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see se have two outliers from the studentized plot, those that are more than 2 standard deviation from the fit.   The QQ-plot shows a fairly normal distributions of the residuals, suggesting the fit is okay.  We can also see if we have any points with high leverage.&lt;/p&gt;
&lt;p&gt;$$\hat{y} = X \dot \beta = X (X^T \ X)^{-1} \ X^T \ y = H \ y$$&lt;/p&gt;
&lt;p&gt;$$ H = X (X^T \ X)^{-1} \ X^T $$  &lt;/p&gt;
&lt;p&gt;The diagonals of the hat matrix (H) are considered to be estimates of leverage of each point.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def get_H(X):
    H = X.dot( np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()) )
    return H.diagonal()

def lev_plot(model,X):
    h = get_H(X)
    stud = model.resid/model.resid.std()
    plt.figure(figsize=(10,5))
    plt.plot(h,stud,marker=&amp;#39;o&amp;#39;,lw=0,color=&amp;#39;steelblue&amp;#39;,alpha=0.8)
    plt.xlabel(&amp;quot;H Leverage&amp;quot;)
    plt.ylabel(&amp;quot;Studentized Residual&amp;quot;)
    plt.ylim([-5,5])
    plt.show()

lev_plot(model1,x.values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_17_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that we have 3 points that are leveraging the results, and one of them is an outlier.&lt;/p&gt;
&lt;p&gt;Now we have the question - what do we do with the outliers and the high leverage points?  I think we will stick wih removing all the high leverage points.  We before we do lets look that these three points.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x[get_H(x.values)&amp;gt;0.15]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;minister&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;conductor&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;RR.engineer&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;y[get_H(x.values)&amp;gt;=0.15]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;prestige&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;minister&lt;/th&gt;
      &lt;td&gt;87&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;conductor&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;RR.engineer&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We see the engineer and conductor have higher income, but lower eduation, while the minister has lower education and higher income.   To decide what we do with these values we really need to know the purpose of the model.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;hp = get_H(x.values)
model2 = sm.OLS(y[hp&amp;lt;0.15],x[hp&amp;lt;0.15]).fit()
model2.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;        &lt;td&gt;prestige&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.870&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   138.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;2.02e-18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;07:50:23&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -160.59&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    42&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   327.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    39&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   332.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt;   -6.3174&lt;/td&gt; &lt;td&gt;    3.680&lt;/td&gt; &lt;td&gt;   -1.717&lt;/td&gt; &lt;td&gt; 0.094&lt;/td&gt; &lt;td&gt;  -13.760     1.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    0.9307&lt;/td&gt; &lt;td&gt;    0.154&lt;/td&gt; &lt;td&gt;    6.053&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.620     1.242&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;    0.2846&lt;/td&gt; &lt;td&gt;    0.121&lt;/td&gt; &lt;td&gt;    2.345&lt;/td&gt; &lt;td&gt; 0.024&lt;/td&gt; &lt;td&gt;    0.039     0.530&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.811&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.468&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.149&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   2.802&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.614&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.303&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    158.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Removing 3 of the 45 data points slightly improved the R-squared value, but dramatically changed the model.   The coefficients were initialy (0.5987,0.5458).  The new model coefficient is (0.9407,2846).  The minister must have been pulling the fit down because they have high prestigue but low income.  Removing them changed the model.   Few people become ministers, so it could be a better model without including them. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(model2)
qq_plot(model2)
lev_plot(model2,x[get_H(x.values)&amp;lt;0.15].values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_24_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_24_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The leverage plot does not show a few points dominating the fit, buter there are a few outliers in the fit.   The QQ plot shows some signs of the erros not being normal.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Model 1 RSE: &amp;quot;, np.sqrt(np.sum(np.power(y.values.reshape(1,45)-model1.predict(x),2))/(45-2-1))
print &amp;quot;Model 2 RSE: &amp;quot;, np.sqrt(np.sum(np.power(y.values.reshape(1,45)-model2.predict(x),2))/(45-2-1))

Model 1 RSE:  13.3690283982
Model 2 RSE:  14.6713753944
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On the entire dataset, the first model does a better job fitting the entire dataset then the second model.  The second model fits the subsetted data better.   On these vlaues we have&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Model 1 RSE: &amp;quot;, np.sqrt(np.sum(np.power(y[hp&amp;lt;0.15].values.reshape(1,42)-model1.predict(x[hp&amp;lt;0.15]),2))/(42-2-1))
print &amp;quot;Model 2 RSE: &amp;quot;, np.sqrt(np.sum(np.power(y[hp&amp;lt;0.15].values.reshape(1,42)-model2.predict(x[hp&amp;lt;0.15]),2))/(42-2-1))

Model 1 RSE:  12.2166311202
Model 2 RSE:  11.492268427
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On the subsetted data, the second model does do better.   If this model was unleashed into the wild we would need to identify outliers and attempt to decide what to do with them&lt;/p&gt;
&lt;h2&gt;Credit Analysis&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(10,10))
scatter_matrix(credit_card,diagonal=&amp;#39;kde&amp;#39;,alpha=0.5,figsize=(14,14))
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10a815250&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_30_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;credit_card.corr()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;AVGEXP&lt;/th&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;th&gt;INCOME&lt;/th&gt;
      &lt;th&gt;INCOMESQ&lt;/th&gt;
      &lt;th&gt;OWNRENT&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;AVGEXP&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.168367&lt;/td&gt;
      &lt;td&gt;0.443135&lt;/td&gt;
      &lt;td&gt;0.372679&lt;/td&gt;
      &lt;td&gt;0.243342&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;td&gt;0.168367&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.385108&lt;/td&gt;
      &lt;td&gt;0.316498&lt;/td&gt;
      &lt;td&gt;0.438236&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;INCOME&lt;/th&gt;
      &lt;td&gt;0.443135&lt;/td&gt;
      &lt;td&gt;0.385108&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.964707&lt;/td&gt;
      &lt;td&gt;0.473079&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;INCOMESQ&lt;/th&gt;
      &lt;td&gt;0.372679&lt;/td&gt;
      &lt;td&gt;0.316498&lt;/td&gt;
      &lt;td&gt;0.964707&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.434500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;OWNRENT&lt;/th&gt;
      &lt;td&gt;0.243342&lt;/td&gt;
      &lt;td&gt;0.438236&lt;/td&gt;
      &lt;td&gt;0.473079&lt;/td&gt;
      &lt;td&gt;0.434500&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;credit_card.corr(method=&amp;#39;spearman&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;AVGEXP&lt;/th&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;th&gt;INCOME&lt;/th&gt;
      &lt;th&gt;INCOMESQ&lt;/th&gt;
      &lt;th&gt;OWNRENT&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;AVGEXP&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.182947&lt;/td&gt;
      &lt;td&gt;0.533556&lt;/td&gt;
      &lt;td&gt;0.533556&lt;/td&gt;
      &lt;td&gt;0.340274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;td&gt;0.182947&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.444940&lt;/td&gt;
      &lt;td&gt;0.444940&lt;/td&gt;
      &lt;td&gt;0.434660&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;INCOME&lt;/th&gt;
      &lt;td&gt;0.533556&lt;/td&gt;
      &lt;td&gt;0.444940&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.410097&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;INCOMESQ&lt;/th&gt;
      &lt;td&gt;0.533556&lt;/td&gt;
      &lt;td&gt;0.444940&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.410097&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;OWNRENT&lt;/th&gt;
      &lt;td&gt;0.340274&lt;/td&gt;
      &lt;td&gt;0.434660&lt;/td&gt;
      &lt;td&gt;0.410097&lt;/td&gt;
      &lt;td&gt;0.410097&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The scatter matrix shows that therre are some variations in between the average expense and age, income, and income squaree.  Obviously income and income squared are related.   The spearmann correlations show that age and income are ordered together, but that ordering does not seem to be a linear relationship because of the significant increason from pearson to spearman.  This could affect our modeling.   &lt;/p&gt;
&lt;p&gt;We'll do a basic fit and see what we are starting with.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;yc = credit_card[[&amp;#39;AVGEXP&amp;#39;]]
xc = sm.add_constant(credit_card[[&amp;#39;INCOME&amp;#39;,&amp;#39;INCOMESQ&amp;#39;,&amp;#39;AGE&amp;#39;,&amp;quot;OWNRENT&amp;quot;]])
model3 = sm.OLS(yc,xc).fit()
model3.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;AVGEXP&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.244&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.198&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   5.394&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;0.000795&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;08:06:50&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -506.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    72&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   1023.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    67&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   1034.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     4&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;        &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;    &lt;td&gt; -237.1465&lt;/td&gt; &lt;td&gt;  199.352&lt;/td&gt; &lt;td&gt;   -1.190&lt;/td&gt; &lt;td&gt; 0.238&lt;/td&gt; &lt;td&gt; -635.054   160.761&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;INCOME&lt;/th&gt;   &lt;td&gt;  234.3470&lt;/td&gt; &lt;td&gt;   80.366&lt;/td&gt; &lt;td&gt;    2.916&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;   73.936   394.758&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;INCOMESQ&lt;/th&gt; &lt;td&gt;  -14.9968&lt;/td&gt; &lt;td&gt;    7.469&lt;/td&gt; &lt;td&gt;   -2.008&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; &lt;td&gt;  -29.906    -0.088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;AGE&lt;/th&gt;      &lt;td&gt;   -3.0818&lt;/td&gt; &lt;td&gt;    5.515&lt;/td&gt; &lt;td&gt;   -0.559&lt;/td&gt; &lt;td&gt; 0.578&lt;/td&gt; &lt;td&gt;  -14.089     7.926&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;OWNRENT&lt;/th&gt;  &lt;td&gt;   27.9409&lt;/td&gt; &lt;td&gt;   82.922&lt;/td&gt; &lt;td&gt;    0.337&lt;/td&gt; &lt;td&gt; 0.737&lt;/td&gt; &lt;td&gt; -137.573   193.455&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt;69.024&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.640&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt; 497.349&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 2.844&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;1.00e-108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt;14.551&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    227.&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(model3)
qq_plot(model3)
lev_plot(model3,xc.values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_35_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_35_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_35_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our inital linear model is not a strong fit, age and own/rent are not significantly involved in the model, the residuals are not normally distributed, and there are outliers and high leverage points.   We clearly need to do something with this model.  The variances does not look constant.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.stats.diagnostic&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;HetGoldfeldQuandt&lt;/span&gt;
&lt;span class="n"&gt;HGQ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;HetGoldfeldQuandt&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;HGQ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;xc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.3799238831599079&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.18743313399026834&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;increasing&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;equal_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fittedvalues&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fittedvalues&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
    &lt;span class="n"&gt;half&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fittedvalues&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;levene&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;half&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;half&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;equal_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;7.6983153917314482&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0070815962364431974&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that th HetGoldfeldQuandt test shows the variance of the data is not significantly increasing with fitted values, but the levene test shows that there is not equal variance between the lower half and the upper half of the results.   If true, we need to have a fundamentally different model.  We could try a power law relationship.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;model4 = sm.OLS(np.log(yc+1),xc).fit()
model4.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;AVGEXP&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   6.576&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;0.000159&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;08:01:19&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -97.129&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    72&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   204.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    67&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   215.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     4&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;        &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;    &lt;td&gt;    3.6758&lt;/td&gt; &lt;td&gt;    0.677&lt;/td&gt; &lt;td&gt;    5.432&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.325     5.027&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;INCOME&lt;/th&gt;   &lt;td&gt;    0.7680&lt;/td&gt; &lt;td&gt;    0.273&lt;/td&gt; &lt;td&gt;    2.815&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;    0.223     1.313&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;INCOMESQ&lt;/th&gt; &lt;td&gt;   -0.0463&lt;/td&gt; &lt;td&gt;    0.025&lt;/td&gt; &lt;td&gt;   -1.826&lt;/td&gt; &lt;td&gt; 0.072&lt;/td&gt; &lt;td&gt;   -0.097     0.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;AGE&lt;/th&gt;      &lt;td&gt;   -0.0238&lt;/td&gt; &lt;td&gt;    0.019&lt;/td&gt; &lt;td&gt;   -1.273&lt;/td&gt; &lt;td&gt; 0.207&lt;/td&gt; &lt;td&gt;   -0.061     0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;OWNRENT&lt;/th&gt;  &lt;td&gt;    0.3307&lt;/td&gt; &lt;td&gt;    0.281&lt;/td&gt; &lt;td&gt;    1.175&lt;/td&gt; &lt;td&gt; 0.244&lt;/td&gt; &lt;td&gt;   -0.231     0.893&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 5.638&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.902&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.060&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   4.883&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.511&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;  0.0870&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.764&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    227.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(model4)
qq_plot(model4)
lev_plot(model4,xc.values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_41_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_41_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_41_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;HGQ = HetGoldfeldQuandt()
print HGQ.run(np.log(yc+1),xc)
print equal_var(model4)

(0.54047445064175459, 0.95411861481885185, &amp;#39;increasing&amp;#39;)
(0.0049114588257328637, 0.94432837966256522)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Both tests show that the variance appears to be constant with this fit.   The exponential model seems to be an improvement from this perspective.   We can look at the variance indication factor to see if there are any variables we should not be including.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def print_vif(x):
    for i,col in enumerate(x.columns):
        print col, vif(x.values,i)
print_vif(xc)

const 35.2892427253
INCOME 16.3339165879
INCOMESQ 15.21011121
AGE 1.3624340126
OWNRENT 1.43105661679
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this case the income and income square obviously predict each other, but the other variables appear to be independant.   Maybe this is the case for an interaction term in our model.  Owners and renters probably have different credit habits.  We also saw spearman correlation between age and income.  The improvement of this model will have to be explored after we cover interaction terms&lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;In the afternoon we were given a new data set and told to perform a regression fit on the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;balance = pd.read_csv(&amp;#39;../linear-regression/data/balance.csv&amp;#39;)
balance.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 400 entries, 0 to 399
Data columns (total 12 columns):
Unnamed: 0    400 non-null int64
Income        400 non-null float64
Limit         400 non-null int64
Rating        400 non-null int64
Cards         400 non-null int64
Age           400 non-null int64
Education     400 non-null int64
Gender        400 non-null object
Student       400 non-null object
Married       400 non-null object
Ethnicity     400 non-null object
Balance       400 non-null int64
dtypes: float64(1), int64(7), object(4)
memory usage: 40.6+ KB



balance.head(10)
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;80.180&lt;/td&gt;
      &lt;td&gt;8047&lt;/td&gt;
      &lt;td&gt;569&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;1151&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;20.996&lt;/td&gt;
      &lt;td&gt;3388&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;African American&lt;/td&gt;
      &lt;td&gt;203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;71.408&lt;/td&gt;
      &lt;td&gt;7114&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;87&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;872&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;15.125&lt;/td&gt;
      &lt;td&gt;3300&lt;/td&gt;
      &lt;td&gt;266&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;71.061&lt;/td&gt;
      &lt;td&gt;6819&lt;/td&gt;
      &lt;td&gt;491&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;African American&lt;/td&gt;
      &lt;td&gt;1350&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;scatter_matrix(balance,diagonal=&amp;#39;kde&amp;#39;,figsize=(14,14))
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_48_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;In order to do regression we need to remove the categorical variables and replace them with variables that are zero or one.  With Ethnicity we need to decided ona  baseline, which we decided on african american (alphabetical order).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;balance = balance.drop(&amp;quot;Unnamed: 0&amp;quot;,axis=1)
balance.Gender = balance.Gender.str.strip()
balance.Gender = np.where(balance.Gender==&amp;#39;Male&amp;#39;,1,0)
balance.Student = np.where(balance.Student==&amp;#39;Yes&amp;#39;,1,0)
balance.Married = np.where(balance.Married==&amp;#39;Yes&amp;#39;,1,0)
balance[&amp;#39;Caucasian&amp;#39;] = np.where(balance.Ethnicity==&amp;#39;Caucasian&amp;#39;,1,0)
balance[&amp;#39;Asian&amp;#39;] = np.where(balance.Ethnicity==&amp;#39;Asian&amp;#39;,1,0)
balance = balance.drop(&amp;#39;Ethnicity&amp;#39;,axis=1)
balance.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
      &lt;th&gt;Caucasian&lt;/th&gt;
      &lt;th&gt;Asian&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;yb = balance[[&amp;#39;Balance&amp;#39;]]
xb = sm.add_constant(balance.drop([&amp;#39;Balance&amp;#39;],axis=1))
modelb1 = sm.OLS(yb,xb).fit()
modelb1.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Balance&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.955&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.954&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   750.3&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;1.11e-253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;08:33:39&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -2398.7&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   400&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   4821.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   388&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   4869.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;    11&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt; -489.8611&lt;/td&gt; &lt;td&gt;   35.801&lt;/td&gt; &lt;td&gt;  -13.683&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -560.250  -419.473&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Income&lt;/th&gt;    &lt;td&gt;   -7.8031&lt;/td&gt; &lt;td&gt;    0.234&lt;/td&gt; &lt;td&gt;  -33.314&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -8.264    -7.343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.1909&lt;/td&gt; &lt;td&gt;    0.033&lt;/td&gt; &lt;td&gt;    5.824&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.126     0.255&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;    1.1365&lt;/td&gt; &lt;td&gt;    0.491&lt;/td&gt; &lt;td&gt;    2.315&lt;/td&gt; &lt;td&gt; 0.021&lt;/td&gt; &lt;td&gt;    0.171     2.102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cards&lt;/th&gt;     &lt;td&gt;   17.7245&lt;/td&gt; &lt;td&gt;    4.341&lt;/td&gt; &lt;td&gt;    4.083&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.190    26.259&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -0.6139&lt;/td&gt; &lt;td&gt;    0.294&lt;/td&gt; &lt;td&gt;   -2.088&lt;/td&gt; &lt;td&gt; 0.037&lt;/td&gt; &lt;td&gt;   -1.192    -0.036&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Education&lt;/th&gt; &lt;td&gt;   -1.0989&lt;/td&gt; &lt;td&gt;    1.598&lt;/td&gt; &lt;td&gt;   -0.688&lt;/td&gt; &lt;td&gt; 0.492&lt;/td&gt; &lt;td&gt;   -4.241     2.043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Gender&lt;/th&gt;    &lt;td&gt;   10.6532&lt;/td&gt; &lt;td&gt;    9.914&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.283&lt;/td&gt; &lt;td&gt;   -8.839    30.145&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student&lt;/th&gt;   &lt;td&gt;  425.7474&lt;/td&gt; &lt;td&gt;   16.723&lt;/td&gt; &lt;td&gt;   25.459&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  392.869   458.626&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Married&lt;/th&gt;   &lt;td&gt;   -8.5339&lt;/td&gt; &lt;td&gt;   10.363&lt;/td&gt; &lt;td&gt;   -0.824&lt;/td&gt; &lt;td&gt; 0.411&lt;/td&gt; &lt;td&gt;  -28.908    11.841&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Caucasian&lt;/th&gt; &lt;td&gt;   10.1070&lt;/td&gt; &lt;td&gt;   12.210&lt;/td&gt; &lt;td&gt;    0.828&lt;/td&gt; &lt;td&gt; 0.408&lt;/td&gt; &lt;td&gt;  -13.899    34.113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asian&lt;/th&gt;     &lt;td&gt;   16.8042&lt;/td&gt; &lt;td&gt;   14.119&lt;/td&gt; &lt;td&gt;    1.190&lt;/td&gt; &lt;td&gt; 0.235&lt;/td&gt; &lt;td&gt;  -10.955    44.564&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt;34.899&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;  41.766&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.782&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;8.52e-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.241&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;3.87e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(modelb1)
qq_plot(modelb1)
lev_plot(modelb1,xb.values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_52_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_52_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_52_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have some very strange behavor with our model.   I am currious what is happening here.  Lets start by looking at the data points where we have the strang line on the studentized plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;balance[modelb1.fittedvalues &amp;lt; 100].describe()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
      &lt;th&gt;Caucasian&lt;/th&gt;
      &lt;th&gt;Asian&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;28.950296&lt;/td&gt;
      &lt;td&gt;2078.024691&lt;/td&gt;
      &lt;td&gt;177.074074&lt;/td&gt;
      &lt;td&gt;2.753086&lt;/td&gt;
      &lt;td&gt;55.679012&lt;/td&gt;
      &lt;td&gt;13.419753&lt;/td&gt;
      &lt;td&gt;0.530864&lt;/td&gt;
      &lt;td&gt;0.012346&lt;/td&gt;
      &lt;td&gt;0.592593&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.481481&lt;/td&gt;
      &lt;td&gt;0.308642&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;15.648895&lt;/td&gt;
      &lt;td&gt;736.739048&lt;/td&gt;
      &lt;td&gt;47.463085&lt;/td&gt;
      &lt;td&gt;1.145981&lt;/td&gt;
      &lt;td&gt;17.237914&lt;/td&gt;
      &lt;td&gt;2.801179&lt;/td&gt;
      &lt;td&gt;0.502156&lt;/td&gt;
      &lt;td&gt;0.111111&lt;/td&gt;
      &lt;td&gt;0.494413&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.502770&lt;/td&gt;
      &lt;td&gt;0.464811&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;10.726000&lt;/td&gt;
      &lt;td&gt;855.000000&lt;/td&gt;
      &lt;td&gt;93.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;24.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;18.036000&lt;/td&gt;
      &lt;td&gt;1501.000000&lt;/td&gt;
      &lt;td&gt;142.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;40.000000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;27.039000&lt;/td&gt;
      &lt;td&gt;2047.000000&lt;/td&gt;
      &lt;td&gt;173.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;57.000000&lt;/td&gt;
      &lt;td&gt;14.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;34.772000&lt;/td&gt;
      &lt;td&gt;2529.000000&lt;/td&gt;
      &lt;td&gt;199.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;70.000000&lt;/td&gt;
      &lt;td&gt;15.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;92.112000&lt;/td&gt;
      &lt;td&gt;4612.000000&lt;/td&gt;
      &lt;td&gt;344.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;84.000000&lt;/td&gt;
      &lt;td&gt;19.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;balance[modelb1.fittedvalues &amp;gt; 100].describe()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
      &lt;th&gt;Caucasian&lt;/th&gt;
      &lt;th&gt;Asian&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;49.349781&lt;/td&gt;
      &lt;td&gt;5410.407524&lt;/td&gt;
      &lt;td&gt;400.103448&lt;/td&gt;
      &lt;td&gt;3.009404&lt;/td&gt;
      &lt;td&gt;55.664577&lt;/td&gt;
      &lt;td&gt;13.457680&lt;/td&gt;
      &lt;td&gt;0.470219&lt;/td&gt;
      &lt;td&gt;0.122257&lt;/td&gt;
      &lt;td&gt;0.617555&lt;/td&gt;
      &lt;td&gt;652.056426&lt;/td&gt;
      &lt;td&gt;0.501567&lt;/td&gt;
      &lt;td&gt;0.241379&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;37.582144&lt;/td&gt;
      &lt;td&gt;2071.839874&lt;/td&gt;
      &lt;td&gt;139.162472&lt;/td&gt;
      &lt;td&gt;1.419730&lt;/td&gt;
      &lt;td&gt;17.279892&lt;/td&gt;
      &lt;td&gt;3.206312&lt;/td&gt;
      &lt;td&gt;0.499896&lt;/td&gt;
      &lt;td&gt;0.328097&lt;/td&gt;
      &lt;td&gt;0.486748&lt;/td&gt;
      &lt;td&gt;422.907364&lt;/td&gt;
      &lt;td&gt;0.500783&lt;/td&gt;
      &lt;td&gt;0.428592&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;10.354000&lt;/td&gt;
      &lt;td&gt;1160.000000&lt;/td&gt;
      &lt;td&gt;126.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;23.000000&lt;/td&gt;
      &lt;td&gt;5.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;23.022500&lt;/td&gt;
      &lt;td&gt;3907.000000&lt;/td&gt;
      &lt;td&gt;298.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;42.500000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;305.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;36.508000&lt;/td&gt;
      &lt;td&gt;5110.000000&lt;/td&gt;
      &lt;td&gt;377.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;56.000000&lt;/td&gt;
      &lt;td&gt;14.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;607.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;62.848500&lt;/td&gt;
      &lt;td&gt;6408.000000&lt;/td&gt;
      &lt;td&gt;467.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;69.000000&lt;/td&gt;
      &lt;td&gt;16.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;950.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;186.634000&lt;/td&gt;
      &lt;td&gt;13913.000000&lt;/td&gt;
      &lt;td&gt;982.000000&lt;/td&gt;
      &lt;td&gt;9.000000&lt;/td&gt;
      &lt;td&gt;98.000000&lt;/td&gt;
      &lt;td&gt;20.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1999.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;balance.describe()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
      &lt;th&gt;Caucasian&lt;/th&gt;
      &lt;th&gt;Asian&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.00000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
      &lt;td&gt;400.00000&lt;/td&gt;
      &lt;td&gt;400.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;45.218885&lt;/td&gt;
      &lt;td&gt;4735.600000&lt;/td&gt;
      &lt;td&gt;354.940000&lt;/td&gt;
      &lt;td&gt;2.957500&lt;/td&gt;
      &lt;td&gt;55.667500&lt;/td&gt;
      &lt;td&gt;13.450000&lt;/td&gt;
      &lt;td&gt;0.482500&lt;/td&gt;
      &lt;td&gt;0.100000&lt;/td&gt;
      &lt;td&gt;0.61250&lt;/td&gt;
      &lt;td&gt;520.015000&lt;/td&gt;
      &lt;td&gt;0.49750&lt;/td&gt;
      &lt;td&gt;0.255000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;35.244273&lt;/td&gt;
      &lt;td&gt;2308.198848&lt;/td&gt;
      &lt;td&gt;154.724143&lt;/td&gt;
      &lt;td&gt;1.371275&lt;/td&gt;
      &lt;td&gt;17.249807&lt;/td&gt;
      &lt;td&gt;3.125207&lt;/td&gt;
      &lt;td&gt;0.500319&lt;/td&gt;
      &lt;td&gt;0.300376&lt;/td&gt;
      &lt;td&gt;0.48779&lt;/td&gt;
      &lt;td&gt;459.758877&lt;/td&gt;
      &lt;td&gt;0.50062&lt;/td&gt;
      &lt;td&gt;0.436407&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;10.354000&lt;/td&gt;
      &lt;td&gt;855.000000&lt;/td&gt;
      &lt;td&gt;93.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;23.000000&lt;/td&gt;
      &lt;td&gt;5.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;21.007250&lt;/td&gt;
      &lt;td&gt;3088.000000&lt;/td&gt;
      &lt;td&gt;247.250000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;41.750000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;68.750000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;33.115500&lt;/td&gt;
      &lt;td&gt;4622.500000&lt;/td&gt;
      &lt;td&gt;344.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;56.000000&lt;/td&gt;
      &lt;td&gt;14.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;459.500000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;57.470750&lt;/td&gt;
      &lt;td&gt;5872.750000&lt;/td&gt;
      &lt;td&gt;437.250000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;70.000000&lt;/td&gt;
      &lt;td&gt;16.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;863.000000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;186.634000&lt;/td&gt;
      &lt;td&gt;13913.000000&lt;/td&gt;
      &lt;td&gt;982.000000&lt;/td&gt;
      &lt;td&gt;9.000000&lt;/td&gt;
      &lt;td&gt;98.000000&lt;/td&gt;
      &lt;td&gt;20.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1999.000000&lt;/td&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We see that relative to the averages, the trouble points looked to be people with no balance.  If we do a fit without balance, just to check, I am curreous if we get a good model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;modelb2 = sm.OLS(yb[yb.values&amp;gt;0],xb[yb.values&amp;gt;0]).fit()
modelb2.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Balance&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.999&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.999&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;4.366e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt;  &lt;td&gt;  0.00&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;08:44:21&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -1162.5&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   310&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   2349.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   298&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   2394.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;    11&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt; -696.6745&lt;/td&gt; &lt;td&gt;    4.406&lt;/td&gt; &lt;td&gt; -158.103&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -705.346  -688.003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Income&lt;/th&gt;    &lt;td&gt;   -9.9916&lt;/td&gt; &lt;td&gt;    0.029&lt;/td&gt; &lt;td&gt; -339.458&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -10.050    -9.934&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.3360&lt;/td&gt; &lt;td&gt;    0.004&lt;/td&gt; &lt;td&gt;   84.135&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328     0.344&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;   -0.1433&lt;/td&gt; &lt;td&gt;    0.059&lt;/td&gt; &lt;td&gt;   -2.428&lt;/td&gt; &lt;td&gt; 0.016&lt;/td&gt; &lt;td&gt;   -0.259    -0.027&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cards&lt;/th&gt;     &lt;td&gt;   25.4764&lt;/td&gt; &lt;td&gt;    0.500&lt;/td&gt; &lt;td&gt;   50.962&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.493    26.460&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -1.0029&lt;/td&gt; &lt;td&gt;    0.036&lt;/td&gt; &lt;td&gt;  -28.215&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.073    -0.933&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Education&lt;/th&gt; &lt;td&gt;   -0.0080&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;   -0.042&lt;/td&gt; &lt;td&gt; 0.966&lt;/td&gt; &lt;td&gt;   -0.381     0.365&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Gender&lt;/th&gt;    &lt;td&gt;   -0.2332&lt;/td&gt; &lt;td&gt;    1.200&lt;/td&gt; &lt;td&gt;   -0.194&lt;/td&gt; &lt;td&gt; 0.846&lt;/td&gt; &lt;td&gt;   -2.596     2.129&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student&lt;/th&gt;   &lt;td&gt;  500.8310&lt;/td&gt; &lt;td&gt;    1.880&lt;/td&gt; &lt;td&gt;  266.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  497.132   504.530&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Married&lt;/th&gt;   &lt;td&gt;   -2.0625&lt;/td&gt; &lt;td&gt;    1.261&lt;/td&gt; &lt;td&gt;   -1.636&lt;/td&gt; &lt;td&gt; 0.103&lt;/td&gt; &lt;td&gt;   -4.543     0.418&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Caucasian&lt;/th&gt; &lt;td&gt;   -0.0700&lt;/td&gt; &lt;td&gt;    1.467&lt;/td&gt; &lt;td&gt;   -0.048&lt;/td&gt; &lt;td&gt; 0.962&lt;/td&gt; &lt;td&gt;   -2.956     2.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asian&lt;/th&gt;     &lt;td&gt;   -1.3785&lt;/td&gt; &lt;td&gt;    1.731&lt;/td&gt; &lt;td&gt;   -0.796&lt;/td&gt; &lt;td&gt; 0.426&lt;/td&gt; &lt;td&gt;   -4.784     2.027&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.799&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.067&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.150&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   3.766&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.269&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.152&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.963&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;4.37e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(modelb2)
qq_plot(modelb2)
lev_plot(modelb2,xb[yb.values&amp;gt;0].values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_59_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_59_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_59_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;So we have a great fit where the variance is constant, the residuals are normally distributed, and there are no high leverage points if we fit the data with removing the zero-balance points.   People with zero balance, however, are in the data.   Our regression does not make valid predictions for people with zero balnce.  &lt;/p&gt;
&lt;p&gt;This seems like a two step process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Classify if the person is likely to have zero balance.&lt;/li&gt;
&lt;li&gt;If they do predict zero, otherwise predict the regression model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another solution would be to cut on credit rating.  One model for those above 350 credit rating, and another model for those with less than 350 credit rating.  This is because in the zero balance group, the max credit rating was 344.  Some of the issues is that we not know if this is a general cutoff, but lets try it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;modelb3 = sm.OLS(yb[xb.Rating &amp;gt; 350],xb[xb.Rating &amp;gt; 350]).fit()
modelb3.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Balance&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.999&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.999&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;1.854e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;1.07e-273&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;09:06:41&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -741.54&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   196&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   1507.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   184&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   1546.&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;    11&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt; -691.8364&lt;/td&gt; &lt;td&gt;    6.766&lt;/td&gt; &lt;td&gt; -102.258&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -705.184  -678.488&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Income&lt;/th&gt;    &lt;td&gt;   -9.9829&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt; -271.846&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -10.055    -9.910&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.3346&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   65.301&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.325     0.345&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;   -0.1283&lt;/td&gt; &lt;td&gt;    0.076&lt;/td&gt; &lt;td&gt;   -1.698&lt;/td&gt; &lt;td&gt; 0.091&lt;/td&gt; &lt;td&gt;   -0.277     0.021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cards&lt;/th&gt;     &lt;td&gt;   25.3670&lt;/td&gt; &lt;td&gt;    0.660&lt;/td&gt; &lt;td&gt;   38.407&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.064    26.670&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -1.0321&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;  -21.524&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.127    -0.938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Education&lt;/th&gt; &lt;td&gt;   -0.0099&lt;/td&gt; &lt;td&gt;    0.252&lt;/td&gt; &lt;td&gt;   -0.039&lt;/td&gt; &lt;td&gt; 0.969&lt;/td&gt; &lt;td&gt;   -0.507     0.487&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Gender&lt;/th&gt;    &lt;td&gt;   -1.5948&lt;/td&gt; &lt;td&gt;    1.597&lt;/td&gt; &lt;td&gt;   -0.998&lt;/td&gt; &lt;td&gt; 0.319&lt;/td&gt; &lt;td&gt;   -4.746     1.557&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student&lt;/th&gt;   &lt;td&gt;  501.4411&lt;/td&gt; &lt;td&gt;    2.587&lt;/td&gt; &lt;td&gt;  193.814&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  496.337   506.546&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Married&lt;/th&gt;   &lt;td&gt;   -2.5047&lt;/td&gt; &lt;td&gt;    1.725&lt;/td&gt; &lt;td&gt;   -1.452&lt;/td&gt; &lt;td&gt; 0.148&lt;/td&gt; &lt;td&gt;   -5.907     0.898&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Caucasian&lt;/th&gt; &lt;td&gt;    0.1485&lt;/td&gt; &lt;td&gt;    1.961&lt;/td&gt; &lt;td&gt;    0.076&lt;/td&gt; &lt;td&gt; 0.940&lt;/td&gt; &lt;td&gt;   -3.721     4.018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asian&lt;/th&gt;     &lt;td&gt;   -2.5407&lt;/td&gt; &lt;td&gt;    2.280&lt;/td&gt; &lt;td&gt;   -1.115&lt;/td&gt; &lt;td&gt; 0.266&lt;/td&gt; &lt;td&gt;   -7.038     1.957&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 5.399&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.067&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   5.524&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.404&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;  0.0632&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.842&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;5.92e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;modelb4 = sm.OLS(yb[xb.Rating &amp;lt;= 350],xb[xb.Rating &amp;lt;= 350]).fit()
modelb4.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Balance&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.842&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   99.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Wed, 17 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;3.80e-73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;09:07:02&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -1197.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   204&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   2420.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   192&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   2459.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;    11&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt; -218.3506&lt;/td&gt; &lt;td&gt;   47.949&lt;/td&gt; &lt;td&gt;   -4.554&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -312.924  -123.777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Income&lt;/th&gt;    &lt;td&gt;   -7.1002&lt;/td&gt; &lt;td&gt;    0.446&lt;/td&gt; &lt;td&gt;  -15.913&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -7.980    -6.220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.1523&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;    3.563&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.068     0.237&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;    0.4791&lt;/td&gt; &lt;td&gt;    0.654&lt;/td&gt; &lt;td&gt;    0.732&lt;/td&gt; &lt;td&gt; 0.465&lt;/td&gt; &lt;td&gt;   -0.811     1.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cards&lt;/th&gt;     &lt;td&gt;   13.9981&lt;/td&gt; &lt;td&gt;    5.762&lt;/td&gt; &lt;td&gt;    2.430&lt;/td&gt; &lt;td&gt; 0.016&lt;/td&gt; &lt;td&gt;    2.634    25.362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -0.6226&lt;/td&gt; &lt;td&gt;    0.368&lt;/td&gt; &lt;td&gt;   -1.691&lt;/td&gt; &lt;td&gt; 0.092&lt;/td&gt; &lt;td&gt;   -1.349     0.104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Education&lt;/th&gt; &lt;td&gt;   -2.7730&lt;/td&gt; &lt;td&gt;    2.072&lt;/td&gt; &lt;td&gt;   -1.338&lt;/td&gt; &lt;td&gt; 0.182&lt;/td&gt; &lt;td&gt;   -6.859     1.313&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Gender&lt;/th&gt;    &lt;td&gt;   25.9813&lt;/td&gt; &lt;td&gt;   12.584&lt;/td&gt; &lt;td&gt;    2.065&lt;/td&gt; &lt;td&gt; 0.040&lt;/td&gt; &lt;td&gt;    1.161    50.801&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student&lt;/th&gt;   &lt;td&gt;  339.5260&lt;/td&gt; &lt;td&gt;   22.216&lt;/td&gt; &lt;td&gt;   15.283&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  295.707   383.345&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Married&lt;/th&gt;   &lt;td&gt;   -8.2622&lt;/td&gt; &lt;td&gt;   12.799&lt;/td&gt; &lt;td&gt;   -0.646&lt;/td&gt; &lt;td&gt; 0.519&lt;/td&gt; &lt;td&gt;  -33.507    16.983&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Caucasian&lt;/th&gt; &lt;td&gt;   22.5267&lt;/td&gt; &lt;td&gt;   15.427&lt;/td&gt; &lt;td&gt;    1.460&lt;/td&gt; &lt;td&gt; 0.146&lt;/td&gt; &lt;td&gt;   -7.901    52.955&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asian&lt;/th&gt;     &lt;td&gt;   14.5170&lt;/td&gt; &lt;td&gt;   17.628&lt;/td&gt; &lt;td&gt;    0.824&lt;/td&gt; &lt;td&gt; 0.411&lt;/td&gt; &lt;td&gt;  -20.252    49.286&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.153&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.919&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.207&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   2.562&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.155&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.547&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;2.49e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(modelb3)
qq_plot(modelb3)
lev_plot(modelb3,xb[xb.Rating&amp;gt;350].values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_63_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_63_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The model for above 350 Credit Rating looks good.  Lets check below 350.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;stud_plot(modelb4)
qq_plot(modelb4)
lev_plot(modelb4,xb[xb.Rating&amp;lt;=350].values)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_65_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_65_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D2/output_65_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have the same issue before.   We must made a cut to make a good prediction, then removed the values from the data that were causing issues without knowing the balance.   This can be useful in practice, but i find it a little unstatisfying.   For now we will stop with this exploration.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="regression"></category></entry><entry><title>Galvanize - Week 03 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-01/" rel="alternate"></link><updated>2015-06-15T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-15:galvanize/galvanize-data-science-03-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 3 - Day 1&lt;/h2&gt;
&lt;p&gt;Today we had the first quiz I did not finish.  I attempted to do algebra in latex, and made a sign mistake and had difficulty traking my mistake.   &lt;/p&gt;
&lt;p&gt;The morning involved using numpy to solve some linear algrebra based problems.  The afternoon had to do with exploratory data analysis and linear regression.   &lt;/p&gt;
&lt;h2&gt;Miniquiz&lt;/h2&gt;
&lt;p&gt;Probability Practice&lt;/p&gt;
&lt;p&gt;Let's say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game for us to break even in the long term? Show your work.&lt;/p&gt;
&lt;p&gt;P(n) = p^(n-1)*(1-p)&lt;/p&gt;
&lt;p&gt;$$E(V) = \Sigma_{i=1}^{\infty} p^{n-1} \ (1-p) \ (2n-1) $$&lt;/p&gt;
&lt;p&gt;$$E(V) = \Sigma_{i=1}^{\infty} (2n-1) (p^{n-1} - p^n) $$&lt;/p&gt;
&lt;p&gt;$$E(V) = \Sigma{i=1}^{\infty} 2 \ n \ p^{n-1} - \Sigma{i=1}^{\infty} 2 \ n \ p^{n} - \Sigma{i=1}^{\infty} p^{n-1} + \Sigma{i=1}^{\infty} p^{n} $$&lt;/p&gt;
&lt;p&gt;$$E(V) = 2 + \Sigma{i=1}^{\infty} 2 \ (n+1)\ p^{n} - \Sigma{i=1}^{\infty} 2 \ n \ p^{n} - 1 - \Sigma{i=1}^{\infty} p^{n} + \Sigma{i=1}^{\infty} p^{n} $$&lt;/p&gt;
&lt;p&gt;$$E(V) = 1 + \Sigma_{i=1}^{\infty} 2 p^{n} $$&lt;/p&gt;
&lt;p&gt;$$E(V) = 1 + \frac{2 \ p }{1-p}$$&lt;/p&gt;
&lt;p&gt;Write a program to simulate the game and verify that your answer is correct.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;val&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;an&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pays&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;pays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;pays&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pays&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;simulation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;



&lt;span class="mf"&gt;1.22222222222&lt;/span&gt; &lt;span class="mf"&gt;1.22222222222&lt;/span&gt; &lt;span class="mf"&gt;1.2154&lt;/span&gt;
&lt;span class="mf"&gt;1.46575342466&lt;/span&gt; &lt;span class="mf"&gt;1.46575342466&lt;/span&gt; &lt;span class="mf"&gt;1.4702&lt;/span&gt;
&lt;span class="mf"&gt;1.76923076923&lt;/span&gt; &lt;span class="mf"&gt;1.76923076923&lt;/span&gt; &lt;span class="mf"&gt;1.7798&lt;/span&gt;
&lt;span class="mf"&gt;2.15789473684&lt;/span&gt; &lt;span class="mf"&gt;2.15789473684&lt;/span&gt; &lt;span class="mf"&gt;2.1384&lt;/span&gt;
&lt;span class="mf"&gt;2.67346938776&lt;/span&gt; &lt;span class="mf"&gt;2.67346938776&lt;/span&gt; &lt;span class="mf"&gt;2.6592&lt;/span&gt;
&lt;span class="mf"&gt;3.39024390244&lt;/span&gt; &lt;span class="mf"&gt;3.39024390244&lt;/span&gt; &lt;span class="mf"&gt;3.3546&lt;/span&gt;
&lt;span class="mf"&gt;4.45454545455&lt;/span&gt; &lt;span class="mf"&gt;4.45454545455&lt;/span&gt; &lt;span class="mf"&gt;4.3904&lt;/span&gt;
&lt;span class="mf"&gt;6.2&lt;/span&gt; &lt;span class="mf"&gt;6.2&lt;/span&gt; &lt;span class="mf"&gt;6.186&lt;/span&gt;
&lt;span class="mf"&gt;9.58823529412&lt;/span&gt; &lt;span class="mf"&gt;9.58823529412&lt;/span&gt; &lt;span class="mf"&gt;9.6414&lt;/span&gt;
&lt;span class="mf"&gt;19.0&lt;/span&gt; &lt;span class="mf"&gt;19.0&lt;/span&gt; &lt;span class="mf"&gt;19.1224&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Linear Algebra Practice:&lt;/h2&gt;
&lt;p&gt;The stochastic matrix is central to the Markov process. It is a sqaure matrix specifying that probabilities of going from one state to the other such that every column of the matrix sums to 1.  The probability of entering a certain state depends only on the last state occupied and the stochastic matrix, not on any earlier states&lt;/p&gt;
&lt;p&gt;Suppose that the 2004 &lt;strong&gt;state of land use&lt;/strong&gt; in a city of 60 mi^2 of built-up
area is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;In 2004:

C (Commercially Used): 25%
I (Industrially Used): 20%
R (Residentially Used): 55%
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Find the &lt;strong&gt;state of land use&lt;/strong&gt; in &lt;strong&gt;2009&lt;/strong&gt; and &lt;strong&gt;2014&lt;/strong&gt;,
   assuming that the transition probabilities for 5-year intervals are given
   by the matrix &lt;strong&gt;A&lt;/strong&gt; and remain practically the same over the time considered.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div align="center"&gt;
      &lt;img src="images/transition_matix_A.png"&gt;
   &lt;/div&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;A_5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;A_10&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A_5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A_5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;A_10&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2004: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2009: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A_5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2014: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A_10&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="mi"&gt;2004&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;  &lt;span class="mf"&gt;0.2&lt;/span&gt;   &lt;span class="mf"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;2009&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.195&lt;/span&gt;  &lt;span class="mf"&gt;0.34&lt;/span&gt;   &lt;span class="mf"&gt;0.465&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;2014&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.1705&lt;/span&gt;  &lt;span class="mf"&gt;0.438&lt;/span&gt;   &lt;span class="mf"&gt;0.3915&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Part 1.2&lt;/h3&gt;
&lt;p&gt;This following question uses the &lt;code&gt;iris&lt;/code&gt; dataset. Load the data in with the following code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="c"&gt;# The 1st column is sepal length and the 2nd column is sepal width&lt;/span&gt;
&lt;span class="n"&gt;sepalLength_sepalWidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make a scatter plot of sepal width vs sepal length&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the mean vector (column-wise) of the data matrix. The &lt;code&gt;shape&lt;/code&gt;
   of the mean vector should be &lt;code&gt;(1, 2)&lt;/code&gt; and plot it.&lt;/p&gt;
&lt;p&gt;%matplotlib inline
from sklearn import datasets
import matplotlib.pyplot as plt&lt;/p&gt;
&lt;p&gt;sepalLength_sepalWidth = datasets.load_iris().data[:, :2]
plt.plot(sepalLength_sepalWidth[:,0],sepalLength_sepalWidth[:,1],linewidth=0,marker='o',color='steelblue',alpha=0.25,label="Data")
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.plot(sepalLength_sepalWidth[:,0].mean(),sepalLength_sepalWidth[:,1].mean(),'x',
         markeredgewidth=10,markersize=20,color='indianred',label='Mean',alpha=0.5)
print "Shape: ",sepalLength_sepalWidth.shape&lt;/p&gt;
&lt;p&gt;Shape:  (150, 2)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_5_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Write a function (&lt;code&gt;euclidean_dist&lt;/code&gt;) to calculate the euclidean distance
   between two &lt;strong&gt;column vectors (not row vector)&lt;/strong&gt;. Your function should check
   if the vectors are column vectors and the shape of the two vectors are the same .&lt;/p&gt;
&lt;p&gt;def euclidean_dist(c1,c2):
    if c2.shape==c1.shape and c1.shape[1]==1:
        return np.sqrt(np.sum(np.power(c1-c2,2)))
    return None&lt;/p&gt;
&lt;p&gt;a = np.ones((10,1))
b = np.zeros((10,1))
euclidean_dist(a,b)==np.sqrt(10)&lt;/p&gt;
&lt;p&gt;True&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write a function (&lt;code&gt;cosine_sim&lt;/code&gt;) to calculate the cosine similarity_between 
   two &lt;strong&gt;column vectors (not row vector)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;def cosine_sim(c1,c2):
    if c2.shape==c1.shape and c1.shape[1]==1:
        result = c1.transpose().dot(c2)/(np.linalg.norm(c1)*np.linalg.norm(c2))
        return result[0,0]
    return None&lt;/p&gt;
&lt;p&gt;a = np.array([1,0]).reshape(2,1)
b = np.array([0,1]).reshape(2,1)
print cosine_sim(a,b)==0
a = np.array([1,0]).reshape(2,1)
b = np.array([1,1]).reshape(2,1)
print cosine_sim(a,b) == 1/np.sqrt(2)&lt;/p&gt;
&lt;p&gt;True
True&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write a function that would loop through all the data points in a given matrix and 
   calculate the given distance metric between each of the data point and the mean
   vector. Use the function to compute Euclidean Distance and Cosine Similarity between each of
   the data points and the mean of the data points. You should be able to call the function
   in this manner:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot histograms of the euclidean distances and cosine similarities.&lt;/p&gt;
&lt;p&gt;def compute_dist(data,func):
    means = data.mean(axis=0).transpose().reshape(data.shape[1],1)
    dists = []
    for i in range(data.shape[0]):
        dists.append(func(data[i,:].reshape(data.shape[1],1),means))
    dists = np.array(dists)
    dists = dists.reshape(data.shape[0],1)
    return dists&lt;/p&gt;
&lt;p&gt;values = compute_dist(sepalLength_sepalWidth,euclidean_dist)
plt.hist(values[:,0],color='steelblue',alpha=0.5,edgecolor='black',linewidth=0.1)
plt.xlabel('Euclidean Distance From Mean')
plt.ylabel('Count')
plt.show()
values = compute_dist(sepalLength_sepalWidth,cosine_sim)
plt.hist(values[:,0],color='steelblue',alpha=0.5,linewidth=0.1)
plt.xlabel('Cosign Similarity From Mean')
plt.ylabel('Count')
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_11_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_11_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Extra Credit: Implementing the PageRank Algorithm&lt;/h2&gt;
&lt;p&gt;The &lt;a href="http://en.wikipedia.org/wiki/PageRank"&gt;Page Rank Algorithm&lt;/a&gt; is used by Google
Search (in their early days) to rank websites in their search engine in terms 
of the importance of webpages. 
&lt;a href="http://books.google.com/books/p/princeton?id=5o_K4rri1CsC&amp;amp;printsec=frontcover&amp;amp;source=gbs_ViewAPI&amp;amp;hl=en#v=onepage&amp;amp;q&amp;amp;f=false"&gt;More about PageRank&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will implement PageRank on this simple network of websites.&lt;/p&gt;
&lt;p&gt;&lt;div align="center"&gt;
    &lt;img src="images/pageweb.png"&gt;
   &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the above image:&lt;/strong&gt;
   - Each node is a web page
   - Each directed edge corresponds to one page referencing the other
   - These web pages correspond to the states our Markov chain can be in
   - Assume that the model of our chain is that of a random surfer/walker.&lt;/p&gt;
&lt;p&gt;In this model, we transition from one web page (state) to the next with
equal probability (to begin).  Or rather we randomly pick an outgoing edge
from our current state.  Before we can do any sort of calculation we need to
know how we will move on this Markov Chain.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PR = np.array([[0,1,0,0,0],[0.5,0,0.5,0,0],[0.333333,0.333333,0,0,0.333333],[1.,0,0,0,0],[0,0.3333333,0.3333333,0.3333333,0]]).transpose()
PR




array([[ 0.       ,  0.5      ,  0.333333 ,  1.       ,  0.       ],
       [ 1.       ,  0.       ,  0.333333 ,  0.       ,  0.3333333],
       [ 0.       ,  0.5      ,  0.       ,  0.       ,  0.3333333],
       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.3333333],
       [ 0.       ,  0.       ,  0.333333 ,  0.       ,  0.       ]])
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Now that we have a transition matrix, the next step is to iterate on this
   from one page to the next (like someone blindly navigating the internet) and
   see where we end up. The probability distribution for our random surfer can
   be described in this matrix notation as well (or vector rather).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Initialize a vector for the probability of where our random surfer is.
   It will be a vector with length equal to the number of pages.
   Initialize it to be equally probable to start on any page
   (i.e. you start randomly in a state on the chain).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;start = np.array([0.2,0.2,0.2,0.2,0.2]).reshape(5,1)
start




array([[ 0.2],
       [ 0.2],
       [ 0.2],
       [ 0.2],
       [ 0.2]])
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To take a step on the chain, simply matrix multiple our user vector by the
   transition matrix.
   After one iteration, what is the most likely location for your random surfer?&lt;/p&gt;
&lt;p&gt;step1 = PR.dot(start)
step1&lt;/p&gt;
&lt;p&gt;array([[ 0.3666666 ],
       [ 0.33333326],
       [ 0.16666666],
       [ 0.06666666],
       [ 0.0666666 ]])&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot how the probabilities change.
   Iterate the matrix through the first ten steps.
   At each step create a bar plot of the surfers probability vector.&lt;/p&gt;
&lt;p&gt;steps = start.copy()
plt.figure(figsize=(15,8))
for i in range(10):
    steps = PR.dot(steps)
    ax = plt.subplot(2,5,i+1)
    plt.bar(np.arange(5),steps[:,0],color='steelblue',alpha=0.5,linewidth=0.1)
    ax.set_xticklabels (('A', 'B', 'C', 'D', 'E') )
    plt.title(str(i+1) + " Steps")
plt.show()
print "Final Distribution: ",steps&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_19_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Final Distribution:  [[ 0.29401788]
 [ 0.38811497]
 [ 0.22055748]
 [ 0.02429332]
 [ 0.07301416]]
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;This time to compute the stationary distribution, we can use numpy's
   matrix operations. Using the function for calculating &lt;a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html"&gt;eigenvectors&lt;/a&gt; compute the
   stationary distibution (page rank).  Is it the same as what you found
   from above?  What is it's eigenvalue?&lt;/p&gt;
&lt;p&gt;values,Vectors = np.linalg.eig(PR)
values&lt;/p&gt;
&lt;p&gt;array([ 0.99999977+0.j        , -0.64800046+0.27072373j,
       -0.64800046-0.27072373j,  0.14800058+0.30123034j,
        0.14800058-0.30123034j])&lt;/p&gt;
&lt;p&gt;solution = np.real(Vectors[:,0]/np.sum(Vectors[:,0]))
solution.reshape(5,1)&lt;/p&gt;
&lt;p&gt;array([[ 0.29268293],
       [ 0.39024392],
       [ 0.21951223],
       [ 0.02439023],
       [ 0.07317069]])&lt;/p&gt;
&lt;p&gt;import pandas as pd
df = pd.DataFrame(columns=["A","B","C","D","E"])
steps = start.copy()
for i in range(20):
    df.loc[i] = steps[:,0].transpose()
    steps = PR.dot(steps)&lt;/p&gt;
&lt;p&gt;df.plot()
plt.legend(bbox_to_anchor=(0.08,1),ncol=5, loc=3, borderaxespad=0.)
print "Final Distribution: "
steps&lt;/p&gt;
&lt;p&gt;Final Distribution: &lt;/p&gt;
&lt;p&gt;array([[ 0.29265924],
       [ 0.39030474],
       [ 0.21945581],
       [ 0.02438301],
       [ 0.07319274]])&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_23_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the stationary state is found in 6 states.   The vector produced by this method matches the eigenvectors found by numpy linear algrebra library&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PR.dot(steps)




array([[ 0.29268725],
       [ 0.39020868],
       [ 0.21954995],
       [ 0.02439758],
       [ 0.07315186]])
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Exploratory Data Analysis (EDA)&lt;/h2&gt;
&lt;p&gt;Exploratory data analysis is a first crucial step to building predictive models from your data. EDA allows you
to confirm or invalidate some of the assumptions you are making about your data and understand relationships between your variables.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this scenario, you are a data scientist at &lt;a href="http://www.bayareabikeshare.com/"&gt;Bay Area Bike Share&lt;/a&gt;. Your task
is to provide insights on bike user activity and behavior to the products team. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Load the file &lt;code&gt;data/201402_trip_data.csv&lt;/code&gt; into a dataframe.&lt;/p&gt;
&lt;p&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as scs
import statsmodels.api as sm
from pandas.tools.plotting import scatter_matrix
%matplotlib inline&lt;/p&gt;
&lt;p&gt;df = pd.read_csv('data/201402_trip_data.csv')&lt;/p&gt;
&lt;p&gt;df['start_date'] = pd.to_datetime(df.start_date)
df['end_date'] = pd.to_datetime(df.end_date)
df['month'] = df.start_date.dt.month
df['dayofweek'] = df.start_date.dt.dayofweek
df['date'] = df.start_date.dt.date
df['hour'] = df.start_date.dt.hour&lt;/p&gt;
&lt;p&gt;df.info()&lt;/p&gt;
&lt;p&gt;&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 144015 entries, 0 to 144014
Data columns (total 15 columns):
trip_id              144015 non-null int64
duration             144015 non-null int64
start_date           144015 non-null datetime64[ns]
start_station        144015 non-null object
start_terminal       144015 non-null int64
end_date             144015 non-null datetime64[ns]
end_station          144015 non-null object
end_terminal         144015 non-null int64
bike_#               144015 non-null int64
subscription_type    144015 non-null object
zip_code             137885 non-null object
month                144015 non-null int64
dayofweek            144015 non-null int64
date                 144015 non-null object
hour                 144015 non-null int64
dtypes: datetime64&lt;a href="2"&gt;ns&lt;/a&gt;, int64(8), object(5)
memory usage: 17.6+ MB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Group the bike rides by &lt;code&gt;month&lt;/code&gt; and count the number of users per month. Plot the number of users for each month. 
   What do you observe? Provide a likely explanation to your observation. Real life data can often be messy/incomplete
   and cursory EDA is often able to reveal that.&lt;/p&gt;
&lt;p&gt;m = df.groupby('month')
a = m.trip_id.count().tolist()
a = a[2:]+a[:2]
fig,ax = plt.subplots()
plt.bar(range(7),a,color='steelblue',alpha=0.5)
ax.set_xticklabels(('Aug','Sep','Oct',"Nov","Dec",'Jan','Feb'))
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_30_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that we do not have data for the entire month of august, so it is not fair to evaluate it to the other months.  We see that there is a slowdown in december, and febuary has less days.   It might make more sense to look at the daily usage rates. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Plot the daily user count from September to December. Mark the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;mean +/- 1.5 * Standard Deviation&lt;/code&gt; as 
   horizontal lines on the plot. This would help you identify the outliers in your data. Describe your observations. &lt;/p&gt;
&lt;p&gt;duc = df[df.month.isin([9, 10, 11, 12])].groupby('date')
counts = duc.trip_id.count().reset_index().set_index('date')
counts.columns = ['Count']
high = counts.mean()+1.5&lt;em&gt;counts.std()
low = counts.mean()-1.5&lt;/em&gt;counts.std()
counts.plot(color='steelblue',lw=2,alpha=.75)
plt.axhline(high.values,color='black',linestyle='--')
plt.axhline(low.values,color='black',linestyle='--')
plt.xticks(rotation=30)
plt.ylabel('Daily User Count')
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_33_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The dashed black lines are the 'outlier' boundaries.   Though these are not the traditional definition of an outlire, we see points outside of these bounds.  The issues is that we have seasonal variation.  The low months pull down the average making high months look like possible outliers.   I do not believe we see any outlires in this data set, and won't believe it until I understand the cyclical nature of the data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Plot the distribution of the daily user counts for all months as a histogram. Fit a 
   &lt;a href="http://glowingpython.blogspot.com/2012/08/kernel-density-estimation-with-scipy.html"&gt;KDE&lt;/a&gt; to the histogram.
   What is the distribution and explain why the distribution might be shaped as such. &lt;/p&gt;
&lt;p&gt;duc = df.groupby('date')
counts = duc.trip_id.count()
counts.hist(alpha=.5, linewidth=0, bins=20, normed=True)&lt;/p&gt;
&lt;p&gt;kde = scs.gaussian_kde(counts)
s = np.linspace(counts.min(), counts.max(), 100)
plt.plot(s, kde(s), color='indianred', lw=3)&lt;/p&gt;
&lt;p&gt;[&lt;matplotlib.lines.Line2D at 0x11123e610&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_36_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This looks like we could have two different contributers to daily user counts.   One that has a mean of around 1000, and another with a mean around 400.   The combination of the two sources could produce a double peaked distribution like this.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,8))
ax = plt.gca()
df[&amp;#39;weekend&amp;#39;] = df.dayofweek.isin([5,6])
duc_we = df[df[&amp;#39;weekend&amp;#39;]].groupby(&amp;#39;date&amp;#39;)
duc_wd = df[~df[&amp;#39;weekend&amp;#39;]].groupby(&amp;#39;date&amp;#39;)
counts_we = duc_we.trip_id.count().reset_index().set_index(&amp;#39;date&amp;#39;)
counts_wd = duc_wd.trip_id.count().reset_index().set_index(&amp;#39;date&amp;#39;)
counts_we.hist(alpha=.7, linewidth=0, bins=20, normed=True, color=&amp;#39;burlywood&amp;#39;,ax=ax)
counts_wd.hist(alpha=.7, linewidth=0, bins=20, normed=True, color=&amp;#39;seagreen&amp;#39;,ax=ax)

kde_we = scs.gaussian_kde(counts_we.values[:,0])
kde_wd = scs.gaussian_kde(counts_wd.values[:,0])
plt.plot(s, kde_we(s), color=&amp;#39;goldenrod&amp;#39;, lw=3, label=&amp;#39;Weekend&amp;#39;)
plt.plot(s, kde_wd(s), color=&amp;#39;forestgreen&amp;#39;, lw=3, label=&amp;#39;Weekday&amp;#39;)
plt.xlabel(&amp;#39;Number of Daily Users&amp;#39;)
plt.ylabel(&amp;#39;Probability Density&amp;#39;)
plt.title(&amp;#39;Weekend vs Weekday Daily User Count&amp;#39;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that from this distirbution that the weekend users seem to be around 400, and the weekday user counts seem to be peaked around 1000.  The combintation of the two distributions produces the initial histogram we looked at&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Now we are going to explore hourly trends of user activity. Group the bike rides by &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;hour&lt;/code&gt; and count 
   the number of rides in the given hour on the given date. Make a 
   &lt;a href="http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/"&gt;boxplot&lt;/a&gt; of the hours in the day &lt;strong&gt;(x)&lt;/strong&gt; against
   the number of users &lt;strong&gt;(y)&lt;/strong&gt; in that given hour. &lt;/p&gt;
&lt;p&gt;plt.figure(figsize=(14,8))
ax=plt.gca()
huc = df.groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Hourly User Count')
plt.show()&lt;/p&gt;
&lt;p&gt;huc.trip_id.count().reset_index().boxplot('trip_id',by='hour')
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_41_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_41_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Replot the boxplot from above after binning your data into weekday and weekend. Describe the differences you observe between hour user activity between weekday and weekend? &lt;/p&gt;
&lt;p&gt;plt.figure(figsize=(14,6))
ax=plt.subplot(1,2,1)
huc = df[df.weekend].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekend Hourly User Count')
plt.title('Weekend Hourly User Counts')&lt;/p&gt;
&lt;p&gt;ax=plt.subplot(1,2,2)
huc = df[~df.weekend].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekday Hourly User Count')
plt.title('Weekday Hourly User Counts')
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_43_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Weekend users center in daylight hours and peak shortly after lunch.   Weekday users seem to be commuters, using the bikes before and after work hours, with a similar proportion of users using the bikes around lunchtime as weekend users.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;There are two types of bike users (specified by column &lt;code&gt;Subscription Type&lt;/code&gt;: &lt;code&gt;Subscriber&lt;/code&gt; and &lt;code&gt;Customer&lt;/code&gt;. Given this
   information and the weekend and weekday categorization, plot and inspect the user activity trends. Suppose the 
   product team wants to run a promotional campaign, what are you suggestions in terms of who the promotion should 
   apply to and when it should apply for the campaign to be effective?&lt;/p&gt;
&lt;p&gt;plt.figure(figsize=(14,12))
ax=plt.subplot(2,2,1)
huc = df[(df.weekend)&amp;amp;(df.subscription_type=='Customer')].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekend Hourly User Count')
plt.ylim([0,150])
plt.title('Customer Weekend Hourly User Counts')&lt;/p&gt;
&lt;p&gt;ax=plt.subplot(2,2,2)
huc = df[(~df.weekend)&amp;amp;(df.subscription_type=='Customer')].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekday Hourly User Count')
plt.ylim([0,150])
plt.title('Customer Weekday Hourly User Counts')&lt;/p&gt;
&lt;p&gt;ax=plt.subplot(2,2,3)
huc = df[(df.weekend)&amp;amp;(df.subscription_type=='Subscriber')].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekend Hourly User Count')
plt.ylim([0,150])
plt.title('Subscriber Weekend Hourly User Counts')&lt;/p&gt;
&lt;p&gt;ax=plt.subplot(2,2,4)
huc = df[(~df.weekend)&amp;amp;(df.subscription_type=='Subscriber')].groupby(['date','hour'])
huc_vals = huc.trip_id.count().unstack()
huc_vals.fillna(0.0,inplace=True)
huc_vals.plot(kind='box',ax=ax)
plt.xlabel('Hour of Day')
plt.ylabel('Weekday Hourly User Count')
plt.ylim([0,150])
plt.title('Subscriber Weekday Hourly User Counts')&lt;/p&gt;
&lt;p&gt;plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;It seems that subscribers are mostly commuters, and responsible for the weekday double peek.  Because there seems to be a large number of bikes not be used on the weekends, or during lunch periods, a promotion targeting customers during non-peak times could help bring in more revenue.  &lt;/p&gt;
&lt;h2&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;Linear regression is an approach to modeling the relationship between a continuous dependent (&lt;strong&gt;y&lt;/strong&gt;) variable and 
one or more continuous independent (&lt;strong&gt;x&lt;/strong&gt;) variables. Here you will be introduced to fitting the model and interpreting
the results before we dive more into the details of linear regression tomorrow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We will be using the &lt;code&gt;prestige&lt;/code&gt; data in &lt;code&gt;statsmodels&lt;/code&gt;. &lt;code&gt;statsmodels&lt;/code&gt; is the de facto library for performing regression tasks in Python. Load the data with the follow code.&lt;/p&gt;
&lt;p&gt;import statsmodels.api as sm
prestige = sm.datasets.get_rdataset("Duncan", "car", cache=True).data
y = prestige['prestige']
x = prestige[['income', 'education']].astype(float)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;accountant&lt;/th&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pilot&lt;/th&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;architect&lt;/th&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;author&lt;/th&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;chemist&lt;/th&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Explore the data by making a &lt;a href="http://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html#visualization-scatter-matrix"&gt;scatter_matrix&lt;/a&gt;
   and a &lt;a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.boxplot.html"&gt;boxplot&lt;/a&gt;
   to show the range of each of your variables.&lt;/p&gt;
&lt;p&gt;scatter_matrix(prestige[['prestige','income','education']], diagonal='kde', figsize=(10,10))
plt.show()&lt;/p&gt;
&lt;p&gt;prestige[['prestige','income','education']].boxplot()
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_51_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW03D1/output_51_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the values have been normalized between 0 and 100.  It also looks that prestige is related to education and income, but so are income and education.   Since these values are not independant, we might see funny results in our fits.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The beta coefficients of a linear regression model can be calculated by solving the normal equation.
   Using numpy, write a function that solves the &lt;strong&gt;normal equation&lt;/strong&gt; (below).
   As input your function should take a matrix of features (&lt;strong&gt;x&lt;/strong&gt;) and
   a vector of target (&lt;strong&gt;y&lt;/strong&gt;). You should return a vector of beta coefficients 
   that represent the line of best fit which minimizes the residual. 
   Calculate  R&lt;sup&gt;2&lt;/sup&gt;. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div align="center"&gt;
      &lt;img height="30" src="images/normal_equation.png"&gt;
   &lt;/div&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = sm.add_constant(x)
x.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;accountant&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pilot&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;architect&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;author&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;chemist&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;def solve_normal(matrix, vector):
    x = matrix.values
    y = vector.values
    return np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)

betas = solve_normal(x,y)
print &amp;quot;Constant: &amp;quot;, betas[0]
print &amp;quot;Income: &amp;quot;, betas[1]
print &amp;quot;Education: &amp;quot;, betas[2]

Constant:  -6.0646629221
Income:  0.598732821529
Education:  0.545833909401



TSS = np.sum(np.power(y.values-y.values.mean(),2))
RSS = np.sum(np.power(y.values-np.dot(x.values,betas),2))
print &amp;quot;R-Squared: &amp;quot;, 1-RSS/TSS

R-Squared:  0.828173417254
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Verify your results using statsmodels. Use the code below as a reference.&lt;/p&gt;
&lt;p&gt;import statsmodels.api as sm
model = sm.OLS(y, x).fit()
model.summary()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;        &lt;td&gt;prestige&lt;/td&gt;     &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   101.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Tue, 16 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;8.65e-17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;07:23:19&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -178.98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    45&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   364.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    42&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   369.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;     &lt;td&gt;   -6.0647&lt;/td&gt; &lt;td&gt;    4.272&lt;/td&gt; &lt;td&gt;   -1.420&lt;/td&gt; &lt;td&gt; 0.163&lt;/td&gt; &lt;td&gt;  -14.686     2.556&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    0.5987&lt;/td&gt; &lt;td&gt;    0.120&lt;/td&gt; &lt;td&gt;    5.003&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.357     0.840&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;    0.5458&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    5.555&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.348     0.744&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 1.279&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.528&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   0.520&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.155&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.426&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    163.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;ol&gt;
&lt;li&gt;Interpret your result summary, focusing on the beta coefficents, p-values, F-statistic, and the R&lt;sup&gt;2&lt;/sup&gt;. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results from the manual calculation match the values from statsmodels' OLS regression.   We have 82.8% of the varience is explained by the model.  The F-statistics, assuming independant variables, says our model does describe some of the behavior of the prestige.   The coefficients say that for every unit increase in income, we have a 0.5987 incraese in prestige, and for every unit increase in educaiton we have a 0.5458 increase in prestigue.  The issue with the model, highlighted by the p-value on the constant term, is that as educaiton increases, so does income.   They will increase together, somehow double increasing the prestigue.   We need a way to remove this co-linearity between income and educaiton&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.random.rand(20,1)*10
y = 5*x+2+np.random.rand(20,1)*30
z = 2*x+4*y+2+np.random.rand(20,1)*40
ones = np.ones(20).reshape(20,1)

X = pd.DataFrame({&amp;#39;x&amp;#39;:x[:,0],&amp;#39;y&amp;#39;:y[:,0],&amp;#39;c&amp;#39;:ones[:,0]})
Y = pd.DataFrame({&amp;#39;z&amp;#39;:z[:,0]})
model = sm.OLS(Y, X).fit()
model.summary()
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;            &lt;td&gt;z&lt;/td&gt;        &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.975&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.972&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   325.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Tue, 16 Jun 2015&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;2.79e-14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;07:40:32&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -78.012&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    20&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   162.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    17&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   165.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;td&gt;&lt;/td&gt;     &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;c&lt;/th&gt; &lt;td&gt;   18.9688&lt;/td&gt; &lt;td&gt;    9.231&lt;/td&gt; &lt;td&gt;    2.055&lt;/td&gt; &lt;td&gt; 0.056&lt;/td&gt; &lt;td&gt;   -0.507    38.445&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x&lt;/th&gt; &lt;td&gt;    2.2866&lt;/td&gt; &lt;td&gt;    2.065&lt;/td&gt; &lt;td&gt;    1.107&lt;/td&gt; &lt;td&gt; 0.284&lt;/td&gt; &lt;td&gt;   -2.071     6.644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;y&lt;/th&gt; &lt;td&gt;    4.0274&lt;/td&gt; &lt;td&gt;    0.378&lt;/td&gt; &lt;td&gt;   10.667&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.231     4.824&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.159&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.527&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.206&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   1.277&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.090&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 1.775&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    158.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We can see that if we take away the constant, removing a degree of freedom, the R-squared becomes larger.   I think this could be a sign that we have a covariance in the variables. &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="EDA"></category><category term="Linear Algebra"></category></entry><entry><title>Galvanize - Week 02 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-05/" rel="alternate"></link><updated>2015-06-12T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-12:galvanize/galvanize-data-science-02-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 5&lt;/h2&gt;
&lt;p&gt;Our more quiz was a survey about our progress in the program.  The morning lecture was about the beta distribution, and its relation to A/B testing from a Bayesian perspective, and a brief introduction to the multi-arm bandit method.   &lt;/p&gt;
&lt;h2&gt;Individual Morning Sprint&lt;/h2&gt;
&lt;p&gt;This morning we had some simulated click through data for two different sites and attempted to answer question about the outcome of the two pages.  We want to find the probability of a click through on the two pages and be able to quantify how much better one site is over the other. }&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/siteA.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/siteB.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Bayesian Analysis&lt;/h2&gt;
&lt;p&gt;We are going to start with a prior that the click through rate of both sites can be anything between 0 and 1.  This will look like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.01,0.01)
y = sc.beta(a=1.,b=1.).pdf(x)

def plot_with_fill(x,y,label,color):
    lines = plt.plot(x,y,label=label,lw=2,color=color)
    plt.fill_between(x,0,y,alpha=0.2,color=color)
    plt.ylim([0,1.2*y.max()])
    plt.legend()


plot_with_fill(x,y,&amp;#39;Beta a=1,b=1&amp;#39;,&amp;#39;seagreen&amp;#39;)
plt.xlabel(&amp;quot;Probability of Click Through&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_3_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we look at 1 value from the click through where the user does not click through, we update our believe:&lt;/p&gt;
&lt;p&gt;$$\mbox{Posterior} = \mbox{Prior} * \mbox{Likelihood}$$&lt;/p&gt;
&lt;p&gt;$$\mbox{Beta}(\alpha=1, \ \beta=2) = \mbox{Beta}(\alpha=1, \ \beta=1) \times (1 - p)$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b1,a1 = 1,0
y1 = sc.beta(a=1,b=1).pdf(x)
y2 = sc.beta(a=a1+1,b=b1+1).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;1 Views&amp;#39;,&amp;#39;lightsalmon&amp;#39;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_5_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we have 50 views, the equation would be:&lt;/p&gt;
&lt;p&gt;$$\mbox{Beta}(\alpha=1+n_{clicks}, \ \beta=1+n_{no clicks}) = \mbox{Beta}(\alpha=1, \ \beta=1) \times (1 - p)^{n_{no clicks}} \times p^{n_{clicks}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b50,a50 = np.bincount(A[:50].astype(int))
y1 = sc.beta(a=1,b=1).pdf(x)
y2 = sc.beta(a=a50+1,b=b50+1).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;50 Views&amp;#39;,&amp;#39;lightsalmon&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_7_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can imagine that we continue getting data, and our believe about the click through rate would evolve:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.001,0.001)
plt.figure(figsize=(15,10))
y1 = sc.beta(a=1.,b=1.).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
color_s = {&amp;#39;50&amp;#39;:&amp;#39;lightsalmon&amp;#39;,&amp;#39;100&amp;#39;:&amp;#39;aquamarine&amp;#39;,&amp;#39;200&amp;#39;:&amp;#39;turquoise&amp;#39;,&amp;#39;400&amp;#39;:&amp;#39;NavajoWhite&amp;#39;,&amp;#39;800&amp;#39;:&amp;#39;forestgreen&amp;#39;}
for count in [50,100,200,400,800]:
    bc,ac = np.bincount(A[:count].astype(int))
    y = sc.beta(a=ac,b=bc).pdf(x)
    plot_with_fill(x,y,str(count)+&amp;#39; Views&amp;#39;,color_s[str(count)])
plt.xlim([0,.2])




(0, 0.2)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_9_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that as we increase the amount of data we have, we have a more specific believe about the click through rate of site A.  &lt;strong&gt;This is different from a hypthesis test.  It does not required a fix sample size or fixed amount of time&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;We can look at the two sites for all the data.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.001,0.001)
plt.figure(figsize=(15,10))
bA,aA = np.bincount(A.astype(int))
bB,aB = np.bincount(B.astype(int))
y1 = sc.beta(a=float(aA),b=float(bA)).pdf(x)
y2 = sc.beta(a=float(aB),b=float(bB)).pdf(x)
plot_with_fill(x,y1,&amp;#39;A Site&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;B Site&amp;#39;,&amp;#39;aquamarine&amp;#39;)
plt.xlim([0,.2])
plt.ylim([0,50])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_11_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We now want to determine, given these distributions, what is the probability that site B is better than site A.  We can take random variables from both distributions and count the number of times that the random value from B is greater than the random variable from A.   We can do this 10,000 times.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rA = sc.beta(a=aA,b=bA).rvs(size=10000)
rB = sc.beta(a=aB,b=bB).rvs(size=10000)
nLess,nMore = np.bincount((rA&amp;lt;rB).astype(int))

plt.figure()
ps = []
for i in range(1000):
    rA = sc.beta(a=aA,b=bA).rvs(size=10000)
    rB = sc.beta(a=aB,b=bB).rvs(size=10000)
    nLess,nMore = np.bincount((rA&amp;lt;rB).astype(int))
    ps.append(nMore/10000.)

plt.hist(ps,bins=15,color=&amp;#39;aquamarine&amp;#39;,alpha=0.5)
plt.show()
print &amp;quot;Mean Prob: &amp;quot;, nMore/10000.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_13_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Mean Prob:  0.9971
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Based on our current belief based on the data, the chance that sight B is better than site A is 99.7%.  &lt;/p&gt;
&lt;p&gt;We can also estimate the Bayesian equivalant of a confidence interval.&lt;/p&gt;
&lt;p&gt;This is the centeral credible region - range of the 2.5 precentile and the 97.5 percentile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rA = sc.beta(a=aA,b=bA).rvs(size=10000)
HDI_A = (np.percentile(rA,2.5),np.percentile(rA,97.5))
rB = sc.beta(a=aB,b=bB).rvs(size=10000)
HDI_B = (np.percentile(rB,2.5),np.percentile(rB,97.5))
print &amp;quot;Site A: &amp;quot;, HDI_A
print &amp;quot;Site B: &amp;quot;, HDI_B

Site A:  (0.05002097381711422, 0.084804921163330979)
Site B:  (0.082330727908278764, 0.12425696702783418)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;An 95% highest density interval (HDI) is the most dense interval of a posterior distribution containing X% of its mass. It is analagous to frequentist analysis's confidence intervals.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def hdi_beta(data,percent=0.95):
    bD,aD = np.bincount(data.astype(int))
    x_max = sc.beta(a=aD,b=bD).ppf(1 - percent - 1e-6)
    x_high_max = sc.beta(a=aD,b=bD).ppf(percent - 1e-6)
    vals = np.linspace(0,x_max,1000)
    p_vals = 0.95+sc.beta(a=aD,b=bD).cdf(vals)
    x_high = sc.beta(a=aD,b=bD).ppf(p_vals)
    width = x_high-vals
    low = vals[np.argmin(width)]
    high = low+width.min()
    return (low,high)

print &amp;quot;HDI A:&amp;quot;, hdi_beta(A)
print &amp;quot;HDI B:&amp;quot;, hdi_beta(B)


HDI A: (0.049391799061910255, 0.083668677250890555)
HDI B: (0.081861282294831139, 0.12374716027178045)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These HDI are close to the central credibility region, but they are systematically lower.&lt;/p&gt;
&lt;p&gt;What is nicse about Baysina inference is we can ask what is the probability that site B is 2 percentage points better than A.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nLess2,nMore2 = np.bincount((rB&amp;gt;(rA+0.02)).astype(int))
print &amp;quot;Probabilty B &amp;gt; A + 0.02:  &amp;quot;, nMore2/10000.

plt.figure()
plt.hist((rB-rA),bins=30,color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.show()

Probabilty B &amp;gt; A + 0.02:   0.881
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_19_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case we can see that the difference is near zero (really 0.02), but is likely to be higher.   The probabiliy given by the simulation is 88.1%. &lt;/p&gt;
&lt;p&gt;For a sanity check we can do a hypothesis test.  We would perform a 1 sided Hypthesiss test:&lt;/p&gt;
&lt;p&gt;H0: The mean clickthrough rate is the same.&lt;br /&gt;
HA: The mean clicktrhough rate for site B is greater than site A.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mDiff = rB.mean()-rA.mean()
se = np.sqrt(rB.var()/(len(rB)-1)+rA.var()/(len(rA)-1))
Z = mDiff/se
print mDiff,se,Z
print &amp;quot;p-value: &amp;quot;,1-sc.norm.cdf(Z)
print &amp;quot;The mean difference is statistically significant&amp;quot;


0.0363429336139 0.000138288570828 262.805041634
p-value:  0.0
The mean difference is statistically significant
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now told that there is a business model for this website, and it should inform our decision to implement the new site:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;* the average click on site A yields $1.00 in profit
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the average click on site B yields $1.05 in profit  &lt;/p&gt;
&lt;p&gt;gain_per_click = (rB.sum()*1.05-rA.sum())/10000.
gain_per_click&lt;/p&gt;
&lt;p&gt;0.041471285947814275&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an average gain of 4 cents per click increase in revenume.  Roughly 3 cents from the increase in click through rate and 1.4 cent from the increase in yield.  &lt;/p&gt;
&lt;p&gt;We are not told over what time the experiment we analysed took place over.  A site with 10 Million users per month will see an increase revenue of 400,000 dollars per month, while a site of 10,000 users per month will only see an increase of of 400 dollars per month.  The business decision of investing money and time depends on the context.  This could be a great boom for the company, or a waste of time, depending on the situation.  &lt;/p&gt;
&lt;h2&gt;Multi-arm Bandit&lt;/h2&gt;
&lt;p&gt;The multi-arm bandit approach is a method of balancing the exploration of strategies against the explortation of the best strategy.  We impleted a multi-arm bandit class with a number of strategies:  Random Choice, Max Mean, Epsilon Greedy, Soft Max, UCB1, Bayesian, and Annealing.  &lt;/p&gt;
&lt;p&gt;We have two ways to measure the success of a search strategy for now.  The regret:&lt;/p&gt;
&lt;p&gt;$$\mbox{Regret} = N \ p_{optimal} - \Sigma_{i=1}^{N} p_{i} $$&lt;/p&gt;
&lt;p&gt;And the ratio of optimal decision:&lt;/p&gt;
&lt;p&gt;$$ \frac{N_{optimal}}{N}$$&lt;/p&gt;
&lt;p&gt;The bandit class takes an array of deciion options where each value is the probability of success.  The spirit of this is examining multiple metrics at the same time: like conversion rates across multiple simulatenous tests.&lt;/p&gt;
&lt;h2&gt;Max Mean&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bandits&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;banditstrategy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;prettyplotlib&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ppl&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;bandits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BanditStrategy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_choice&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_bandits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ppl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regrets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Random Choice&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;bandits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BanditStrategy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_bandits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ppl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regrets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Max Mean&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_26_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The max mean, on average, performs better than a random choice.  The issue with max mean is that its very exploitive.   When there is a statisitcal fluxtuation in makes it look like a worse option is the best option, it will stick with it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,10))
for i in range(10):
    bandits = Bandits([0.05, 0.03, 0.06])
    strat = BanditStrategy(bandits, random_choice)
    strat.sample_bandits(1000)
    ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

    bandits = Bandits([0.05, 0.03, 0.06])
    strat = BanditStrategy(bandits, max_mean)
    strat.sample_bandits(1000)
    ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)

plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_28_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This is best illustrated if we look at the percent of the time that the optimal solution is chosen.   The max-mean strategy will often find one value and stay with it.&lt;/p&gt;
&lt;p&gt;We can explore this method on three different array options:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;#39;&amp;#39;&amp;#39;
One Stand Out Best = [0.1, 0.1, 0.1, 0.1, 0.9]
One Small Best = [0.1, 0.1, 0.1, 0.1, 0.12]
A clear rank = [0.1, 0.2, 0.3, 0.4, 0.5]
&amp;#39;&amp;#39;&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lets look at one &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_multi_plots(size,func):
    values = [[0.1, 0.1, 0.1, 0.1, 0.9],[0.1, 0.1, 0.1, 0.1, 0.12],[0.1, 0.2, 0.3, 0.4, 0.5]]
    titles = [&amp;#39;One Stand Out Best&amp;#39;,&amp;#39;One Slightly Best&amp;#39;, &amp;quot;A clear rank&amp;quot;]
    for v,t in zip(values,titles): 
        plt.figure(figsize=(14,5))
        plt.subplot(1,2,1)
        for i in range(10):
            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, random_choice)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.regrets,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, func)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.regrets,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)
        plt.title(&amp;#39;Regret - &amp;#39; + t)

        plt.subplot(1,2,2)
        for i in range(10):
            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, random_choice)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, func)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)
        plt.title(&amp;#39;Percent Optimal - &amp;#39; + t)
        plt.show()

make_multi_plots(1000,max_mean)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Max-Mean appear to work well in cases where there is a single clear best option, but does not consistently find optimal results in the other options.&lt;/p&gt;
&lt;h2&gt;Epsilon Greedy&lt;/h2&gt;
&lt;p&gt;The epsilon greed strategy is a alternative to the max mean where some fraction of the time, say 10%, it will explore instead of exploit.   Well look at it for the same cases as the max mean method.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,epsilon_greedy)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This strategy has the benefit that if it does start to exploit a sub-optimal strategy, it will eventually find its way out.   This is clear in the "Slightly Best" plots where as the number of trials increase, bandits are leaving the sub-optimal strategy.  &lt;/p&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;p&gt;The soft max is another strategy that balances exploitation vs exploration by use of a parameter.   It has an anology to temperature is method of statistical mechanics.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x: softmax(x,tau=0.5)
make_multi_plots(1000,func)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The tau parameter of 0.5, in this example, seems to provide random solutions.   If we reduce the size of tau to 0.005, we should get solutions close to max mean.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x: softmax(x,tau=0.005)
make_multi_plots(1000,func)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Inbetween, the soft max is to reduce the initial effect of outliers, but on these arrays we get less then optimal soltuions.&lt;/p&gt;
&lt;h2&gt;UCB1&lt;/h2&gt;
&lt;p&gt;The UCB1 picks the strategy based on the number of trials and its success rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,ucb1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This method worked great on one standout best, but behaved randomly on the other options&lt;/p&gt;
&lt;h2&gt;Bayesian Bandit&lt;/h2&gt;
&lt;p&gt;This method uses the ration of success and failures in a trial to pick options based on the beta distribution.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,bayesian_bandit)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This method wored well on a clear best and eventually found the optimal solution for a ranked system.  The slight best behaved randomly.&lt;/p&gt;
&lt;h2&gt;Annealing&lt;/h2&gt;
&lt;p&gt;This method is working with a softmax that starts with a 'hot' system that explores randomly and cools to a 'cold' system that exploits.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x:annealing(x,discount=0.90,tau=0.5)
make_multi_plots(1000,annealing)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see evidence of the cooling because the random values are starting to move away from random, but we would need to see more trials to see the evenatual discovery of optimal solutions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;color_scheme = [(227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),(188, 189, 34),(23, 190, 207)] # (219, 219, 141)]#, (]
color_scheme = [ (x/255.,y/255.,z/255.) for x,y,z in color_scheme]

func1 = lambda x: softmax(x,tau=0.5)
strategies = [max_mean,random_choice,epsilon_greedy,func1,ucb1,bayesian_bandit]

size=10000
values = [[0.1, 0.1, 0.1, 0.1, 0.9],[0.1, 0.1, 0.1, 0.1, 0.12],[0.1, 0.2, 0.3, 0.4, 0.5]]
titles = [&amp;#39;One Stand Out Best&amp;#39;,&amp;#39;One Slightly Best&amp;#39;, &amp;quot;A clear rank&amp;quot;]
for v,t in zip(values,titles): 
    plt.figure(figsize=(14,5))
    for color,func in zip(color_scheme,strategies):
        plt.subplot(1,2,1)
        bandits = Bandits(v[:])
        strat = BanditStrategy(bandits, func)
        strat.sample_bandits(size)
        ppl.plot(range(strat.N),strat.regrets,label=func.__name__,alpha=1,color=color,lw=4)
        plt.title(&amp;#39;Regret - &amp;#39; + t)
        plt.legend(loc=2)


        plt.subplot(1,2,2)
        bandits = Bandits(v[:])
        strat = BanditStrategy(bandits, func)
        strat.sample_bandits(size)
        ppl.plot(range(strat.N),strat.percentOpt,label=func.__name__,alpha=1,color=color,lw=4)
        plt.title(&amp;#39;Percent Optimal - &amp;#39; + t)

    plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the epsilon greed strategy and bayesian bandit algorithms have the best overall performance and avoid getting stuck in non-optimal solutions.   The soft max and annealing both require turning, so they are not fairly compared.   It is nice to see that both epsilon greed and bayesian bandit solutions offer easy to implment and robust use.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="multi-arm bandit"></category></entry><entry><title>Galvanize - Week 02 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-04/" rel="alternate"></link><updated>2015-06-11T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-11:galvanize/galvanize-data-science-02-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 4&lt;/h2&gt;
&lt;p&gt;Our morning quiz was fun.  It was the first time that we built off a prevous quiz.  We were required to build a random variable class that used our probability mass function from the day before.   The end result was being able to simulate a random value from any distrubiton able to be defined by a PMF.  &lt;/p&gt;
&lt;p&gt;Our morning lecture was about power and sample size, as was our individual sprint.  The afternoon lecture was on Bayesian Inference, and the afternoon paired sprint was investigating evolving likelihood functions as we gained more information/data.  &lt;/p&gt;
&lt;h2&gt;Power&lt;/h2&gt;
&lt;p&gt;-Suppose you are interested in testing if on average a bottle of coke weighs 20.4 ounces. You have collected
simple random samples of 130 bottles of coke and weighed them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/coke_weights.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="mi"&gt;130&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;State your null and alternative hypothesis.&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;H0: The mean weight of coke bottles is 20.4 oz&lt;/strong&gt;&lt;br /&gt;
2.&lt;strong&gt;HA: The mean weight of coke bottles is different form 20.4 oz&lt;/strong&gt;     &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the mean and standard error of the sample. State why you are able to apply the Central
   Limit Theorem here to approximate the sample distribution to be normal.&lt;/p&gt;
&lt;p&gt;mean = data.mean()
std = data.std()
se = sc.sem(data)
print "Sample Mean: ", mean
print "Sample STD", std
print "Sample Standard Error",se
print "Sample Size: ", len(data)&lt;/p&gt;
&lt;p&gt;Sample Mean:  20.519861441
Sample STD 0.957682215104
Sample Standard Error 0.084319217426
Sample Size:  130&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;We can use the CLT on this problem because the sample size is 130, much greater than 30.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can make a simulation of the sampling distribution of the null hypthosis, and we can make a sampling distirubiton for another believe, say the true value is the sample man.   If this is true, we can ask questions about how powerful our test is at discovering the mean coke bottle weight is not 20.4, but 20.52.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def power_graph(mu1,std1,n1,mu2,std2,n2,alpha=0.05,two_sided=True):
    x = np.array([sc.norm.rvs(loc=mu1,scale=std1,size=n1).mean() for i in range(10000)])
    y = np.array([sc.norm.rvs(loc=mu2,scale=std2,size=n2).mean() for i in range(10000)])
    plt.figure()
    plt.hist(x,normed=True,color=&amp;#39;lightseagreen&amp;#39;,edgecolor=&amp;#39;lightseagreen&amp;#39;,alpha=0.4,bins=30,label=&amp;quot;Null&amp;quot;)
    plt.hist(y,normed=True,color=&amp;#39;lightsalmon&amp;#39;,edgecolor=&amp;#39;lightsalmon&amp;#39;,alpha=0.4,bins=30,label=&amp;quot;W=20.52&amp;quot;)
    if two_sided:
        x95 = np.percentile(x,100-100*alpha/2.)
    else:
        x95 = np.percentile(x,100-100*alpha)

    plt.axvline(x95,0,10,color=&amp;#39;gray&amp;#39;,lw=2,linestyle=&amp;#39;--&amp;#39;,alpha=0.8)
    plt.legend()
    plt.show()
    print &amp;quot;Value Threshold For Significance: &amp;quot;, x95
    power = len(y[y&amp;gt;x95])/len(y)
    print &amp;quot;Power of Finding True Positive: &amp;quot;,power
    return power

power_graph(20.4,data.std(),130,data.mean(),data.std(),130)
power_graph(20.4,data.std(),130,data.mean(),data.std(),130,two_sided=False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_6_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5657024198
Power of Finding True Positive:  0.2899
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_6_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5387014434
Power of Finding True Positive:  0.4037





0.4037
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These powers are 30% for the two sided, and 42% for the onsided.   It deends if our originaly hypothesis test is a not equal or greater than.   &lt;/p&gt;
&lt;p&gt;We can also do this analytically since we are using the centeral limit.  For a two sided test, we find the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(20.4-4*se,20.4+4*se,100)

y1 = sc.norm.pdf(x,loc=20.4,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.4,scale=se)
y2 = sc.norm.pdf(x,loc=data.mean(),scale=se)
x975 = 20.4+1.96*se
x025 = 20.4-1.95*se
print x025,x975
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.show()

20.235577526 20.5652656662
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_8_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can see that we would not reject the null hypothesis in favor of the alternative because the peak of the blue curve is to the left of the bounding of significance.  The area under the red cuver outside of the black boundaries is 0.05, the signifcance level.   The power of detecing a signficant difference assuming the data's mean is the true value is the blue area.   In this case it is ~ 30%&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The probability of making a type II error (false negative) is called beta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = sc.norm.cdf(x975,loc=data.mean(),scale=se)-sc.norm.cdf(x025,loc=data.mean(),scale=se)
print &amp;quot;Beta (Prob of Type II Error) Assuming data value is the true value&amp;quot;, beta

Beta (Prob of Type II Error) Assuming data value is the true value 0.704503425135
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The power is always 1 minus the false negative rate:&lt;/p&gt;
&lt;p&gt;$$\mbox{Power} = 1 - \beta$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Power: &amp;quot;,1-beta

Power:  0.295496574865
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Statistical power is affected by a number of factors, including the &lt;strong&gt;sample size&lt;/strong&gt;, the &lt;strong&gt;effect size (difference
between the sample statistic and the statistic formulated under the null)&lt;/strong&gt;, and the &lt;strong&gt;significance level&lt;/strong&gt;. Here
we are going to explore the effect of these factors on power.&lt;/p&gt;
&lt;p&gt;If we assuming that we have a different null hypothesis, we can find the power of detecting anther effect size.  Lets stick with the sample data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def explore_power(null_mu,sample_size,effect_size,null_standard_dev,significance_level=0.95,two_sided=True):
    if two_sided:
        critical_z = sc.norm.isf((1-significance_level)/2.)
        se = np.sqrt(null_standard_dev**2/(sample_size-1))
        x025 = null_mu-critical_z*se
        x975 = null_mu+critical_z*se
        alt_mean = null_mu+effect_size
        return (1-np.abs(sc.norm.cdf(x975,loc=alt_mean,scale=se)-sc.norm.cdf(x025,loc=alt_mean,scale=se)))*100
    else:
        critical_z = sc.norm.isf(significance_level)
        se = np.sqrt(null_standard_dev**2/(sample_size-1))
        x95 = null_mu-critical_z*se
        alt_mean = null_mu+effect_size
        return 100*(np.abs(sc.norm.cdf(x95,loc=alt_mean,scale=se)))

print explore_power(20.4,130,data.mean()-20.4,data.std())
explore_power(20.2,130,data.mean()-20.2,data.std())

29.5495708063





96.663546292685481
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The power increased when we assumed the distributions were more different.  This makes sense because the two values are father appart, and the sampling distributions have less overlap.  We can see that by calling power_graph, or calling the analyical functions because we have access to the central limit theorem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;power_graph(20.2,data.std(),130,data.mean(),data.std(),130)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_17_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.364830609
Power of Finding True Positive:  0.9698





0.9698




x = np.linspace(20.2-5*se,20.2+6*se,100)

y1 = sc.norm.pdf(x,loc=20.2,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.2,scale=se)
y2 = sc.norm.pdf(x,loc=data.mean(),scale=se)
x975 = 20.2+1.96*se
x025 = 20.2-1.95*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_18_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This makes me wonder how the power changes with the effect size.  I image that bigger effects are easier to detect.  It makes good sense&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.plot(np.linspace(0.1,1.2,20),[explore_power(20.2,130,x,data.std()) for x in np.linspace(0.0,1.2,20)],&amp;#39;bo--&amp;#39;,
        color=&amp;#39;lightseagreen&amp;#39;,alpha=0.8,label=&amp;quot;Power&amp;quot;)
plt.ylim([0,110])
plt.xlabel(&amp;quot;Effect Size&amp;quot;)
plt.ylabel(&amp;quot;Power&amp;quot;)
plt.legend(loc=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_20_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The power of a study also depends on the sample size.  As the sample size increases, the standard error decreases by the central limit theorem.   This will make two different distributions overlap less. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;power_graph(20.4,data.std(),130,data.mean(),data.std(),130)
power_graph(20.4,data.std(),1000,data.mean(),data.std(),1000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_22_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5657528314
Power of Finding True Positive:  0.2857
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_22_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.4590858498
Power of Finding True Positive:  0.9802





0.9802




data2 = np.loadtxt(&amp;#39;data/coke_weights_1000.txt&amp;#39;)
mean = data2.mean()
std = data2.std()
se = sc.sem(data2)
x = np.linspace(20.4-5*se,20.4+6*se,100)

y1 = sc.norm.pdf(x,loc=20.4,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.4,scale=se)
y2 = sc.norm.pdf(x,loc=data2.mean(),scale=se)
x975 = 20.4+1.96*se
x025 = 20.4-1.95*se

plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
beta = sc.norm.cdf(x975,loc=data2.mean(),scale=se)-sc.norm.cdf(x025,loc=data2.mean(),scale=se)
print &amp;quot;Beta (Prob of Type II Error) Assuming data value is the true value&amp;quot;, beta
print &amp;quot;Power:&amp;quot;,1-beta
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_23_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Beta (Prob of Type II Error) Assuming data value is the true value 0.271504963539
Power: 0.728495036461
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can also see how chaning the significance level affects the power of a study.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.plot(np.linspace(0.01,0.3,40),[explore_power(20.4,130,0.01,data.std(),significance_level=x) for x in np.linspace(0.01,0.3,40)],&amp;#39;bo--&amp;#39;,
        color=&amp;#39;lightseagreen&amp;#39;,alpha=0.6,label=&amp;quot;Power&amp;quot;)
plt.ylim([0,110])
plt.xlabel(&amp;quot;Significance Level&amp;quot;)
plt.ylabel(&amp;quot;Power&amp;quot;)
plt.legend(loc=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_25_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Power Calculations for A/B testing&lt;/h2&gt;
&lt;p&gt;We continued yesterday's case study with Esty to find the power needed.  It looked like our Etsy Tuesday Landing Page experiment was under-powered.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;etsy = pd.read_csv(&amp;#39;data/experiment.csv&amp;#39;)
old_data = etsy[etsy[&amp;#39;landing_page&amp;#39;] == &amp;#39;old_page&amp;#39;][&amp;#39;converted&amp;#39;]
new_data = etsy[etsy[&amp;#39;landing_page&amp;#39;] == &amp;#39;new_page&amp;#39;][&amp;#39;converted&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will set up the following hypthesis test.&lt;/p&gt;
&lt;p&gt;X ~ p_new - p_old&lt;/p&gt;
&lt;p&gt;H0: X = 0.001
H1: X &amp;gt; 0.001&lt;/p&gt;
&lt;p&gt;We need to set up the proportions of conversions and find the standard error for this experiment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;po = old_data.mean()
pn = new_data.mean()
p = (old_data.mean()*len(old_data)+new_data.mean()*len(new_data))/(len(old_data)+len(new_data))
se = np.sqrt(p*(1-p)/len(old_data)+p*(1-p)/len(new_data))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can make the same plots as before and compared null to our data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(-5*se,5*se,100)

y1 = sc.norm.pdf(x,loc=0.001,scale=se)
cumy1 = sc.norm.cdf(x,loc=0.001,scale=se)
y2 = sc.norm.pdf(x,loc=(pn-po),scale=se)
x95 = 0.001+1.68*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;New Data&amp;#39;)
plt.axvline(x95,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x95],y2[x&amp;gt;=x95],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x95],y1[x&amp;gt;=x95],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see our data is not statistically different of 0.001, but if we assume the data represents the true mean, we have a very weak test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;critical_z = sc.norm.ppf(0.95)
x95 = 0.001+critical_z*se
1-sc.norm.cdf(x95,loc=(pn-po),scale=se)




0.005391084734824525
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We have a power of less than 1/2% from our data.   &lt;/p&gt;
&lt;p&gt;Increasing the sample size will weaken the power in this case, because our data is less than the null hypthesis, and we are constructing a one sided t-test.  In this case we accept the results and fail to reject the null hypthesis. &lt;/p&gt;
&lt;p&gt;We were told that Etsy decided the pilot is a plausible enough representation of the company's daily  traffic. As a result, Esty decided on a two-tailed test instead, which is as follows:&lt;/p&gt;
&lt;p&gt;```
   X ~ p_new - p_old&lt;/p&gt;
&lt;p&gt;H0: X = 0.001
   H1: X != 0.001
   ```&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(-5*se,5*se,100)

y1 = sc.norm.pdf(x,loc=0.001,scale=se)
cumy1 = sc.norm.cdf(x,loc=0.001,scale=se)
y2 = sc.norm.pdf(x,loc=(pn-po),scale=se)
x025 = 0.001-1.96*se
x975 = 0.001+1.96*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;New Data&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y2[x&amp;lt;=x025],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_36_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;2-Sided Power: &amp;quot;, sc.norm.cdf(x025,loc=(pn-po),scale=se)+(1-sc.norm.cdf(x975,loc=(pn-po),scale=se))

2-Sided Power:  0.14775925242
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Minimum Sample Size&lt;/h3&gt;
&lt;p&gt;When desigining an experiment, or conducting a test, it is important to get enough data to measure what we are looking for.  In the case of Etsy, its a 1% lift in conversions.  To try to figure out the sample size we might considered our desired false postive and false negative rates, and the Z values associated with them.&lt;/p&gt;
&lt;p&gt;$$\alpha = \mbox{False Positive Rate}, \ Z_{\alpha}$$&lt;/p&gt;
&lt;p&gt;$$\beta = \mbox{False Negative Rate}, \ Z_{\beta}$$&lt;/p&gt;
&lt;p&gt;In an experiment we will have one values, and the Z's will be related to it by the following:&lt;/p&gt;
&lt;p&gt;$$\mu_{exp} = \mbox{Experimental Result}$$&lt;/p&gt;
&lt;p&gt;$$s_{sample} = \mbox{Sampling Erroring}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} = \frac{\mu_{exp} \ - \ \mu_{Null}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\beta} = \frac{\mu_{exp} \ - \ \mu_{Alternative}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;It is worth noting that one of these Z's can be positive, while the other can be negative.  You will find equations that differ from what is done here because of that.  Taking the difference between these two equations give:&lt;/p&gt;
&lt;p&gt;$$\mbox{Effect Size} = \mu_{Alternative} - \mu_{Null}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} \ - \ Z_{\beta} = \frac{\mbox{Effect Size}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;We can use the eqauation for the sampling distribution for two sample proproption test of equal size for the sampling error.&lt;/p&gt;
&lt;p&gt;$$s_{sample} = \sqrt{\frac{ p \ (1 - p) \ 2}{n}}$$&lt;/p&gt;
&lt;p&gt;Where p is the weighted proportion of the two groups.  This gives the previous equation as.&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} \ - \ Z_{\beta} = \frac{\mbox{Effect Size}}{\sqrt{\frac{ p \ (1 - p) \ 2}{n}}}$$&lt;/p&gt;
&lt;p&gt;Squaring both sides we get:&lt;/p&gt;
&lt;p&gt;$$(Z_{\alpha} \ - \ Z_{\beta})^2 = \frac{\mbox{Effect Size}^2 \ n}{ p \ (1 - p) \ 2}$$&lt;/p&gt;
&lt;p&gt;So the needed sample size should be&lt;/p&gt;
&lt;p&gt;$$n = \frac{2 \ (Z_{\alpha} \ - \ Z_{\beta})^2 \ p \ (1-p)}{\mbox{Effect Size}^2}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def calc_min_sample_size(old,new,effect_size,sig=0.05,pow=0.8,one_tail=False):
    pn = new.mean()
    po = old.mean()
    no = len(old)
    nn = len(new)
    dp = pn-po
    p = ( po*no+pn*nn ) / (nn+no)
    se = np.sqrt(p*(1-p)*(1/nn+1/no))
    if one_tail:
        z_null = sc.norm.ppf((1-sig))
    else:
        z_null = sc.norm.ppf((1-sig/2))
    z_pow = sc.norm.ppf(1-pow)


    return (z_null-z_pow)**2*2*p*(1-p)/effect_size**2


calc_min_sample_size(old_data,new_data,0.001,one_tail=False)




1410314.3210247809
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So if we want a power of 80% for our Etsy experiment, meaning its likely for us to detect results that are different from a lift of 1%, we need to have 1.4 million users in each sample.  Our results are definately underpowered.&lt;/p&gt;
&lt;h2&gt;Afternoon - Bayes&lt;/h2&gt;
&lt;p&gt;We covered Bayes' Theorem and updating priors with likelihoods to produce a posterior distribution of sample paramters given data.   We started with a created class, and explore the results of these distributions given data.&lt;/p&gt;
&lt;p&gt;$$ P(Parameters \ | \ Data) = \frac{P(Data \ | \ Parameters) * P(Paramters)}{P(Data)} $$&lt;/p&gt;
&lt;p&gt;$$ \mbox{Posterior} = \frac{\mbox{Likelihood} \ \times \ \mbox{Prior}}{\mbox{Normalization}} $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;Bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;object&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;    INPUT:&lt;/span&gt;
&lt;span class="s1"&gt;        prior (dict): key is the value (e.g. 4-sided die),&lt;/span&gt;
&lt;span class="s1"&gt;                      value is the probability&lt;/span&gt;

&lt;span class="s1"&gt;        likelihood_func (function): takes a new piece of data and the value and&lt;/span&gt;
&lt;span class="s1"&gt;                                    outputs the likelihood of getting that data&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;likelihood_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;prior&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;likelihood_func&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;likelihood_func&lt;/span&gt;


    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        INPUT: None&lt;/span&gt;
&lt;span class="s1"&gt;        OUTPUT: None&lt;/span&gt;

&lt;span class="s1"&gt;        Makes the sum of the probabilities equal 1.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="nx"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;total&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        INPUT:&lt;/span&gt;
&lt;span class="s1"&gt;            data (int or str): A single observation (data point)&lt;/span&gt;

&lt;span class="s1"&gt;        OUTPUT: None&lt;/span&gt;

&lt;span class="s1"&gt;        Conduct a bayesian update. Multiply the prior by the likelihood and&lt;/span&gt;
&lt;span class="s1"&gt;        make this the new prior.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;likelihood_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;print_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        Print the current posterior probability.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        Plot the current prior.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;color&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;color&lt;/span&gt;
        &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="nx"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self.&lt;/span&gt;&lt;span class="nx-Member"&gt;prior&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;likelihood_dice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;likelihood_coin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;H&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;A box contains a 4-sided die, a 6-sided die, an 8-sided die,a 12-sided die, and a 20-sided die. A die is selected at random, and the rest are destroyed.  &lt;/p&gt;
&lt;p&gt;What is the prior?&lt;br /&gt;
&lt;strong&gt;All Dice Equally Likely&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Say I roll an 8. After one bayesian update, what is the probability that I chose each of the dice?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b.update(8)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We know that we do not have a 4 or 6 sided dice, and the 8 is most likly because the 8 has a 1 in 8 chance of getting an 8, the 12 has a 1 in 12 chance of rolling and 8, and the 20 sided dice has a 1 in 20 chance of rolling an 8.&lt;/p&gt;
&lt;p&gt;Comment on the difference in the posteriors if I had rolled the die 50 times instead of 1.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[b.update(8) for i in range(49)]&lt;/span&gt;
&lt;span class="err"&gt;b.plot()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_49_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;However unlikely it is, the posterier suggest that the post likely culperite of 50 roles all coming to 8 is an 8 sided dice.&lt;/p&gt;
&lt;p&gt;Which one of these two sets of data gives you a more certain posterior and why?
&lt;code&gt;[1, 1, 1, 3, 1, 2]&lt;/code&gt; or &lt;code&gt;[10, 10, 10, 10, 8, 8]&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in [1,1,1,3,1,2]]
b.plot()
plt.show()
b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in [10,10,10,10,8,8]]
b.plot()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_52_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_52_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We are most certain in the second case becasue 3 of the dice have been ruled out, and a 1/12 per roll is much bigger than a 1/20.&lt;/p&gt;
&lt;p&gt;Say the prior of the dice is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;```
4-sided die: 8%
6-sided die: 12%
8-sided die: 16%
12-sided die: 24%
20-sided die: 40%
```
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What are posteriors for each die after rolling the 8?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.08,&amp;#39;06&amp;#39;:0.12,&amp;#39;08&amp;#39;:0.16,&amp;#39;12&amp;#39;:0.24,&amp;#39;20&amp;#39;:0.40},likelihood_dice)
b.update(8)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_55_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The postior makes 8 sided, 12 sided, and 20 sided dice equally likely after 1 roll of an 8.  &lt;/p&gt;
&lt;p&gt;Say you keep the same prior and you roll the die 50 times and get values 1-8 every time. What would you expect of the posterior? How different do you think it would be if you'd used the uniform prior?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in np.random.randint(1,8,size=50)]
b.plot()
plt.show()
b = Bayes({&amp;#39;04&amp;#39;:0.08,&amp;#39;06&amp;#39;:0.12,&amp;#39;08&amp;#39;:0.16,&amp;#39;12&amp;#39;:0.24,&amp;#39;20&amp;#39;:0.40},likelihood_dice)
[b.update(x) for x in np.random.randint(1,8,size=50)]
b.plot()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_58_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_58_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;With enough data, the difference in priors do not matter.   They converge to the same value.  &lt;/p&gt;
&lt;h2&gt;Bayes and Coin Flips&lt;/h2&gt;
&lt;p&gt;We can consider a random flip of a coin and ask what is the believe about our belief of the probability the coin resulting in heads.   Before any flip we would assume a unifor distrubiton.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p = np.linspace(0,0.99,100)
prior = dict()
for v in p:
    prior[str(v)] = 0.01
flips = [[&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;]]

plt.figure(figsize=(15,10))
for i,f in enumerate(flips):
    plt.subplot(4,2,i+1)
    b = Bayes(prior.copy(),likelihood_coin)
    for x in f:
         b.update(x)
    b.plot(label=str(f))
    plt.legend(loc=&amp;#39;best&amp;#39;)
    plt.xticks(rotation=70,fontsize=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_61_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We were given a cone class, with a unknown probability.  I am going to plot the update in my posteriers after fixed number of flips to see if a Bayesian would believe this is a biased coin&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;coin&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Coin&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Coin&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;10&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;coral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;50&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lightseagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;250&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;skyblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;500&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;purple&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;likelihood_coin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;249&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;499&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;999&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; Flips&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;70&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;As we update our believes, upto 1000 flips, we see that 50 looks to be in the edge of our posterior space, but the distribution is centered around 0.53, making this the maximum likelihood value after 1000 flips.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="statistics"></category><category term="hypothesis testing"></category></entry><entry><title>Galvanize - Week 02 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-03/" rel="alternate"></link><updated>2015-06-10T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-10:galvanize/galvanize-data-science-02-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 3&lt;/h2&gt;
&lt;p&gt;Today we had a miniquiz on making a python package that could contain an arbitrary probability mass function as a dictionary.  It had to allow new values to be set, maintain a normalized pmf, and return probabilities for values in the dictionary, None otherwise.   &lt;/p&gt;
&lt;h2&gt;Morning&lt;/h2&gt;
&lt;p&gt;The morning lecture was on Hypthesis testing and multiple testing corrections.  We reviewed the languaged, phrasing, caveots, and particulars about the frame works.  Our sprint involved investigating some questions involving multiple testing and clickthru rates&lt;/p&gt;
&lt;h2&gt;Multiple Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A study attempted to measure the influence of patients' astrological signs on their risk for heart failure.
   12 groups of patients (1 group for each astrological sign) were reviewed and the incidence of heart failure in each group was recorded. For each of the 12 groups, the researchers performed a z-test comparing the incidence of heart failure in one group to the incidence among the patients of all the other groups (i.e. 12 tests). The group with the highest rate of heart failure was Pisces, which had a p-value of .026 when assessing the null hypothesis that it had the same heart failure rate as all the other groups. What is the the problem with concluding from this p-value that Pisces have a higher rate of heart failure at a significance level of 0.05? How might you correct this p-value?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We have 12 tests, with a 5% false positive rate.  Using the Bernoulli Distribution we can see the chance of getting x number of false positive.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;


&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Probability of False Positive Count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability Of False Positives For 12 Tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 false postiive with 12 tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 or More false positives with 12 test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;postiive&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="n"&gt;tests&lt;/span&gt; &lt;span class="mf"&gt;0.341280055366&lt;/span&gt;
&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;More&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;positives&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="kp"&gt;test&lt;/span&gt; &lt;span class="mf"&gt;0.459639912337&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_1_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is a 34% chance that one of the 12 tests will be a false positive value.  Instead, it would make more sense to chose a false positive rate such that the chance of having false positives out of 12 tests would be less than 0.05&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p = np.linspace(0,1,10000)
cdf = 1-sc.binom.pmf(0,12,p)
plt.plot(p,cdf,label=&amp;quot;CDF of P&amp;quot;,color=&amp;#39;indianred&amp;#39;)
plt.xlabel(&amp;#39;False Positive Rate For 12 Tests&amp;#39;)
plt.ylabel(&amp;#39;Prob of 1+ False Positives&amp;#39;)
plt.fill_between(p, cdf, where=cdf&amp;lt;0.05, interpolate=True, color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.show()
print &amp;quot;New Significant Level: &amp;quot;, p[cdf &amp;lt; 0.05].max()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_3_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;New Significant Level:  0.004200420042
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;In this case we would want to have a false positive rate for a single test of 0.0042.  By this criteria the p-value of 0.026 is not significant for the relation between Picese and heart failure. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Bonferonni correct suggests taking the significant goal of an individual test and divide it by the number of tests.  This gives a value of 0.000417.  This is very close to the above results&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Significance: &amp;quot;, 0.05/12

Significance:  0.00416666666667
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Click Through Rate&lt;/h2&gt;
&lt;p&gt;We will use hypothesis testing to analyze &lt;strong&gt;Click Through Rate (CTR)&lt;/strong&gt; on the New York Times website.
CTR is defined as the number of clicks the user make per impression that is made upon the user.
We are going to determine if there is statistically significant difference between the mean CTR for
the following groups:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Signed in users v.s. Not signed in users&lt;/li&gt;
&lt;li&gt;Male v.s. Female&lt;/li&gt;
&lt;li&gt;Each of 7 age groups against each other (7 choose 2 = 21 tests)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Because we are construction 23 hypothesis tests on this data set, we can use the Bonferroni Correction of dividing the 5% false error rate by 23 to get:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$\alpha_{23} = 0.00217 $$&lt;/p&gt;
&lt;p&gt;We can now load the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt = pd.read_csv(&amp;quot;../ab-testing/data/nyt1.csv&amp;quot;)
nyt.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 458441 entries, 0 to 458440
Data columns (total 5 columns):
Age            458441 non-null int64
Gender         458441 non-null int64
Impressions    458441 non-null int64
Clicks         458441 non-null int64
Signed_In      458441 non-null int64
dtypes: int64(5)
memory usage: 21.0 MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are not any null values in the data set, but we need to construct a click through rate for each user.   We will do this by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Removing users without Impressions&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dividing the Clicks by Impressions&lt;/p&gt;
&lt;p&gt;nyt = nyt[nyt.Impressions!=0]
nyt['CTR'] = np.nan
nyt.CTR = nyt.Clicks.astype(float)/nyt2.Impressions.astype(float)
nyt.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Impressions&lt;/th&gt;
      &lt;th&gt;Clicks&lt;/th&gt;
      &lt;th&gt;Signed_In&lt;/th&gt;
      &lt;th&gt;CTR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have the click through lets look at the distributions of variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt2.hist(figsize=(15,8),bins=30,color=&amp;#39;indianred&amp;#39;,alpha=0.5)




array([[&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088e9f10&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088d9a50&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x108944150&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x104598750&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x10882f990&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088cafd0&amp;gt;]], dtype=object)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_11_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The age's with zero appear to be users that are not signed in by the height of the bars in the age and signed_in graphs.   The number of total clicks is less then 50,000, and the click through rates are small are mostly zero. &lt;/p&gt;
&lt;p&gt;To answer question 1, we need to split the date between signed-in users and signed-out users.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==1]
nosign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==0]
plt.figure()
sign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
nosign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed32690&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_13_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x108bbc950&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_13_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can construct a hypothesis test on the data where we have the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H0: The mean click through rate between signed in users and signed-out users are the same&lt;/li&gt;
&lt;li&gt;HA: The mean click through rates between signed-in users and signed-out users aer different&lt;/li&gt;
&lt;li&gt;We requrire a p-value less than 0.00217 to rejec the Null Hypothesis in favor for the Alterative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do this with a Weltch's (Non Equal Variance Assigned) t-test results in the following results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign.CTR, nosign.CTR, equal_var=False)




(array(-55.37611793427461), 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The results of the test states that the test statistic to measure the difference is -55, given a p-value very close to zero, which is less than 0.00217.   In this case we can see there is a mean difference in the click through rates between the two populations.  In this case the signed-out users have a higher CTR than signed-in users.&lt;/p&gt;
&lt;p&gt;Question two has to do with the difference in click through rates between genders.   This requires that we only investigate the users that are signed in, because signed out users do not have a gender variable.   Lets look at the data and perform a Wetch's t-test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
sign[sign.Gender==0].hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
sign[sign.Gender==1].hist(figsize=(15,8),bins=10,color=&amp;#39;blue&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed3a550&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_17_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x109a90610&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_17_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;A visual inspect of the data shows that men and women have similar distirubtions, with the exceptions fo the impressions distributions.  Men seem to have a wider range of impressons compared to women.  &lt;/p&gt;
&lt;p&gt;The hypthesis test we are constructing is:&lt;/p&gt;
&lt;p&gt;H0:  Signed-in Men and Women have the same mean CTR&lt;br /&gt;
HA:  Signed-in Men and Women have different mean CTR&lt;br /&gt;
&lt;em&gt;significance is 0.00217&lt;/em&gt;  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign[sign.Gender==0].CTR, sign[sign.Gender==1].CTR, equal_var=False)




(array(3.2897560659373846), 0.0010028527313066396)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our test possted a t-value of 3.29 and a p-value of 0.001, which is significant.  We reject the null in favor of the alternative, concluding that men and women have different mean click through rates.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Click through rates (SI Men, SI Women, NOT Si)&amp;quot;,sign[sign.Gender==1].CTR.mean(),sign[sign.Gender==1].CTR.mean(),nosign.CTR.mean()

Click through rates (SI Men, SI Women, NOT Si) 0.0139185242976 0.0139185242976 0.0283549070617
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The difference in the mean CTR between men and women is significant, but not large. The difference between both groups and not-signed-in users are much more different.&lt;/p&gt;
&lt;p&gt;Now we will construct a set of hypothesis tests comparing different age groups against each other.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign[&amp;#39;AgeGroup&amp;#39;] = np.nan
sign.loc[:,&amp;#39;AgeGroup&amp;#39;] = pd.cut(sign.Age, [0,7, 18, 24, 34, 44, 54, 64, 1000])
df = pd.DataFrame(columns=[&amp;#39;group1&amp;#39;,&amp;#39;group2&amp;#39;,&amp;#39;meanCTR1&amp;#39;,&amp;#39;meanCTR2&amp;#39;,&amp;#39;p_value&amp;#39;,&amp;#39;mean_difference&amp;#39;])
k = 0
AG = sign.AgeGroup.unique()
for i,x in enumerate(AG):
    for j,y in enumerate(AG):
        if x != &amp;#39;(0, 7]&amp;#39; and y!=&amp;#39;(0, 7]&amp;#39; and i &amp;gt; j:
            g1 = sign[sign.AgeGroup==x].CTR
            g2 = sign[sign.AgeGroup==y].CTR
            p = sc.ttest_ind(g1,g2, equal_var=False)[1]
            diff = g1.mean()-g2.mean()
            df.loc[k] = [x,y,g1.mean(),g2.mean(),p,diff]
            k += 1

df = df[df.p_value &amp;lt; 0.00217]
df = df.sort(&amp;#39;mean_difference&amp;#39;).reset_index().drop(&amp;#39;index&amp;#39;,axis=1)
df

/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  if __name__ == &amp;#39;__main__&amp;#39;:
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group1&lt;/th&gt;
      &lt;th&gt;group2&lt;/th&gt;
      &lt;th&gt;meanCTR1&lt;/th&gt;
      &lt;th&gt;meanCTR2&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
      &lt;th&gt;mean_difference&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;2.458627e-272&lt;/td&gt;
      &lt;td&gt;-0.020082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;1.430923e-295&lt;/td&gt;
      &lt;td&gt;-0.019845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;7.860398e-285&lt;/td&gt;
      &lt;td&gt;-0.019656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;6.900980e-144&lt;/td&gt;
      &lt;td&gt;-0.016865&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;9.214903e-56&lt;/td&gt;
      &lt;td&gt;-0.009496&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;8.273993e-20&lt;/td&gt;
      &lt;td&gt;-0.006278&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;3.563408e-05&lt;/td&gt;
      &lt;td&gt;-0.003218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;7.523228e-144&lt;/td&gt;
      &lt;td&gt;0.010020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;5.668132e-141&lt;/td&gt;
      &lt;td&gt;0.010160&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;2.525271e-151&lt;/td&gt;
      &lt;td&gt;0.010349&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;1.007813e-130&lt;/td&gt;
      &lt;td&gt;0.010586&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;4.575147e-146&lt;/td&gt;
      &lt;td&gt;0.016299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;7.449266e-146&lt;/td&gt;
      &lt;td&gt;0.016439&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;4.014382e-151&lt;/td&gt;
      &lt;td&gt;0.016628&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;5.245541e-288&lt;/td&gt;
      &lt;td&gt;0.019516&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We itereated through all 21 groups and found 14 pairs who's mean CTR are different from each other with a signicance less than 0.00217.   These results can be summarized in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;People older than 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 54 adn 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 7 and 18 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The order of CTR seems to be older than 65, 7-18 year olds, and then 54-64 year olds.  &lt;/p&gt;
&lt;p&gt;Its interest that the high click through rate of non-signed-in users matches those of the older groups.   &lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;In the afternoon we learned about AB testing, and our paired programming assignment had to do with analyzing the results of simulated data for &lt;a href="http://etsy.com"&gt;Etsy&lt;/a&gt;.  We were asked to analysis the data from a given Tuesday where an AB test was performed on a website between two pages.   The goal is to change the conversion rate from 10% to 10.1% with this new page, a lift of 1%.  We are told that the weekend users are different than weeday users, and most of Etsy's revenue is made on the weekend.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df = pd.read_csv(&amp;#39;../ab-testing/data/experiment.csv&amp;#39;)
df.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4040615247&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4365389205&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4256174578&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8122359922&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6077269891&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
print dfa.user_id.nunique(),dfa.user_id.count()
print dfb.user_id.nunique(),dfb.user_id.count()
print dfc.user_id.nunique(),dfc.user_id.count()
print dfd.user_id.nunique(),dfd.user_id.count()

95574 95574
90814 90815
0 0
4759 4759
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Right away we have a problem in the data.  We have 4759 users that are in treatment group that have seen the new page and old page.  There is also one person in the control group that appear's twice.  &lt;/p&gt;
&lt;p&gt;In my minds, a good experiment outlines how to handle missing data, mistakes, and errors in the analysis before they a found.  This is not the case for this assignment, so we will have to decided what to do ourselves.  I am going to exam the 4759 users.  First I will get a datetime in the dataframe, and then I will try to figure out what is happening with this group.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df[&amp;#39;dt&amp;#39;] = np.nan
df[&amp;#39;dt&amp;#39;] = pd.to_datetime(df.ts,unit=&amp;#39;s&amp;#39;)
dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)

dfd.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189992&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084275&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151793&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066970&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114751&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050318&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99066&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043251&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104055&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045528&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa[dfa.user_id.isin(dfd.user_id)].head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189991&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084274&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151790&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066969&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114746&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050317&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99064&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043250&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104052&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045527&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;In these 5 cases we see that the users saw the new landing page, then one second later saw the old landing page.  None of these users displayed converted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print dfd[dfd.converted==1].user_id.count()
dfd[dfd.converted==1].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100082&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043708&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67975&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029144&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42109&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017491&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22040&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008350&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87355&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037889&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:09&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].user_id.count()
dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100079&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043707&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67973&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029143&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42108&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017490&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22038&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008349&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87352&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037888&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We have 501 users that saw the new page, then 1 second later saw the old page and converted.  This does not make sense interms of our project/assignment, so I think the safest thing to do for this analysis is throw out these mistakes, and only do the analysis with the untainted results.  If this was a real experiment, I would definatley investigate the details of the test.&lt;/p&gt;
&lt;p&gt;The goal of the experiment is to have a new pages that has a conversion lift of 1 percent.   With that goal in mine we define the following test:&lt;/p&gt;
&lt;p&gt;H0:  The lift in conversions from the new page and old page is equal to 1%
HA:  the lift if conversions from the new page to the old page is less than 1%&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def z_test(old_conversion, new_conversion, old_nrow, new_nrow,
           effect_size=0., two_tailed=True, alpha=.05):
    &amp;quot;&amp;quot;&amp;quot;z-test&amp;quot;&amp;quot;&amp;quot;
    conversion = (old_conversion * old_nrow + new_conversion * new_nrow) / \
                 (old_nrow + new_nrow)

    se = np.sqrt(conversion * (1 - conversion) * (1 / old_nrow + 1 / new_nrow))

    z_score = (new_conversion - old_conversion - effect_size) / se

    if not two_tailed:
        p_val = 1 - sc.norm.cdf(abs(z_score))
    else:
        p_val = (1 - sc.norm.cdf(abs(z_score))) * 2

    reject_null = p_val &amp;lt; alpha
    #print &amp;#39;z-score: %s, p-value: %s, reject null: %s&amp;#39; % (z_score, p_val, reject_null)
    return z_score, p_val, reject_null



conA,cntA = dfa.converted.mean(),dfa.converted.count()
conB,cntB = dfb.converted.mean(),dfb.converted.count()
print conA,conB,cntA,cntB,0.01*conB
z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)

0.0996819218616 0.0996421296041 95574 90815 0.000996421296041





(-0.74648172622270292, 0.22768823318094589, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this frame there results are not significantly different from a 1% left that we can rule them out.  But we could have also tested if there is a difference between them.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;z_test(conA,conB,cntA,cntB,two_tailed=True,effect_size=0.0)




(-0.028666091979081442, 0.9771308999283459, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is also not signifcant difference between the groups either.  The results of our Tuesday experiment are really inconclusive.  Ultimately we are concerned with the effect on weekend users, because they are responsible for most of Etsy's revenue.   We were told this user base is different from the weekend users.  &lt;/p&gt;
&lt;p&gt;AirBNB had a talks (&lt;a href="http://nerds.airbnb.com/experiments-airbnb/"&gt;here&lt;/a&gt; and &lt;a href="http://nerds.airbnb.com/experiments-at-airbnb/"&gt;here&lt;/a&gt;) about looking at the hourly change in a p-vale, and examing if and when it level's off as the 'true' p-value for an experiment.   We are going to explore this method.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_values = []
effect = 0
last_effect=0
for i in range(23):

    # Grab the hour
    df_houra = dfa[dfa.dt.dt.hour&amp;lt;=i]
    df_hourb = dfb[dfb.dt.dt.hour&amp;lt;=i]

    conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
    conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

    p_values.append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)[1] )

plt.figure()
plt.plot(range(23),p_values,color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;From this method we see that the p-value is still changing, making me believe that our experiment could be under-powered.&lt;/p&gt;
&lt;p&gt;There is additional data about the country of each user.  It could be interesting to look that these results by country.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;countries = pd.read_csv(&amp;quot;../ab-testing/data/country.csv&amp;quot;)
countries.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;9160993935&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5879439034&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;8915383273&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2917824565&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3980216975&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries,on=&amp;#39;user_id&amp;#39;)
dfbc = dfb.merge(countries,on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
print dfbc.user_id.count(),dfbc.user_id.nunique()
dfac.head()

97151 92554
87927 87924
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23267&lt;/td&gt;
      &lt;td&gt;1357066015&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 18:46:55&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;79973&lt;/td&gt;
      &lt;td&gt;1357018111&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:28:31&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;338650&lt;/td&gt;
      &lt;td&gt;1357083484&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:38:04&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;340147&lt;/td&gt;
      &lt;td&gt;1357083599&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:39:59&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;382429&lt;/td&gt;
      &lt;td&gt;1357002072&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 01:01:12&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac.groupby(&amp;#39;user_id&amp;#39;).country.count().max()




2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that some users have two countries listed.  Since the user country data is not time sensitive, we have to drop them.  We do not know which country they were in at the type of the experiment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
dfbc = dfb.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfbc.user_id.count(),dfbc.user_id.nunique()

92554 92554
87925 87924




p_values = {&amp;#39;US&amp;#39;:[],&amp;#39;CA&amp;#39;:[],&amp;#39;UK&amp;#39;:[]}
effect = {&amp;#39;US&amp;#39;:0,&amp;#39;CA&amp;#39;:0,&amp;#39;UK&amp;#39;:0}
last_effect=0
for i in range(23):
    for country in dfac.country.unique():
    # Grab the hour
        df_houra = dfac[(dfac.dt.dt.hour&amp;lt;=i)&amp;amp;(dfac.country==country)]
        df_hourb = dfbc[(dfbc.dt.dt.hour&amp;lt;=i)&amp;amp;(dfbc.country==country)]

        conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
        conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

        p_values[country].append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*effect[country])[1] )
        effect[country] = conB

plt.figure()
plt.plot(range(23),p_values[&amp;#39;US&amp;#39;],color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;US&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;CA&amp;#39;],color=&amp;#39;blue&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;CA&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;UK&amp;#39;],color=&amp;#39;green&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;UK&amp;#39;)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D3/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we constructed a hypthoesis at the end of day on Tuesday, the US would look like they were responsive to the new page, but the time plot shows that this could be a cherry-picked value.  If the experiment was allowed to run for more time, or we collect more data, the value would have changed.   It is clear from these plots that the sample sizes are not large enough that a additional data does not heavily influence the result.   I would be hesitant to make any strong conclusions about the new page based on these results.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="statistics"></category><category term="hypthesis testing"></category></entry><entry><title>Galvanize - Week 02 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-02/" rel="alternate"></link><updated>2015-06-09T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-09:galvanize/galvanize-data-science-02-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 2&lt;/h2&gt;
&lt;p&gt;Today we started with a mini-quiz, had a lecture on sampling methods, were given a talk about searching for a job, and finished the day with lecture on estimations/bootstraping and a reinforcement paired programming section.&lt;/p&gt;
&lt;h2&gt;Mini-Quiz&lt;/h2&gt;
&lt;p&gt;The mini-quiz is interesting because it involved using pandas, which I have found to be great and flexible, while also mysterious.&lt;/p&gt;
&lt;p&gt;We were given a salary dataset and asked to make some changes to it and answer some questions.  The first was to read in the data and convert the names to a human readiable text and transform variables to the correct type.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../estimation-sampling/data/salary_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;info&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nc"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;core&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;Int64Index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;32159&lt;/span&gt;
&lt;span class="n"&gt;Data&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;          &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;job_title&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;department&lt;/span&gt;    &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;        &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;Join&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;



&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;job_title&lt;/th&gt;
      &lt;th&gt;department&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.columns = [&amp;#39;Name&amp;#39;,&amp;#39;Position Title&amp;#39;,&amp;#39;Department&amp;#39;,&amp;#39;Employee Annual Salary&amp;#39;,&amp;#39;Join Date&amp;#39;]
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This is how I traditionally have renamed columns.   I learned a new way that involed using pandas' 'rename' function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;../estimation-sampling/data/salary_data.csv&amp;#39;)
salary.rename(columns={&amp;#39;name&amp;#39;: &amp;#39;Name&amp;#39;,
                  &amp;#39;job_title&amp;#39;: &amp;#39;Position Title&amp;#39;,
                  &amp;#39;department&amp;#39;:&amp;#39;Department&amp;#39;,
                  &amp;#39;salary&amp;#39;:&amp;#39;Employee Annual Salary&amp;#39;,
                   &amp;#39;join_data&amp;#39;: &amp;#39;Join Date&amp;#39;},
                   inplace=True)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;I personally do not like these names for the columns because they involve spaces.   That removes the ability to us the pd.variable notation.   &lt;/p&gt;
&lt;p&gt;I have also found multiple ways to update a variable type.  I am still not sure if there is a better method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary[&amp;#39;Employee Annual Salary&amp;#39;] = salary[&amp;#39;Employee Annual Salary&amp;#39;].str.replace(&amp;quot;$&amp;quot;,&amp;quot;&amp;quot;).astype(float)
salary[&amp;#39;Join Date&amp;#39;] = pd.to_datetime(salary[&amp;#39;Join Date&amp;#39;])
salary.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 32160 entries, 0 to 32159
Data columns (total 5 columns):
Name                      32160 non-null object
Position Title            32160 non-null object
Department                32160 non-null object
Employee Annual Salary    32160 non-null float64
Join Date                 32160 non-null datetime64[ns]
dtypes: datetime64[ns](1), float64(1), object(3)
memory usage: 1.5+ MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have the data in the correct format, we can now answer questions about the dataset.  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the top 5 paying job titles?&lt;/li&gt;
&lt;li&gt;How many people have "Police" in their title?&lt;/li&gt;
&lt;li&gt;What fraction of the people in 2 are a 'Police Officer'&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How many people were hired from July 30, 2000 to Aug 08, 2000&lt;/p&gt;
&lt;p&gt;salary.groupby('Position Title')['Employee Annual Salary'].mean().order(ascending=False).head()&lt;/p&gt;
&lt;p&gt;Position Title
SUPERINTENDENT OF POLICE          260004
MAYOR                             216210
FIRE COMMISSIONER                 202728
FIRST DEPUTY SUPERINTENDENT       188316
FIRST DEPUTY FIRE COMMISSIONER    188316
Name: Employee Annual Salary, dtype: float64&lt;/p&gt;
&lt;p&gt;print "Contains 'POLICE': ", salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].count()
salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].value_counts(normalize=True)&lt;/p&gt;
&lt;p&gt;Contains 'POLICE':  11141&lt;/p&gt;
&lt;p&gt;POLICE OFFICER                                      0.847051
POLICE OFFICER (ASSIGNED AS DETECTIVE)              0.076025
POLICE COMMUNICATIONS OPERATOR II                   0.019747
POLICE COMMUNICATIONS OPERATOR I                    0.012925
POLICE OFFICER / FLD TRNG OFFICER                   0.010232
POLICE OFFICER (ASSIGNED AS EVIDENCE TECHNICIAN)    0.006463
POLICE OFFICER/EXPLSV DETECT K9 HNDLR               0.003590
POLICE OFFICER (ASGND AS MARINE OFFICER)            0.002783
POLICE CADET                                        0.002603
ELECTRICAL MECHANIC-AUTO-POLICE MTR MNT             0.002423
MACHINIST (AUTO) POLICE MOTOR MAINT                 0.002244
POLICE OFFICER (ASSIGNED AS CANINE HANDLER)         0.001885
POLICE OFFICER (ASSIGNED AS TRAFFIC SPECIALIST)     0.001795
SUPERVISING POLICE COMMUNICATIONS OPERATOR          0.001616
POLICE OFFICER (ASGND AS MOUNTED PATROL OFFICER)    0.001346
POLICE OFFICER (ASSIGNED AS SECURITY SPECIALIST)    0.001346
POLICE AGENT                                        0.001167
POLICE FORENSIC INVESTIGATOR I                      0.001077
POLICE OFFICER(ASGND AS LATENT PRINT EX)            0.000987
POLICE OFFICER (PER ARBITRATION AWARD)              0.000898
POLICE TECHNICIAN                                   0.000539
POLICE LEGAL OFFICER II                             0.000359
POLICE LEGAL OFFICER I                              0.000269
DIR OF POLICE RECORDS                               0.000090
MANAGER OF POLICE PAYROLLS                          0.000090
POLICE OFFICER(ASGND AS SUPVG LATENT PRINT EX)      0.000090
EXECUTIVE DIR - POLICE BOARD                        0.000090
SUPERINTENDENT OF POLICE                            0.000090
ASST SUPVSR OF POLICE RECORDS                       0.000090
MANAGER OF POLICE PERSONNEL                         0.000090
dtype: float64&lt;/p&gt;
&lt;p&gt;salary.set_index('Join Date',inplace=True)
salary.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-09-27&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;87228&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-08-04&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-20&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-04-27&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;80916&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-02-11&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;99648&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.ix[&amp;#39;2000-07-13&amp;#39; : &amp;#39;2000-08-13&amp;#39;].count()




Name                      2866
Position Title            2866
Department                2866
Employee Annual Salary    2866
dtype: int64
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Morning Sprint&lt;/h2&gt;
&lt;p&gt;The individual morning sprint covered sampling and estimation.   We were given a dataset on rain fall and attempted to use &lt;a href="http://en.wikipedia.org/wiki/Method_of_moments_%28statistics%29"&gt;Method of Moments&lt;/a&gt; estimates on the data to approximate the distributions.  We then followed up by looking at &lt;a href="http://en.wikipedia.org/wiki/Maximum_likelihood"&gt;Maximum Likelihood Estimates&lt;/a&gt; of the parameters.  &lt;/p&gt;
&lt;p&gt;I first looked at the data for January rainfall over the course of several years.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = pd.read_csv(&amp;quot;../estimation-sampling/data/rainfall.csv&amp;quot;)
print data.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 140 entries, 0 to 139
Data columns (total 13 columns):
Year    140 non-null int64
Jan     140 non-null float64
Feb     140 non-null float64
Mar     140 non-null float64
Apr     140 non-null float64
May     140 non-null float64
Jun     140 non-null float64
Jul     140 non-null float64
Aug     140 non-null float64
Sep     140 non-null float64
Oct     140 non-null float64
Nov     140 non-null float64
Dec     140 non-null float64
dtypes: float64(12), int64(1)
memory usage: 15.3 KB
None



data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Jan&lt;/th&gt;
      &lt;th&gt;Feb&lt;/th&gt;
      &lt;th&gt;Mar&lt;/th&gt;
      &lt;th&gt;Apr&lt;/th&gt;
      &lt;th&gt;May&lt;/th&gt;
      &lt;th&gt;Jun&lt;/th&gt;
      &lt;th&gt;Jul&lt;/th&gt;
      &lt;th&gt;Aug&lt;/th&gt;
      &lt;th&gt;Sep&lt;/th&gt;
      &lt;th&gt;Oct&lt;/th&gt;
      &lt;th&gt;Nov&lt;/th&gt;
      &lt;th&gt;Dec&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1871&lt;/td&gt;
      &lt;td&gt;2.76&lt;/td&gt;
      &lt;td&gt;4.58&lt;/td&gt;
      &lt;td&gt;5.01&lt;/td&gt;
      &lt;td&gt;4.13&lt;/td&gt;
      &lt;td&gt;3.30&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;0.95&lt;/td&gt;
      &lt;td&gt;1.31&lt;/td&gt;
      &lt;td&gt;2.13&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1872&lt;/td&gt;
      &lt;td&gt;2.32&lt;/td&gt;
      &lt;td&gt;2.11&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;5.91&lt;/td&gt;
      &lt;td&gt;3.09&lt;/td&gt;
      &lt;td&gt;5.17&lt;/td&gt;
      &lt;td&gt;6.10&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;4.50&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.25&lt;/td&gt;
      &lt;td&gt;2.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1873&lt;/td&gt;
      &lt;td&gt;2.96&lt;/td&gt;
      &lt;td&gt;7.14&lt;/td&gt;
      &lt;td&gt;4.11&lt;/td&gt;
      &lt;td&gt;3.59&lt;/td&gt;
      &lt;td&gt;6.31&lt;/td&gt;
      &lt;td&gt;4.20&lt;/td&gt;
      &lt;td&gt;4.63&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
      &lt;td&gt;4.28&lt;/td&gt;
      &lt;td&gt;4.36&lt;/td&gt;
      &lt;td&gt;5.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1874&lt;/td&gt;
      &lt;td&gt;5.22&lt;/td&gt;
      &lt;td&gt;9.23&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;11.84&lt;/td&gt;
      &lt;td&gt;1.49&lt;/td&gt;
      &lt;td&gt;2.87&lt;/td&gt;
      &lt;td&gt;2.65&lt;/td&gt;
      &lt;td&gt;3.52&lt;/td&gt;
      &lt;td&gt;3.12&lt;/td&gt;
      &lt;td&gt;2.63&lt;/td&gt;
      &lt;td&gt;6.12&lt;/td&gt;
      &lt;td&gt;4.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1875&lt;/td&gt;
      &lt;td&gt;6.15&lt;/td&gt;
      &lt;td&gt;3.06&lt;/td&gt;
      &lt;td&gt;8.14&lt;/td&gt;
      &lt;td&gt;4.22&lt;/td&gt;
      &lt;td&gt;1.73&lt;/td&gt;
      &lt;td&gt;5.63&lt;/td&gt;
      &lt;td&gt;8.12&lt;/td&gt;
      &lt;td&gt;1.60&lt;/td&gt;
      &lt;td&gt;3.79&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
      &lt;td&gt;5.46&lt;/td&gt;
      &lt;td&gt;4.30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_16_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;To me this looks like it could be well fitted by a &lt;a href="http://en.wikipedia.org/wiki/Poisson_distribution"&gt;Poisson distribution&lt;/a&gt; or a &lt;a href="http://en.wikipedia.org/wiki/Gamma_distribution"&gt;Gamma distribution&lt;/a&gt;.   A poisson distribution models random events occuring in a fixed time interval, and is a discreate distribution.  Gamma distributions are a continuous distribution that model how long one must way for N events to happen.   That seems like a better framework to think about rain fall.&lt;/p&gt;
&lt;p&gt;The mean and variance for a Poisson distribution is the lambda parameter:&lt;/p&gt;
&lt;p&gt;$$\mu = \lambda$$&lt;/p&gt;
&lt;p&gt;So we will use this to see the fit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;

&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Jan Rain&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Poisson Fit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Fall In Janary For All Years&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mf"&gt;4.54457142857&lt;/span&gt; &lt;span class="mf"&gt;6.91677463515&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_18_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This gives a fair fit to the distribution for january.   I want to compare this with the gama distribution.&lt;/p&gt;
&lt;p&gt;The gamma function is given by:&lt;/p&gt;
&lt;p&gt;$$X = Gamma(\alpha \ ,\beta) = \frac{\beta^\alpha \ x^{\alpha-1} \ e^{-\beta x}}{\Gamma(\alpha)}$$&lt;/p&gt;
&lt;p&gt;The mean and variance of the gamma distribution is given by&lt;/p&gt;
&lt;p&gt;$$\mu = \frac{\alpha}{\beta}$$&lt;/p&gt;
&lt;p&gt;$$\sigma^2 = \frac{\alpha}{\beta^2}$$&lt;/p&gt;
&lt;p&gt;So the estimate of alpha and beta are given by:&lt;/p&gt;
&lt;p&gt;$$\beta = \frac{\mu}{\sigma^2}$$&lt;/p&gt;
&lt;p&gt;$$\alpha = \frac{\mu^2}{\sigma^2}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = mean/var
alpha = mean**2/var
print alpha,beta
x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1/beta)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x,y,color=&amp;#39;red&amp;#39;,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Poisson Fit&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_20_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Gamma distribution fit matches the distribution's peak and tail better than the Poisson distribution fit.  There are method's to test the relative fit but I saved that for another day.&lt;/p&gt;
&lt;p&gt;Now lets look at the Gamma fits for all months.  The reason we bin by months is that we have the prior that rain and weather is season.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;f, axarr = plt.subplots(3, 4,figsize=(14, 8))

x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    y = sc.gamma.pdf(x,alpha,scale=1/beta)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;Gamma Fit&amp;quot;,color=&amp;#39;red&amp;#39;)
    label = &amp;#39;alpha = %.2f\nbeta = %.2f&amp;#39; % (alpha, beta)
    axarr[i/4,i%4].annotate(label, xy=(4, 0.25))

plt.tight_layout()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_22_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have a Method of Moments fit of the gamma distribution of rainfall for each month.   Now lets doo Maximum Likely Hood.  &lt;/p&gt;
&lt;p&gt;First we need to make a funciton.  In order to test the method I will try it on a poisson generated dataset.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def poisson_likelihood(x, lam):
    return sc.poisson.pmf(x,lam)

##produces the probabilty of of each lambda(lam) given a value of 6
plt.plot(range(1,20),[poisson_likelihood(6, lam) for lam in range(1,20)])
plt.xlabel(&amp;quot;Lambda Value&amp;quot;)
plt.ylabel(&amp;quot;Probability of Lambda Value Given x=6&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c5d0290&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This make sense because the maximum likelihood is 6, but there are still changes that the value is different.  Lets run this on the data now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_data = pd.read_csv(&amp;#39;../estimation-sampling/data/poisson.txt&amp;#39;,header=None)
p_data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;x_values = np.linspace(1,20,1000)
likelihoods = np.array([np.log(poisson_likelihood(p_data.values[:,0],i)).sum() for i in x_values])
plt.plot(x_values,likelihoods)
plt.ylabel(&amp;quot;Log Likelyhood For Lambda Fro Data&amp;quot;)
plt.xlabel(&amp;quot;Lambda Values&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c67f090&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_27_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We want to compare the maximum likelihood (argmax) of this distribution to the mean fo the data since the mean of Poisson distribution should be the lambda parameter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x_values[likelihoods.argmax()],p_data.mean()




(5.0510510510510516, 0    5.0437
 dtype: float64)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The maximum likelihood estimate of the lambda parameter is very close to the sample mean, which also matches the value of lambda used to generated the data ($$\lambda = 5$$).&lt;/p&gt;
&lt;p&gt;Scipy Stats has a fit function for each of the distributions that uses the maximum likelihood method.   I want to compare the plots the difference between the fits of the Method of Moments and the Maximum Likelihood.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mean = data.Jan.mean()
var = data.Jan.var()
beta = mean/var
alpha = mean**2/var

x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1./beta)
alpha_MLE,loc_MFL,one_over_beta_MLE = sc.gamma.fit(data.Jan,floc=0) #Loc = 0 - no rainfall minimum
y2 = sc.gamma.pdf(x1,alpha_MLE,scale=one_over_beta_MLE)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MoM)&amp;#39;,linestyle=&amp;#39;--&amp;#39;)
plt.plot(x1,y2,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MLE)&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()
print alpha, alpha_MLE
print beta, beta_MLE

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_31_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;2.98594801173 3.25219914651
0.65703621533 1.39738411574
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the two distributions are similar, but slightly different.   The MLE is more skewed right, and the MLE fits are larger then the method of moments.  I just want to finish this section with a plot of all the months.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    print month, alpha, alpha_fit, beta, beta_fit
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()

Jan 2.98594801173 3.25219914651 0.65703621533 0.715622847528
Feb 3.0418721755 3.0803224672 0.740681272732 0.750043734186
Mar 4.67867768543 4.64576013378 0.946813252137 0.94015180285
Apr 4.28032831959 4.25175417646 1.01660157558 1.00981505905
May 3.53902182175 3.82055049055 0.815644176548 0.880528551612
Jun 2.97083704473 2.89974494499 0.765862938963 0.747535846757
Jul 3.98358361758 3.6314371778 1.02531889482 0.934681309897
Aug 3.02948804114 3.20185713476 0.907886646459 0.959542766645
Sep 2.29238948802 2.14489278382 0.678766821038 0.635093671449
Oct 2.46786112353 1.96116795936 0.945359556993 0.751261428601
Nov 3.69539855825 3.30306402837 1.00014653216 0.893962581139
Dec 3.23590721109 3.52396472416 0.77216125712 0.840898349041



&amp;lt;matplotlib.figure.Figure at 0x10c387e10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_33_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;For each moths the distributions are similar, but like january the alpha and beta values are different. &lt;/p&gt;
&lt;h2&gt;Kernal Density Estimates&lt;/h2&gt;
&lt;p&gt;The last topic we had was to use the non-parametric method for fitting a distributions using gaussian kernal density estimates.   The idea is that each data point is fit with a gausian for some unknown variance, and the variance is shared for each such gaussian.   The variance paramenter is adjusted until an 'optimal' fit is found.&lt;/p&gt;
&lt;p&gt;We can do this with an example by convoluting two gaussian data sets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data2 = [sc.norm.rvs(loc=0,scale=2) for x in range(500)]+[sc.norm.rvs(loc=4,scale=1) for x in range(400)]
plt.figure()
plt.hist(data,bins=30,color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_35_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fit = sc.gaussian_kde(data2)
plt.figure()
plt.hist(data,bins=30,normed=True,alpha=0.2)
x=np.linspace(-6,8,100)
plt.plot(x,fit(x),color=&amp;#39;blue&amp;#39;,label=&amp;#39;KDE Fit&amp;#39;)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The fit function can be used for a density estimate when we do not wish to model the data with a particlar model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    gfit = sc.gaussian_kde(data[month])
    yg = gfit(x)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y2,label=&amp;quot;KDE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;blue&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10c65e490&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_38_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In each case the non-KDE function seem tot fit the data better, but it could be used to for other estimates if we did not have a model.&lt;/p&gt;
&lt;h2&gt;Paired Programming&lt;/h2&gt;
&lt;p&gt;In the afternoon session we had to investigate the centeral limit theorem, produce confidence intervals, and attempt some bootstrapping estimates&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_draws(distribution, parameters, size):
    &amp;#39;&amp;#39;&amp;#39;
        returns distribrution or None if valid distribution is not selected
    &amp;#39;&amp;#39;&amp;#39;    
    dist = None

    if distribution.lower() == &amp;#39;binomial&amp;#39;:
        n, p = parameters[&amp;#39;n&amp;#39;], parameters[&amp;#39;p&amp;#39;]
        dist = sc.binom(n, p).rvs(size)

    elif distribution.lower() == &amp;#39;exponential&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.expon(scale = l).rvs(size)

    elif distribution.lower() == &amp;#39;poisson&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.poisson(mu=l).rvs(size)

    elif distribution.lower() == &amp;#39;gamma&amp;#39;:
        a, b = parameters[&amp;#39;alpha&amp;#39;],parameters[&amp;#39;beta&amp;#39;]
        dist = sc.gamma(a=a,scale=1./b).rvs(size)

    elif distribution.lower() == &amp;#39;normal&amp;#39;:
        mean, var = parameters[&amp;#39;mean&amp;#39;], parameters[&amp;#39;var&amp;#39;]
        dist = sc.norm(loc=mean, scale=var).rvs(size)

    elif distribution.lower() == &amp;#39;uniform&amp;#39;:
        low, high = parameters[&amp;#39;low&amp;#39;], parameters[&amp;#39;high&amp;#39;]
        dist = sc.uniform(loc=low, scale=(high-low)).rvs(size)

    return dist

def plot_means(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).mean())
    plt.figure()
    plt.title(&amp;quot;Centeral Limit Theorem: &amp;quot; + distribution + &amp;quot; for N = &amp;quot; + str(size))
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 10, 5000)
plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these distirubtions we see that for N = 10, espeicaly for the discrete distriubiton, that the sampling distirubiton of the mean is not normal.   If the underlying distribution is skewed, so is the sampling distribution.  If we look at means of samples of size 200, the central limit theorm holds and the sampling distribution is normal even if the underlying distribution is skewed or discrete.  &lt;/p&gt;
&lt;p&gt;The central limit theorm does not hold for all statistics.  We can look at the max, for instance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def plot_max(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).max())
    plt.figure()
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_max(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:5, &amp;#39;high&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_5.png" /&gt;&lt;/p&gt;
&lt;p&gt;These distributions are clearly not normally distributed, and the discrete distribution results remain discrete.&lt;/p&gt;
&lt;h3&gt;Population Inference and Confidence Interval&lt;/h3&gt;
&lt;p&gt;Our next section had to do with constructing confidence intervals on means for different situations.   We were given some lunch data, and attempted to construct the confidence interval for the mean lunch break.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lunch_hour = np.loadtxt(&amp;#39;../estimation-sampling/data/lunch_hour.txt&amp;#39;)
plt.figure()
plt.hist(lunch_hour)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_50_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;There are 25 data points in the data.   Even though this distrubtion is not normal, the sampling distribution of the mean should be approching a normal distribution.  &lt;/p&gt;
&lt;p&gt;The standard deviation of the sampling distirubiton is suppose to be well approximated by the standard error of the sample.&lt;/p&gt;
&lt;p&gt;$$s = \sqrt{ \frac{\Sigma_{i}(x_i \ - \ \bar{x})^2}{N-1} }$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = lunch_hour.std(ddof=1) / np.sqrt( len(lunch_hour) )
se, sc.sem(lunch_hour) ##scipy standard error comparison




(0.040925827625524797, 0.040925827625524797)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using this standard error we can attempt to construct the confidence interval on the population mean.  We choose Z=1.96 for a 95% confidence interval&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lm = lunch_hour.mean()
(ci_95_low, ci_95_hi) = (lm-1.96*se,lm+1.96*se)
(ci_95_low, ci_95_hi)




(2.1042853778539716, 2.264714622146029)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The 95% confidence interval interpretation is that 95% of the confidence intervals constructed through this method will contrain the population mean lunch hour.   If the sample size was smaller, the both the standard error and normal approximation would change in a way that would not allow this method to work.   &lt;/p&gt;
&lt;p&gt;For smaller sample sizes we would want to try another method.  Bootstrapping could be effective.&lt;/p&gt;
&lt;p&gt;Bootstrapping does not assume normality, or more any assumptions about the underlying distribution.   It is a non-parametric method of constructiong an confidence interval.   If the distribution is well approximateldy by some common distribution, the bootstrapped CI will overestimate the boundaries compared to this distribution.  &lt;/p&gt;
&lt;p&gt;We will try this for another data set involving productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;productivity = np.loadtxt(&amp;#39;../estimation-sampling/data/productivity.txt&amp;#39;)
productivity




array([-19.1, -15.2, -12.4, -15.4,  -8.7,  -6.7,  -5.9,  -3.5,  -3.1,
        -2.1,   4.2,   6.1,   7. ,   9.1,  10. ,  10.3,  13.2,  10.1,
        14.1,  14.4,  20.1,  26.3,  27.7,  22.2,  23.4])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because the sample size is large enough, we would expect the centeral limit to hold.  Lets see if the boot strapping gives similar values.  If we do not know the population variance, we should us the t-distribution.   We will also check that.  The two results should be close&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = sc.sem(productivity)
mean = productivity.mean()
ci95lo, ci95hi = mean-1.96*se, mean+1.96*se
print ci95lo, ci95hi

-0.330202770421 10.4182027704



ci95lo, ci95hi = mean+sc.t.ppf(0.025,24)*se, mean+sc.t.ppf(0.975,24)*se
print ci95lo, ci95hi

-0.615086412127 10.7030864121
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These intervals are very close.   Bootstrapping should give a similar result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt; &lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;np.random.randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

    &lt;span class="nx"&gt;statistic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;array&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;means&lt;/span&gt;
    &lt;span class="nx"&gt;low&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="nx"&gt;high&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;low&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;high&lt;/span&gt;

&lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.49619999999999997&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.265000000000001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is in line with the previous estimates.   Lets look at the histogram of bootstrapped means.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bootstrap_plot_means(sample, iterations=1000):
    samples = bootstrap(sample, iterations)
    means = np.apply_along_axis(np.mean, 1, samples)
    plt.figure()
    plt.hist(means,normed=True,color=&amp;#39;red&amp;#39;,alpha=0.1)
    plt.xlabel(&amp;#39;Mean Values&amp;#39;)
    plt.ylabel(&amp;#39;Probability Density&amp;#39;)
    plt.show()

bootstrap_plot_means(productivity)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Even though the results are not a statistically significant difference from zero, the results suggest that the population value is likely to be different from zero.   The uncertainty from the sample does not allow us to know that it is not zero, but we do know that it does not significantly harm productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping Correlation&lt;/h3&gt;
&lt;p&gt;We can bootstrap other variables.   We will try it for the correlation between LSAT and GPA from law data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;law_sample = np.loadtxt(&amp;#39;../estimation-sampling/data/law_sample.txt&amp;#39;)
plt.scatter(law_sample[:,0],law_sample[:,1])
plt.xlabel(&amp;quot;LSAT Score&amp;quot;)
plt.ylabel(&amp;quot;GPA&amp;quot;)
print sc.pearsonr(law_sample[:,0],law_sample[:,1])

(0.77637449128940705, 0.00066510201110281625)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_65_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = bootstrap(law_sample,B=10000)
corrs = np.array([sc.pearsonr(mat[:,0],mat[:,1])[0] for mat in data])
plt.figure()
plt.hist(corrs)
plt.show()
np.percentile(corrs,2.5),np.percentile(corrs,97.5)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_66_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.45069334540504685, 0.96239529249176581)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bootstrapping the correlation between the variables from the sampple finds that a 95% confidence interval estimates the population correlation of LSAT with GPA should be between 0.45 and 0.96.   Thankfully we have the full dataset from which this sample was pulled.   Lets compare.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;all_law = np.loadtxt(&amp;#39;../estimation-sampling/data/law_all.txt&amp;#39;)
sc.pearsonr(all_law[:,0],all_law[:,1])[0]




0.75999785550389798
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is smack in the middle of the confidence interval.   Pretty cool.   &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="bootstraping"></category><category term="statistics"></category><category term="confidence interval"></category></entry><entry><title>Galvanize - Week 02 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-01/" rel="alternate"></link><updated>2015-06-08T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-08:galvanize/galvanize-data-science-02-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 1&lt;/h2&gt;
&lt;p&gt;Since this is the first day of the week we started with an hour long assessment on what we did last week.  The assessment was straight forward, and very doable with a modest understanding of the previous material.   &lt;/p&gt;
&lt;p&gt;After the test, we started a lecture on probability, which we finished in the afternoon.  There was the individual sprint after the morning lecture, and a paired spring as the afternoon lecture&lt;/p&gt;
&lt;h2&gt;Conditional Probabilities&lt;/h2&gt;
&lt;p&gt;We started with some simple questions like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose two cards are drawn from a standard 52 card deck.  What's the probability that the first is a queen and the second is a king?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(K,Q\right) = P\left(K|Q\right)*P\left(Q\right) = \frac{4}{51} * \frac{4}{52} = \frac{16}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 16./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00603318250377&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that both cards are queens?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q|Q\right)*P\left(Q\right) = \frac{3}{51} * \frac{4}{52} = \frac{12}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 12./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00452488687783&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose that before the second card was drawn, the first was inserted back into the deck and the deck reshuffled. What's the probability that both cards are queens?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q\right)*P\left(Q\right) = \frac{4}{52} * \frac{4}{52} = \frac{16}{2705}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 16./2705

Answer:  0.00591497227357
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We had similar questions about tables of data:&lt;/p&gt;
&lt;p&gt;A Store Manager wants to understand how his customers use different payment methods, and suspects that the size of the purchase is a major deciding factor. He organizes the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="right"&gt;Cash&lt;/th&gt;
&lt;th align="right"&gt;Debit&lt;/th&gt;
&lt;th align="right"&gt;Credit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Under 20&lt;/td&gt;
&lt;td align="right"&gt;400&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20 - 50&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;1200&lt;/td&gt;
&lt;td align="right"&gt;800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Over 50&lt;/td&gt;
&lt;td align="right"&gt;100&lt;/td&gt;
&lt;td align="right"&gt;600&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer spent over $50, what's the probability that the customer used a credit card?&lt;/p&gt;
&lt;p&gt;$$P\left(C|S&amp;gt;$50\right) = \frac{1400}{100+600+1400}$$ &lt;/p&gt;
&lt;p&gt;print "Answer: ", 1400./(100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.666666666667&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer paid in cash, what's the probability that the customer spent less than $20?&lt;/p&gt;
&lt;p&gt;$$P\left(S&amp;lt;20|Cash\right) = \frac{400}{400+200+100}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+200+100)&lt;/p&gt;
&lt;p&gt;Answer:  0.571428571429&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that a customer spent under $20 using cash?&lt;/p&gt;
&lt;p&gt;$$P\left(S &amp;lt; $20,Cash\right) = \frac{400}{400+150+150+200+1200+800+100+600+1400}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+150+150+200+1200+800+100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.08&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also had a question about job offers - something near and dear to our hearts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A gSchool grad is looking for her first job!  Given that she is freaked out, her chances of not getting an offer are 70%.  Given that she isn't freaked out, her chances of not getting an offer are 30%.  Suppose that the probability that she's freaked out is 80%. What's the probability that she gets an offer?&lt;/p&gt;
&lt;p&gt;$$P\left(Offer|Freak Out\right) = 0.7$$  &lt;/p&gt;
&lt;p&gt;$$P\left(Offer|No Freak Oout\right) = 0.3$$&lt;/p&gt;
&lt;p&gt;$$P\left(Freak Out\right) = 0.8$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ P\left(A\right) = \Sigma_{B} P\left(A|B\right) $$&lt;/p&gt;
&lt;p&gt;$$ P\left(Offer\right) = P\left(Offer|Freak Out\right) * P\left(Freak Out\right) + P\left(Offer|No Freak Out\right) * P\left(No Freak Out\right)$$&lt;/p&gt;
&lt;p&gt;$$P\left(Offer\right) = 0.7 * 0.8 + 0.3 * 0.2$$ &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.7 * 0.8 + 0.3 * 0.2

Answer:  0.62
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also tackled the deep issue go heroin use at Google: &lt;/p&gt;
&lt;p&gt;*. Google decides to do random drug tests for heroin on their employees.
   They know that 3% of their population uses heroin. The drug test has the
   following accuracy: The test correctly identifies 95% of the
   heroin users (sensitivity) and 90% of the non-users (specificity).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test Results&lt;/th&gt;
&lt;th&gt;Uses heroin&lt;/th&gt;
&lt;th&gt;Doesn't use heroin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tests positive&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tests negative&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alice gets tested and the test comes back positive. What is the probability
   that she uses heroin?&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test)}$$&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test|Heroin) P(Heroin) + P(Positive Test|No Heroin) P(No Heroin)}$$ &lt;/p&gt;
&lt;p&gt;$$ = \frac{0.95 \ 0.03}{0.95 \ 0.03 + 0.1 \ 0.97}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.95*0.03/(0.95*0.03 + 0.1*0.97)

Answer:  0.227091633466
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally we had the manditory birthday problem:   &lt;/p&gt;
&lt;p&gt;*The Birthday Problem.  Suppose there are 23 people in a data science class, lined up in a single file line.&lt;br /&gt;
Let A_i be the probability that the i'th person doesn't have the same birthday as the j'th person for any j &amp;lt; i.&lt;br /&gt;
Use the chain rule from probability to calculate the probability that at least 2 people share the same birthday. &lt;/p&gt;
&lt;p&gt;$$P(1,2,3,...,23) = \mbox{Probability that 23 people do not have the same birthday}$$&lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2, \ 3, \ ..., \ 23) = P(1) \ P(2|1) \ P(3 \ | \ 2 \ , \ 1) \ ... \ P(23|22 \ , \ ... \ , \ 2, \ 1)$$&lt;/p&gt;
&lt;p&gt;Given that 2 people don't have the same birthday, there are 363 days that are not taken that a new person could have:&lt;/p&gt;
&lt;p&gt;$$P(3|2,1) = \frac{363}{365}$$&lt;/p&gt;
&lt;p&gt;Similarly, given that 3 people don't have the same birthday, then there are 362 days not occupied:&lt;/p&gt;
&lt;p&gt;$$P(4|3,2,1) = \frac{362}{365}$$&lt;/p&gt;
&lt;p&gt;Extending this to the problem we have the probability of no matching birthdays being &lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2,\ 3, \ ..., \ 23) = \frac{1 \ 364 \ 363 \ ... \ (365-23)}{365^{23}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bday(N=23):
    prob = 1
    for i in range(1,N+1):
        prob = prob*(365.0-i+1)/365.
    return prob

print bday()

0.492702765676
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The probability of having 2 or more matches is us the inverse of this&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1 - P(No Maches|23)$$&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1-0.4927$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, round(100-100*bday(),1)

Answer:  50.7
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Distributions&lt;/h2&gt;
&lt;p&gt;The afternoon paired programming assignment involved developing an intuition for and using various distributions.&lt;/p&gt;
&lt;h3&gt;Discrete:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model one instance of a success or failure trial (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binomial&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of successes out of a number of trials (n), each with probability of success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Poisson&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model the number of events occurring in a fixed interval&lt;/li&gt;
&lt;li&gt;Events occur at an average rate (lambda) independently of the last event&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Geometric&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence of Bernoulli trials until first success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Continuous:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uniform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any of the values in the interval of a to b are equally likely&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Commonly occurring distribution shaped like a bell curve&lt;/li&gt;
&lt;li&gt;Often comes up because of the Central Limit Theorem (to be discussed later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exponential&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model time between Poisson events&lt;/li&gt;
&lt;li&gt;Events occur continuously and independently&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of our questions involved being given examples and identify the distribution that describes it.&lt;/p&gt;
&lt;p&gt;Often we have to identify what distribution we should use to model a real-life
situation. This exercise is designed to equip you with the ability to do so.&lt;/p&gt;
&lt;p&gt;*. A typist makes on average 2 mistakes per page.  What is the probability of a particular page having no errors on it?&lt;/p&gt;
&lt;p&gt;$$X = Poisson(\lambda = 2 \frac{mistakes}{page})$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Mistakes on a Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of No Mistakes: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt; &lt;span class="n"&gt;Mistakes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Components are packed in boxes of 20. The probability of a component being
   defective is 0.1.  What is the probability of a box containing 2 defective components?&lt;/p&gt;
&lt;p&gt;$$ X = Binomial(p=0.1,k=2,n=20) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(20)
y = sc.binom.pmf(x,20,.1)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of Defective Components&amp;quot;)
plt.ylabel(&amp;quot;Probability&amp;quot;)
print &amp;quot;Prob of 2 Defects: &amp;quot;, sc.binom.pmf(2,20,.1)

Prob of 2 Defects:  0.285179807064
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_26_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Patrons arrive at a local bar at a mean rate of 30 per hour.  What is the probability that the bouncer has to wait more than 3 minutes to card the next patron?&lt;/p&gt;
&lt;p&gt;$$X = Exponential(\lambda = .5 \frac{cards}{minute})$$&lt;/p&gt;
&lt;p&gt;$$ P(t \ &amp;gt; \ 3min) = \int_{t=3}^{t=\infty} Exponential(\lambda = .5 \frac{cards}{minute}) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, sc.expon.cdf(1e10,scale=0.5)-sc.expon.cdf(3,scale=0.5)


 Anser:  0.00247875217667



x = np.linspace(0,5,1000)
y = sc.expon.pdf(x,scale = 0.5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.ylim([0,0.1])
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=3, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_29_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. A variable is normally distributed with a mean of 120 and a standard
   deviation of 5. One score is randomly sampled. What is the probability the score is above 127?&lt;/p&gt;
&lt;p&gt;$$Z = (127-120)/5 = 7/5 = 1.4$$&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;gt;1.4) = 1 - .91924 \ \mbox{(area to left)}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(100,140,1000)
y = sc.norm.pdf(x,loc=120,scale=5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Variable Value&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=127, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. You need to find a tall person, at least 6 feet tall, to help you reach
   a cookie jar. 8% of the population is 6 feet or taller.  If you wait on the sidewalk, how many people would you expect to have passed you by before you'd have a candidate to reach the jar?&lt;/p&gt;
&lt;p&gt;$$X = Geometric(p=0.08)$$&lt;/p&gt;
&lt;p&gt;$$average = \frac{1}{p} = \frac{1}{0.08} = 12.5$$&lt;/p&gt;
&lt;p&gt;We round up - 13th person is expected to be it - 12 people pass.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.pmf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106020790&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_33_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.cdf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Cumlative Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106451c50&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_34_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A harried passenger will be several minutes late for a scheduled 10 A.M.
   flight to NYC. Nevertheless, he might still make the flight, since boarding
   is always allowed until 10:10 A.M., and boarding is sometimes
   permitted up to 10:30 AM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assuming the extended boarding time is &lt;strong&gt;uniformly distributed&lt;/strong&gt; over the above
   limits, find the probability that the passenger will make his flight,
   assuming he arrives at the boarding gate at 10:25.&lt;/p&gt;
&lt;p&gt;$$X = Uniform(0,30)$$&lt;/p&gt;
&lt;p&gt;$$P( x &amp;gt; 25 ) = \int_{25}^{30} \frac{dx}{30}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 5./30

Answer:  0.166666666667



x = np.linspace(0,30,1000)
y = sc.uniform.pdf(x,loc=0,scale=30)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes Late&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=25, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_37_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Covariance and Joint Distribution&lt;/h2&gt;
&lt;p&gt;Suppose a university wants to look for factors that are correlated with the GPA of the students that they
are going to admit. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;admissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../probability/data/admissions.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;addmissions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;covariance&lt;/code&gt; function and compute the covariance matrix of the dataframe. Check your results 
   with &lt;code&gt;df.cov()&lt;/code&gt;. Make sure you understand what each of the numbers in the matrix represents&lt;/p&gt;
&lt;p&gt;def make_cov(df):
    N = len(df)
    cols = df.columns
    return [[(df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;from pprint import pprint
pprint (make_cov(addmissions))&lt;/p&gt;
&lt;p&gt;[[332910756.59847927, 4014.9337921708066, -1226.2147143883631],
 [4014.9337921708066, 0.087883196618951942, -0.028782641179958546],
 [-1226.2147143883631, -0.028782641179958546, 112]]&lt;/p&gt;
&lt;p&gt;addmissions.cov()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;3.329410e+08&lt;/td&gt;
      &lt;td&gt;4015.299085&lt;/td&gt;
      &lt;td&gt;-1226.326280&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;4.015299e+03&lt;/td&gt;
      &lt;td&gt;0.087891&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-1.226326e+03&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
      &lt;td&gt;112.977442&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;normalize&lt;/code&gt; function that would compute the correlation matrix from the covariance matrix.
   Check your results with &lt;code&gt;df.corr()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def make_corr(df):
    N = len(df)
    cols = df.columns
    return [[((df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2))/(df[x].std() * df[y].std()) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;pprint (make_corr(addmissions))&lt;/p&gt;
&lt;p&gt;[[0.99990902474526921, 0.74220186205662952, -0.0063224730309758576],
 [0.74220186205662952, 0.99990902474528243, -0.0091340229188836969],
 [-0.0063224730309758576, -0.0091340229188836969, 0.99134834685640671]]&lt;/p&gt;
&lt;p&gt;addmissions.corr()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;You should identify &lt;code&gt;family_income&lt;/code&gt; as being the most correlated with GPA. The university wants to make
   an effort to make sure people of all family income are being fairly represented in the admissions process.
   In order to achieve that, different GPA thresholds will be set according to family income. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The low, medium and high family income groups are &lt;code&gt;0 to 26832&lt;/code&gt;, &lt;code&gt;26833 to 37510&lt;/code&gt; and &lt;code&gt;37511 to 51112&lt;/code&gt; respectively. 
   Implement a function that would plot the distribution of GPA scores for each family income category. These are the 
   conditional probability distributions of &lt;code&gt;gpa&lt;/code&gt; given certain levels of &lt;code&gt;family_income&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_hist(df):
    low = df[df.family_income &amp;lt;= 26832]
    med = df[(df.family_income &amp;gt; 26832) &amp;amp; (df.family_income &amp;lt;= 37519)]
    high = df[(df.family_income &amp;gt; 37519) &amp;amp; (df.family_income &amp;lt;= 51112)]
    low.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;blue&amp;quot;,label=&amp;#39;Low Income&amp;#39;)
    med.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;green&amp;quot;,label=&amp;#39;Medium Income&amp;#39;)
    high.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;red&amp;quot;,label=&amp;#39;High Income&amp;#39;)
    plt.xlim([2.0, 4.0])
    plt.legend()
    plt.show()

make_hist(addmissions)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_47_0.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the university decides to accept students with GPA above the 90th percentile within the respective family 
   income categories, what are the GPA thresholds for each of the categories?&lt;/p&gt;
&lt;p&gt;low = addmissions[addmissions.family_income &amp;lt;= 26832]
med = addmissions[(addmissions.family_income &amp;gt; 26832) &amp;amp; (addmissions.family_income &amp;lt;= 37519)]
high = addmissions[(addmissions.family_income &amp;gt; 37519) &amp;amp; (addmissions.family_income &amp;lt;= 51112)]
print "Low 90th Percentile", low.gpa.quantile(.9)
print "Medium 90th Percentile", med.gpa.quantile(.9)
print "High 90th Percentile", high.gpa.quantile(.9)&lt;/p&gt;
&lt;p&gt;Low 90th Percentile 3.01
Medium 90th Percentile 3.26
High 90th Percentile 3.36&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pearson Correlation vs Spearman Correlation&lt;/h2&gt;
&lt;p&gt;The Pearson correlation evaluates the linear relationship between two continuous 
variables. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables
without assuming linearity of the variables. Spearman correlation is often more robust in capturing non-linear relationship
between variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In addition to the &lt;code&gt;family_income&lt;/code&gt; and &lt;code&gt;parent_avg_age&lt;/code&gt;, you are also given data about the number of hours the 
   students studied. Load the new data in from &lt;code&gt;data/admissions_with_study_hrs_and_sports.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;studydf = pd.read_csv('../probability/data/admissions_with_study_hrs_and_sports.csv')
studydf.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;family_income_cat&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;th&gt;hrs_studied&lt;/th&gt;
      &lt;th&gt;sport_performance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;49.463745&lt;/td&gt;
      &lt;td&gt;0.033196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;16.414467&lt;/td&gt;
      &lt;td&gt;0.000317&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;4.937079&lt;/td&gt;
      &lt;td&gt;0.021845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;160.210286&lt;/td&gt;
      &lt;td&gt;0.153819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;36.417860&lt;/td&gt;
      &lt;td&gt;0.010444&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make a scatter plot of the &lt;code&gt;gpa&lt;/code&gt; against &lt;code&gt;hrs_studied&lt;/code&gt;. Make the points more transperant so you can see the density
   of the points. Use the following command get the slope and intercept of a straight line to fit the data.&lt;/p&gt;
&lt;p&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.hrs_studied)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept&lt;/p&gt;
&lt;p&gt;studydf.plot(kind='scatter',x='gpa',y='hrs_studied',alpha=0.01)
plt.plot(x,y,color='red')
plt.xlabel("GPA")
plt.ylabel("Hours Studied")
plt.show()&lt;/p&gt;
&lt;p&gt;494.329335528 -1400.63719543 0.475940264662 0.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_53_1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the functions &lt;code&gt;scipy.stats.pearsonr&lt;/code&gt; and &lt;code&gt;scipy.stats.spearmanr&lt;/code&gt; to compute the Pearson and Spearman correlation&lt;/p&gt;
&lt;p&gt;print sc.pearsonr(studydf.gpa,studydf.hrs_studied)
print sc.spearmanr(studydf.gpa,studydf.hrs_studied)&lt;/p&gt;
&lt;p&gt;(0.47594026466220946, 0.0)
(0.98495916559333341, 0.0)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat step &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; for &lt;code&gt;gpa&lt;/code&gt; and &lt;code&gt;sport_performance&lt;/code&gt;. Is there a strong relationship between the two variables?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept

studydf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()

0.00979813693421 0.0585103217504 0.0238485969548 0.0124044928587
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_57_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print sc.pearsonr(studydf.gpa,studydf.sport_performance)
print sc.spearmanr(studydf.gpa,studydf.sport_performance)

(0.023848596954761905, 0.012404492858691094)
(0.0022881402736224248, 0.81043264616449484)



temp = studydf[studydf.gpa &amp;gt; 3.0]
slope, intercept, r_value, p_value, std_err = sc.linregress(temp.gpa,temp.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(temp.gpa.min(),temp.gpa.max(),100)
y = slope*x+intercept

temp.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()
print sc.pearsonr(temp.gpa,temp.sport_performance)
print sc.spearmanr(temp.gpa,temp.sport_performance)

0.65608660543 -2.03523616554 0.945140987506 0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_59_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.94514098750633013, 0.0)
(1.0, 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Part 4: Distribution Simulation&lt;/h2&gt;
&lt;p&gt;Often times in real life applications, we can specify the values of a variable to be of a particular distribution,
for example the number of sales made in the next month can be modeled as a uniform distribution over the range of
5000 and 6000.&lt;/p&gt;
&lt;p&gt;In this scenario, we are modeling &lt;code&gt;profit&lt;/code&gt; as a product of &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt;,
where &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt; can be modeled as probabilistic distributions.
By randomly drawing values from these distributions, we are able to get a distribution of the range of &lt;code&gt;profit&lt;/code&gt; 
based on the uncertainties in the other variables.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Profit = Number of views * Conversion * Profit per sale&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Assumptions:
- &lt;code&gt;Number of views&lt;/code&gt; is a uniform distribution over the range of 5000 and 6000
- &lt;code&gt;Conversion is a binomial distribution where the probability of success is&lt;/code&gt;0.12&lt;code&gt;for each sale among the&lt;/code&gt;Number on views made 
- &lt;code&gt;Profit per sale&lt;/code&gt; has &lt;code&gt;0.2&lt;/code&gt; probability of taking the value &lt;code&gt;50&lt;/code&gt; (for wholesale) and &lt;code&gt;0.8&lt;/code&gt; of 
  taking the value &lt;code&gt;60&lt;/code&gt; (non-wholesale)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given the distributions of each of variables, use scipy to write a function that would draw random values from each of the distributions to simulate a distribution for &lt;code&gt;profit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def get_profit():
    num_views = np.round(sc.uniform.rvs(loc=5000,scale=1000,size=1),0)
    conversions = sc.binom.rvs(num_views,0.12)
    wholesale = sc.binom.rvs(conversions,0.2)
    return wholesale&lt;em&gt;50+(conversions-wholesale)&lt;/em&gt;60&lt;/p&gt;
&lt;p&gt;profits = np.array([get_profit() for i in xrange(100000)])
plt.hist(profits)
plt.show()
print "Low: ",np.percentile(profits,2.5)
print "High: ",np.percentile(profits,97.5)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_61_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Low&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;33800.0&lt;/span&gt;
&lt;span class="n"&gt;High&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;42920.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-05/" rel="alternate"></link><updated>2015-06-05T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-05:galvanize/galvanize-data-science-01-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 5&lt;/h2&gt;
&lt;p&gt;This morning we started with a reflection about the week, and completed a survey about our progress and thoughts on the program.   I think it is a very strong, hands-on program so far.&lt;/p&gt;
&lt;p&gt;The morning lesson and sprint was on using (pandas)[http://pandas.pydata.org/].   We were given some hospital data in a CSV format, read in the data, and answered a number of questions about most common diseases, most expensive procedures, the post profitable hospitals, and various subsets of these question on different conditions.  It was a simple exercise that gave us practice making new variables, grouping, and subsetting to look massage the data into a form that allowed us to answer the questions.&lt;/p&gt;
&lt;p&gt;During our lunch we had a presentation on learning, and the approach and attitudes that facilitate the bests learning.  It was partly motivational and partly reflective.  If you are familiar with Carol Dweck's work and the research it created, then you have a feeling for the talk.&lt;/p&gt;
&lt;p&gt;After lunch we did a fun assignment that was to recreate the MoneyBall movie where we are trying to get a set of three players that in aggregate replace the 3 key players that were just lost.   &lt;/p&gt;
&lt;p&gt;The data we used is hosted (here)[http://www.seanlahman.com/baseball-archive/statistics/]&lt;/p&gt;
&lt;h2&gt;Money Ball&lt;/h2&gt;
&lt;p&gt;We first started by downloading the dataset and loading it into Pandas.  We started with the batting data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;batting&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/baseball-csvs/Batting.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SB&lt;/th&gt;
      &lt;th&gt;CS&lt;/th&gt;
      &lt;th&gt;BB&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SFN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BOS&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We then loaded the salaryd data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;data/baseball-csvs/Salaries.csv&amp;#39;)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;murraed02&lt;/td&gt;
      &lt;td&gt;1472819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lynnfr01&lt;/td&gt;
      &lt;td&gt;1090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;ripkeca01&lt;/td&gt;
      &lt;td&gt;800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lacyle01&lt;/td&gt;
      &lt;td&gt;725000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;flanami01&lt;/td&gt;
      &lt;td&gt;641667&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary[salary.yearID==2001].salary.describe()




count         860.000000
mean      2279841.061628
std       2907710.250521
min        200000.000000
25%        269375.000000
50%        925000.000000
75%       3250000.000000
max      22000000.000000
Name: salary, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the year 2001, the year we are concerned with, the minimum salary was $200,000.&lt;/p&gt;
&lt;p&gt;The next thing we did was merged the two dataframes and limited the data to 2001.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf = batting.merge(salary,on=[&amp;#39;playerID&amp;#39;,&amp;#39;yearID&amp;#39;],how=&amp;#39;left&amp;#39;)
mergeddf = mergeddf[mergeddf.yearID==2001]
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can see some of the salaries are missing.  There are players that can be aquired, but are not on a payroll.   If we pick them up we have to pay them 200,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.salary = mergeddf.salary.fillna(200000)
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now we need to make some new variables.   The number of times they were on First Base,the Batting Average (BA), the On Base Percentage (OBP), and the Slugg (SLG)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf[&amp;#39;BA&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf.BA.describe()




count    1044.000000
mean        0.202532
std         0.140697
min         0.000000
25%         0.117647
50%         0.235227
75%         0.274705
max         1.000000
Name: BA, dtype: float64




mergeddf[&amp;#39;1B&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]-mergeddf[&amp;#39;2B&amp;#39;]-mergeddf[&amp;#39;3B&amp;#39;]-mergeddf[&amp;#39;HR&amp;#39;]
mergeddf[&amp;#39;1B&amp;#39;].describe()




count    1237.000000
mean       23.185125
std        34.327716
min         0.000000
25%         0.000000
50%         4.000000
75%        33.000000
max       192.000000
Name: 1B, dtype: float64




mergeddf[&amp;#39;SLG&amp;#39;]=(mergeddf[&amp;#39;1B&amp;#39;]+2*mergeddf[&amp;#39;2B&amp;#39;]+3*mergeddf[&amp;#39;3B&amp;#39;] \
                 +4*mergeddf[&amp;#39;HR&amp;#39;])/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf[&amp;#39;SLG&amp;#39;].describe()




count    1044.000000
mean        0.303628
std         0.214569
min         0.000000
25%         0.142857
50%         0.337722
75%         0.436874
max         2.000000
Name: SLG, dtype: float64




mergeddf[&amp;#39;OBP&amp;#39;]=(mergeddf[&amp;#39;H&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]) \
/(mergeddf[&amp;#39;AB&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]+mergeddf[&amp;#39;SF&amp;#39;])
mergeddf[&amp;#39;OBP&amp;#39;].describe()




count    1047.000000
mean        0.254084
std         0.159932
min         0.000000
25%         0.162162
50%         0.293103
75%         0.338235
max         1.000000
Name: OBP, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The A's lost Jason Giambi (&lt;code&gt;giambja01&lt;/code&gt;), Johnny Damon (&lt;code&gt;damonjo01&lt;/code&gt;), Jason Isringhausen (&lt;code&gt;isrinja01&lt;/code&gt;), and Rainer Gustavo "Ray" Olmedo (&lt;code&gt;'saenzol01'&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These player need to replaced with similar player that bat, in total, as much as these guys, get on base as often as these guys, and can be payed less than these guys.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;my_mask = mergeddf[&amp;#39;playerID&amp;#39;].isin([&amp;#39;giambja01&amp;#39;,&amp;#39;damonjo01&amp;#39;,&amp;#39;isrinja01&amp;#39;,&amp;#39;saenzol01&amp;#39;])
lostboysdf = mergeddf[my_mask]
imp_var = [&amp;#39;playerID&amp;#39;, &amp;#39;teamID_x&amp;#39;,&amp;#39;AB&amp;#39;,&amp;#39;HR&amp;#39;, &amp;#39;OBP&amp;#39;, &amp;#39;SLG&amp;#39;, &amp;#39;salary&amp;#39;]
lostboysdf[imp_var]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;HR&lt;/th&gt;
      &lt;th&gt;OBP&lt;/th&gt;
      &lt;th&gt;SLG&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;7065&lt;/th&gt;
      &lt;td&gt;damonjo01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;644&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.323529&lt;/td&gt;
      &lt;td&gt;0.363354&lt;/td&gt;
      &lt;td&gt;7100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10836&lt;/th&gt;
      &lt;td&gt;giambja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;520&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0.476900&lt;/td&gt;
      &lt;td&gt;0.659615&lt;/td&gt;
      &lt;td&gt;4103333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14911&lt;/th&gt;
      &lt;td&gt;isrinja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27408&lt;/th&gt;
      &lt;td&gt;saenzol01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.291176&lt;/td&gt;
      &lt;td&gt;0.383607&lt;/td&gt;
      &lt;td&gt;290000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Avg OBP Needed to be replaced:&amp;quot;, 3*lostboysdf[imp_var].OBP.mean()
print &amp;quot;Total Bats needed to be replaced:&amp;quot;, lostboysdf[imp_var].AB.sum()

Avg OBP Needed to be replaced: 1.09160603138
Total Bats needed to be replaced: 1469.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We would ideally like to get every combination of 3 players that are available.  That would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;factorial&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;1335&lt;/span&gt;
&lt;span class="mi"&gt;395654395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is almost 400,000,000 combinations to search through.   Less make some reasonable assumptions about about the minimum At Bats and On Base Percentages&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;AB&amp;#39;,y=&amp;#39;OBP&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1063f3f10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/gw1d5_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the variance does not change much above 200 AB.  We probably want them to bat about 500 times a season, however.  We also want the average to be above 0.33 for the OPB, so that seems like a reasonable initial cutoff.  We also want there average salary to be less than 5000000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;size = len(mergeddf[(mergeddf.AB &amp;gt; 400) &amp;amp; (mergeddf.OBP &amp;gt; 0.33)])
nCr(size,3)




246905L
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us 11,480 combinations to search through.  Lets do it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#mdf = mergeddf[(mergeddf.yearID==2001)&amp;amp;(mergeddf.AB&amp;gt;50)] #.salary.describe()&lt;/span&gt;
&lt;span class="c"&gt;#mdf.salary = mdf.salary.fillna(200000)&lt;/span&gt;
&lt;span class="c"&gt;#mdf[imp_var].head()&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;
&lt;span class="n"&gt;good_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_AB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_OBP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;15000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0961&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;total_AB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;player1&lt;/th&gt;
      &lt;th&gt;player2&lt;/th&gt;
      &lt;th&gt;player3&lt;/th&gt;
      &lt;th&gt;total_AB&lt;/th&gt;
      &lt;th&gt;total_OBP&lt;/th&gt;
      &lt;th&gt;total_salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;28&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1.261767&lt;/td&gt;
      &lt;td&gt;5338333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;32&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1754&lt;/td&gt;
      &lt;td&gt;1.264850&lt;/td&gt;
      &lt;td&gt;5455000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;martied01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1637&lt;/td&gt;
      &lt;td&gt;1.256603&lt;/td&gt;
      &lt;td&gt;6005000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;edmonji01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;1.243410&lt;/td&gt;
      &lt;td&gt;6838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;38&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;olerujo01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1739&lt;/td&gt;
      &lt;td&gt;1.234375&lt;/td&gt;
      &lt;td&gt;7205000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gilesbr02&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1743&lt;/td&gt;
      &lt;td&gt;1.236756&lt;/td&gt;
      &lt;td&gt;7838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;alomaro01&lt;/td&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1742&lt;/td&gt;
      &lt;td&gt;1.247866&lt;/td&gt;
      &lt;td&gt;8255000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;43&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;thomeji01&lt;/td&gt;
      &lt;td&gt;1693&lt;/td&gt;
      &lt;td&gt;1.249345&lt;/td&gt;
      &lt;td&gt;8380000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;55&lt;/th&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1786&lt;/td&gt;
      &lt;td&gt;1.263189&lt;/td&gt;
      &lt;td&gt;9983333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;1773&lt;/td&gt;
      &lt;td&gt;1.290459&lt;/td&gt;
      &lt;td&gt;10088333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print len(gc)

66
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This allowed us to find 66 combination of players that would, in aggregate, have better statistics that the players that were lost.  It also turns out to be cheaper to do that.   This is the story of money ball.  It was a fun project.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-04/" rel="alternate"></link><updated>2015-06-04T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-04:galvanize/galvanize-data-science-01-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 4&lt;/h2&gt;
&lt;p&gt;The day started out with a mini-quiz on object-oriented programming, and that was followed by an introduction to git and sophisticated join queries.   Our instructor for the day used to work at Facebook, and she walked us through some the queries she would do on the job.  &lt;/p&gt;
&lt;p&gt;She then gave us a simulated data set that match the structure, but not the content, of Facebook tables and we had an individual sprint attempting to complete 10 queries in 2 hours.&lt;/p&gt;
&lt;p&gt;After lunch we had a lecture on pyscopg2, a python library to use to connect and interact with a PostgreSQL server.   We ran a server locally, loaded with the same data as the morning, and were given an assignment to construct a pipeline that we could run each day to give us an updated status of our users.   We were to check on results for today being set to Aug 14, 2014. &lt;/p&gt;
&lt;p&gt;Our resulting script is below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;psycopg2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;2014-08-14&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;%Y-%M-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;CREATE TABLE logins_7d_%s AS&lt;/span&gt;
&lt;span class="sd"&gt;    WITH&lt;/span&gt;
&lt;span class="sd"&gt;    main AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        r.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        tmstmp::date AS reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        CASE WHEN optout.userid IS NULL then 0 ELSE 1 END AS opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM registrations r&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN optout&lt;/span&gt;
&lt;span class="sd"&gt;    ON r.userid = optout.userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY r.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid,&lt;/span&gt;
&lt;span class="sd"&gt;        MAX(tmstmp::date) AS last_login&lt;/span&gt;
&lt;span class="sd"&gt;    FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7d&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7m AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT t.userid, COUNT(t.dt) AS logins_7m&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;mobile&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7w AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7w&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;web&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    uf1 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT * FROM friends)&lt;/span&gt;
&lt;span class="sd"&gt;    UNION ALL&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT userid2, userid1 FROM friends)),&lt;/span&gt;
&lt;span class="sd"&gt;    uf2 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT DISTINCT *&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf1),&lt;/span&gt;
&lt;span class="sd"&gt;    friend_cnt AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid1 AS userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(1) AS num_friends&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf2&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid)&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        main.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        last_login,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7d,0) AS logins_7d,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7m,0) AS logins_7d_mobile,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7w,0) AS logins_7d_web,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(num_friends,0) AS num_friends,&lt;/span&gt;
&lt;span class="sd"&gt;        opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM main&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7m&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7m.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7w&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7w.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN friend_cnt&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = friend_cnt.userid;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;timestamp&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;commit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also learned how pull data and load the data into a pandas dataframe.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sql&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sql&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;SELECT * FROM logins_7d_1389686880 LIMIT 20;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;userid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coerce_float&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reg_date&lt;/th&gt;
      &lt;th&gt;last_login&lt;/th&gt;
      &lt;th&gt;logins_7d&lt;/th&gt;
      &lt;th&gt;logins_7d_mobile&lt;/th&gt;
      &lt;th&gt;logins_7d_web&lt;/th&gt;
      &lt;th&gt;num_friends&lt;/th&gt;
      &lt;th&gt;opt_out&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;userid&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2014-06-23&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-12-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2014-04-18&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-12-17&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2014-08-09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2013-08-18&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2014-03-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2014-05-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2014-06-06&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2013-08-16&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2013-09-12&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2014-07-29&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2013-11-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2013-10-09&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2014-02-16&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;2014-04-20&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;2014-07-02&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;2014-05-10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Today was a very intense day.   But the programming is delivering on what it promised: Hands On Learning From Experience Professions!&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category><category term="psycopg2"></category></entry><entry><title>Galvanize - Week 01 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-03/" rel="alternate"></link><updated>2015-06-03T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-03:galvanize/galvanize-data-science-01-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 3&lt;/h2&gt;
&lt;p&gt;Today was an 'introduction' to SQL and PostgreSQL.  I put introduction in quotes because it does not properly describe what we did.  The pre-reading was to complete all 9 (1-9) tutorials on &lt;a href="http://sqlzoo.net/"&gt;SQLZoo&lt;/a&gt;.  This took me about 5 hours.   During lecture we have a review of the order of operation of SQL queries, as well as a detailed explanation of joins.    &lt;/p&gt;
&lt;p&gt;The sprint for the day involved install &lt;a href="http://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; locally, loading a database into it, then completing ~25 basic and 10 advance (extra credit) queries.   Our database had 3 tables with 300k, 500k, and 5k entries respectively.&lt;/p&gt;
&lt;p&gt;These were a great set of assignment because of how they 'leveled-up'.  Even the few among us that were sophisticated with SQL had difficulty with the advance problems.&lt;/p&gt;
&lt;p&gt;Now that I have completed ~10 hours of SQL queries today, I am going to end this post now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT * 
FROM bryan JOIN bed 
ON bryan.location=bed.location 
AND bryan.state=&amp;#39;sleep&amp;#39; 
AND bed.state=&amp;#39;comfy&amp;#39;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category></entry><entry><title>Galvanize - Week 01 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-02/" rel="alternate"></link><updated>2015-06-02T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-02:galvanize/galvanize-data-science-01-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 2&lt;/h2&gt;
&lt;p&gt;Today was he first 'regular' day in the program.  I showed up about 90 minutes before the mini-quiz to review the readings for the day's lecture on Object Oriented Programming (OOP).   At 9:30 we started the mini-quiz on SQL statements and results.  I found it rather simple.  We were given 30 minutes to complete it, and I finished in about 10 minutes.  Most the topics involved analogs in pandas that I am familiar with, so I think that's why I finished rather quickly.&lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;We had two lectures today.  The first lecture was on object oriented structures, and how to implement them in python.  The afternoon lecture was on scoping in python, and a little bit of debugging.  We were introduced to pdb, but told that the use of debuggers is not well integrated in the data science community.&lt;/p&gt;
&lt;h3&gt;LEGB&lt;/h3&gt;
&lt;p&gt;We were told the variables are looked for in the order of local, enclosing function, global, and python build-in.   I made a set of functions to try to illustrate it for myself.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;locally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;built&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Paired Programming Sprint&lt;/h2&gt;
&lt;p&gt;Today's project involved programming a text based game of black jack with a dealer and 1 number of players.  We also had the extra credit options adding n-players, AI/Bot players, double down, and split.  I am happy to report that we that my partner and I were able to complete the first three, but ran out of time before implementing split.&lt;/p&gt;
&lt;p&gt;We started off with pencil and paper using the noun,verb method of abstraction.   We settled on making a Deck, a Player, and Hand, and Game, and an AI.    Its clear at the end that we should have abstracted the game more, and given the Hand class more responsibilities to best implement the split method.&lt;/p&gt;
&lt;p&gt;After we finished we had our dealer hit until 17 or above, while our AI bot hit until soft 17 below.   We also had it implement a doubling bettering strategy.   In our sample, the AI agent one more often then not came out ahead from this setup.   It added a little credence to the ways dealer's seem to play in Las Vegas.&lt;/p&gt;
&lt;p&gt;The repo is currently private, because it could be a project for future cohorts.   I do not want to make a copy public, but will if they give permission.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry><entry><title>Galvanize - Week 01 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-01/" rel="alternate"></link><updated>2015-06-01T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-01:galvanize/galvanize-data-science-01-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 1&lt;/h2&gt;
&lt;p&gt;Today is my first day attending Galvanize's Immersive Data Science Program in San Francisco, CA.   The program is a 12 week program that is approximately 10 hours a day of learning and activities to reinforce and refine the learning.   I am very excited to be a part of this program.&lt;/p&gt;
&lt;h2&gt;My Background&lt;/h2&gt;
&lt;p&gt;I have a Ph.d in Theoretical High Energy Particle Physics and Cosmology, earned a Data Analysis Nano-degree from Udacity.com, and also am current working on a M.S. in Computer Science from GA Tech.    I have also spent the last 8 years teaching high school physics and robotics.   &lt;/p&gt;
&lt;p&gt;I will likely have some strength with math and theory, but I have no doubt that my programming will significantly improve over the next 12 weeks.   Everyone in the program is well educated and intelligent, and each one of them have strengths in some areas and room for improvements in others.  It seems to be a strength for this program.   No matter your weakness, there are students that have that as a strength.&lt;/p&gt;
&lt;h2&gt;Summary of the Day&lt;/h2&gt;
&lt;p&gt;The first half of the day is getting to know our instructors, hour cohort, and the amazingly nice galvanize complex.  It is a 5 story building filled with startup companies, work spaces, galvanize students, and other tech visitors.   I found this to be an impressive building.&lt;/p&gt;
&lt;p&gt;After about an hour of getting to know everyone, we were given a presentation.  That was followed by a tour of the galvanize building.  We were then given an assessment on the pre-course material that was given to us before we showed up.&lt;/p&gt;
&lt;p&gt;After lunch, we had an 90 minute lecture, then worked on a paired sprint assignment for about 3 hours.   This assignment involved...&lt;/p&gt;
&lt;p&gt;After we finished the sprint, we were invited to a Galvanize happy hour to socialize over beer and wine.   I feel very lucky to be apart of this program, and have been impressed with my cohort, the instructors, and Galvanize.  &lt;/p&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;The pre-course material required us to complete material on the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Numpy/Pandas&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Web Awareness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The initial assessment we were given was a 120 minute test on the first 5 topics.  It was an 'open book' test, but that does not mean it was easy.  A majority of my peers did not finish within the allotted time.   &lt;/p&gt;
&lt;p&gt;I am not going to post details on the test because I would hate to ruin the thrill of discovery for potential future students.&lt;/p&gt;
&lt;h2&gt;LUNCH!!!!&lt;/h2&gt;
&lt;p&gt;They provide us a lunch on the first day, but most days we have an 75 minute break for lunch.  There are kitchen, fridges, storage for us to use if we wish.   The lunch was nice, from a local Thai place.   &lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;The lecture, in my opinion, was a little redundant with the course material.  It seemed structure under the assumption that you didn't read or review the python pre-course materials.  I understand its important that everyone is on the same starting point, but I wish we got to jump in a little deeper.&lt;/p&gt;
&lt;p&gt;I did learn and see the importance of using generators when possible.   It save both memory and time.   &lt;/p&gt;
&lt;h2&gt;Paired programming&lt;/h2&gt;
&lt;p&gt;After the lecture we grouped up for a paired programming assignment.   We trade off roles of being the driver and the navigator in 20 to 30 minute rotations for a 3 hour block of programming.  We start of by forking the day's assignment from a Github repo and cloning it locally.  Today we then worked two projects.  The first project was completing a list of functions based on a description of the function, including inputs and outputs.  The second project was fixing inefficiently running code.&lt;/p&gt;
&lt;p&gt;A simple example is checking if a key is in a dictionary.   Before today I might have checked to see if it was in the keys() results, but we can see that for medium size dictionaries that it is almost 40x slower than just using in in the dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;randint&lt;/span&gt;

&lt;span class="n"&gt;cnt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;

&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.17&lt;/span&gt; &lt;span class="err"&gt;µ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="mi"&gt;10000000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;111&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We saw similar results for iter compared to iteritems, range to xrange, and izip to zip.   It was a useful assignment for the content and the practice of collaborating with someone else.&lt;/p&gt;
&lt;h2&gt;After reception&lt;/h2&gt;
&lt;p&gt;Galvanize SF now runs two cohorts 6 weeks apart.  We had a mixer with previous cohort, enjoying beer, wine, and conversation on the roof of the building.  &lt;/p&gt;
&lt;p&gt;After 11 hours at Galvanize, I decided it was time to head home.   Definitely looking forward to day 2. &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry></feed>