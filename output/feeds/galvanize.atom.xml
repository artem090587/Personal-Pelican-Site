<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bryan Travis Smith, Ph.D</title><link href="http://www.bryantravissmith.com/" rel="alternate"></link><link href="http://www.bryantravissmith.com/feeds/galvanize.atom.xml" rel="self"></link><id>http://www.bryantravissmith.com/</id><updated>2015-06-12T10:30:00-07:00</updated><entry><title>Galvanize - Week 02 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-05/" rel="alternate"></link><updated>2015-06-12T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-12:galvanize/galvanize-data-science-02-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 5&lt;/h2&gt;
&lt;p&gt;Our more quiz was a survey about our progress in the program.  The morning lecture was about the beta distribution, and its relation to A/B testing from a Bayesian perspective, and a brief introduction to the multi-arm bandit method.   &lt;/p&gt;
&lt;h2&gt;Individual Morning Sprint&lt;/h2&gt;
&lt;p&gt;This morning we had some simulated click through data for two different sites and attempted to answer question about the outcome of the two pages.  We want to find the probability of a click through on the two pages and be able to quantify how much better one site is over the other. }&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/siteA.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/siteB.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Bayesian Analysis&lt;/h2&gt;
&lt;p&gt;We are going to start with a prior that the click through rate of both sites can be anything between 0 and 1.  This will look like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.01,0.01)
y = sc.beta(a=1.,b=1.).pdf(x)

def plot_with_fill(x,y,label,color):
    lines = plt.plot(x,y,label=label,lw=2,color=color)
    plt.fill_between(x,0,y,alpha=0.2,color=color)
    plt.ylim([0,1.2*y.max()])
    plt.legend()


plot_with_fill(x,y,&amp;#39;Beta a=1,b=1&amp;#39;,&amp;#39;seagreen&amp;#39;)
plt.xlabel(&amp;quot;Probability of Click Through&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/http://www.bryantravissmith.com/img/GW02D5/output_3_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we look at 1 value from the click through where the user does not click through, we update our believe:&lt;/p&gt;
&lt;p&gt;$$\mbox{Posterior} = \mbox{Prior} * \mbox{Likelihood}$$&lt;/p&gt;
&lt;p&gt;$$\mbox{Beta}(\alpha=1, \ \beta=2) = \mbox{Beta}(\alpha=1, \ \beta=1) \times (1 - p)$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b1,a1 = 1,0
y1 = sc.beta(a=1,b=1).pdf(x)
y2 = sc.beta(a=a1+1,b=b1+1).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;1 Views&amp;#39;,&amp;#39;lightsalmon&amp;#39;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/http://www.bryantravissmith.com/img/GW02D5/output_5_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we have 50 views, the equation would be:&lt;/p&gt;
&lt;p&gt;$$\mbox{Beta}(\alpha=1+n_{clicks}, \ \beta=1+n_{no clicks}) = \mbox{Beta}(\alpha=1, \ \beta=1) \times (1 - p)^{n_{no clicks}} \times p^{n_{clicks}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b50,a50 = np.bincount(A[:50].astype(int))
y1 = sc.beta(a=1,b=1).pdf(x)
y2 = sc.beta(a=a50+1,b=b50+1).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;50 Views&amp;#39;,&amp;#39;lightsalmon&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_7_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can imagine that we continue getting data, and our believe about the click through rate would evolve:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.001,0.001)
plt.figure(figsize=(15,10))
y1 = sc.beta(a=1.,b=1.).pdf(x)
plot_with_fill(x,y1,&amp;#39;Prior&amp;#39;,&amp;#39;seagreen&amp;#39;)
color_s = {&amp;#39;50&amp;#39;:&amp;#39;lightsalmon&amp;#39;,&amp;#39;100&amp;#39;:&amp;#39;aquamarine&amp;#39;,&amp;#39;200&amp;#39;:&amp;#39;turquoise&amp;#39;,&amp;#39;400&amp;#39;:&amp;#39;NavajoWhite&amp;#39;,&amp;#39;800&amp;#39;:&amp;#39;forestgreen&amp;#39;}
for count in [50,100,200,400,800]:
    bc,ac = np.bincount(A[:count].astype(int))
    y = sc.beta(a=ac,b=bc).pdf(x)
    plot_with_fill(x,y,str(count)+&amp;#39; Views&amp;#39;,color_s[str(count)])
plt.xlim([0,.2])




(0, 0.2)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_9_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that as we increase the amount of data we have, we have a more specific believe about the click through rate of site A.  &lt;strong&gt;This is different from a hypthesis test.  It does not required a fix sample size or fixed amount of time&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;We can look at the two sites for all the data.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x=np.arange(0,1.001,0.001)
plt.figure(figsize=(15,10))
bA,aA = np.bincount(A.astype(int))
bB,aB = np.bincount(B.astype(int))
y1 = sc.beta(a=float(aA),b=float(bA)).pdf(x)
y2 = sc.beta(a=float(aB),b=float(bB)).pdf(x)
plot_with_fill(x,y1,&amp;#39;A Site&amp;#39;,&amp;#39;seagreen&amp;#39;)
plot_with_fill(x,y2,&amp;#39;B Site&amp;#39;,&amp;#39;aquamarine&amp;#39;)
plt.xlim([0,.2])
plt.ylim([0,50])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_11_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We now want to determine, given these distributions, what is the probability that site B is better than site A.  We can take random variables from both distributions and count the number of times that the random value from B is greater than the random variable from A.   We can do this 10,000 times.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rA = sc.beta(a=aA,b=bA).rvs(size=10000)
rB = sc.beta(a=aB,b=bB).rvs(size=10000)
nLess,nMore = np.bincount((rA&amp;lt;rB).astype(int))

plt.figure()
ps = []
for i in range(1000):
    rA = sc.beta(a=aA,b=bA).rvs(size=10000)
    rB = sc.beta(a=aB,b=bB).rvs(size=10000)
    nLess,nMore = np.bincount((rA&amp;lt;rB).astype(int))
    ps.append(nMore/10000.)

plt.hist(ps,bins=15,color=&amp;#39;aquamarine&amp;#39;,alpha=0.5)
plt.show()
print &amp;quot;Mean Prob: &amp;quot;, nMore/10000.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_13_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Mean Prob:  0.9971
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Based on our current belief based on the data, the chance that sight B is better than site A is 99.7%.  &lt;/p&gt;
&lt;p&gt;We can also estimate the Bayesian equivalant of a confidence interval.&lt;/p&gt;
&lt;p&gt;This is the centeral credible region - range of the 2.5 precentile and the 97.5 percentile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rA = sc.beta(a=aA,b=bA).rvs(size=10000)
HDI_A = (np.percentile(rA,2.5),np.percentile(rA,97.5))
rB = sc.beta(a=aB,b=bB).rvs(size=10000)
HDI_B = (np.percentile(rB,2.5),np.percentile(rB,97.5))
print &amp;quot;Site A: &amp;quot;, HDI_A
print &amp;quot;Site B: &amp;quot;, HDI_B

Site A:  (0.05002097381711422, 0.084804921163330979)
Site B:  (0.082330727908278764, 0.12425696702783418)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;An 95% highest density interval (HDI) is the most dense interval of a posterior distribution containing X% of its mass. It is analagous to frequentist analysis's confidence intervals.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def hdi_beta(data,percent=0.95):
    bD,aD = np.bincount(data.astype(int))
    x_max = sc.beta(a=aD,b=bD).ppf(1 - percent - 1e-6)
    x_high_max = sc.beta(a=aD,b=bD).ppf(percent - 1e-6)
    vals = np.linspace(0,x_max,1000)
    p_vals = 0.95+sc.beta(a=aD,b=bD).cdf(vals)
    x_high = sc.beta(a=aD,b=bD).ppf(p_vals)
    width = x_high-vals
    low = vals[np.argmin(width)]
    high = low+width.min()
    return (low,high)

print &amp;quot;HDI A:&amp;quot;, hdi_beta(A)
print &amp;quot;HDI B:&amp;quot;, hdi_beta(B)


HDI A: (0.049391799061910255, 0.083668677250890555)
HDI B: (0.081861282294831139, 0.12374716027178045)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These HDI are close to the central credibility region, but they are systematically lower.&lt;/p&gt;
&lt;p&gt;What is nicse about Baysina inference is we can ask what is the probability that site B is 2 percentage points better than A.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nLess2,nMore2 = np.bincount((rB&amp;gt;(rA+0.02)).astype(int))
print &amp;quot;Probabilty B &amp;gt; A + 0.02:  &amp;quot;, nMore2/10000.

plt.figure()
plt.hist((rB-rA),bins=30,color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.show()

Probabilty B &amp;gt; A + 0.02:   0.881
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_19_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case we can see that the difference is near zero (really 0.02), but is likely to be higher.   The probabiliy given by the simulation is 88.1%. &lt;/p&gt;
&lt;p&gt;For a sanity check we can do a hypothesis test.  We would perform a 1 sided Hypthesiss test:&lt;/p&gt;
&lt;p&gt;H0: The mean clickthrough rate is the same.&lt;br /&gt;
HA: The mean clicktrhough rate for site B is greater than site A.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mDiff = rB.mean()-rA.mean()
se = np.sqrt(rB.var()/(len(rB)-1)+rA.var()/(len(rA)-1))
Z = mDiff/se
print mDiff,se,Z
print &amp;quot;p-value: &amp;quot;,1-sc.norm.cdf(Z)
print &amp;quot;The mean difference is statistically significant&amp;quot;


0.0363429336139 0.000138288570828 262.805041634
p-value:  0.0
The mean difference is statistically significant
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now told that there is a business model for this website, and it should inform our decision to implement the new site:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;* the average click on site A yields $1.00 in profit
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the average click on site B yields $1.05 in profit  &lt;/p&gt;
&lt;p&gt;gain_per_click = (rB.sum()*1.05-rA.sum())/10000.
gain_per_click&lt;/p&gt;
&lt;p&gt;0.041471285947814275&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an average gain of 4 cents per click increase in revenume.  Roughly 3 cents from the increase in click through rate and 1.4 cent from the increase in yield.  &lt;/p&gt;
&lt;p&gt;We are not told over what time the experiment we analysed took place over.  A site with 10 Million users per month will see an increase revenue of 400,000 dollars per month, while a site of 10,000 users per month will only see an increase of of 400 dollars per month.  The business decision of investing money and time depends on the context.  This could be a great boom for the company, or a waste of time, depending on the situation.  &lt;/p&gt;
&lt;h2&gt;Multi-arm Bandit&lt;/h2&gt;
&lt;p&gt;The multi-arm bandit approach is a method of balancing the exploration of strategies against the explortation of the best strategy.  We impleted a multi-arm bandit class with a number of strategies:  Random Choice, Max Mean, Epsilon Greedy, Soft Max, UCB1, Bayesian, and Annealing.  &lt;/p&gt;
&lt;p&gt;We have two ways to measure the success of a search strategy for now.  The regret:&lt;/p&gt;
&lt;p&gt;$$\mbox{Regret} = N \ p_{optimal} - \Sigma_{i=1}^{N} p_{i} $$&lt;/p&gt;
&lt;p&gt;And the ratio of optimal decision:&lt;/p&gt;
&lt;p&gt;$$ \frac{N_{optimal}}{N}$$&lt;/p&gt;
&lt;p&gt;The bandit class takes an array of deciion options where each value is the probability of success.  The spirit of this is examining multiple metrics at the same time: like conversion rates across multiple simulatenous tests.&lt;/p&gt;
&lt;h2&gt;Max Mean&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bandits&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;banditstrategy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;prettyplotlib&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ppl&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;bandits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BanditStrategy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_choice&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_bandits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ppl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regrets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Random Choice&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;bandits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bandits&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BanditStrategy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_bandits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ppl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;strat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regrets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Max Mean&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_26_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The max mean, on average, performs better than a random choice.  The issue with max mean is that its very exploitive.   When there is a statisitcal fluxtuation in makes it look like a worse option is the best option, it will stick with it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure(figsize=(14,10))
for i in range(10):
    bandits = Bandits([0.05, 0.03, 0.06])
    strat = BanditStrategy(bandits, random_choice)
    strat.sample_bandits(1000)
    ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

    bandits = Bandits([0.05, 0.03, 0.06])
    strat = BanditStrategy(bandits, max_mean)
    strat.sample_bandits(1000)
    ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)

plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_28_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This is best illustrated if we look at the percent of the time that the optimal solution is chosen.   The max-mean strategy will often find one value and stay with it.&lt;/p&gt;
&lt;p&gt;We can explore this method on three different array options:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;#39;&amp;#39;&amp;#39;
One Stand Out Best = [0.1, 0.1, 0.1, 0.1, 0.9]
One Small Best = [0.1, 0.1, 0.1, 0.1, 0.12]
A clear rank = [0.1, 0.2, 0.3, 0.4, 0.5]
&amp;#39;&amp;#39;&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lets look at one &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_multi_plots(size,func):
    values = [[0.1, 0.1, 0.1, 0.1, 0.9],[0.1, 0.1, 0.1, 0.1, 0.12],[0.1, 0.2, 0.3, 0.4, 0.5]]
    titles = [&amp;#39;One Stand Out Best&amp;#39;,&amp;#39;One Slightly Best&amp;#39;, &amp;quot;A clear rank&amp;quot;]
    for v,t in zip(values,titles): 
        plt.figure(figsize=(14,5))
        plt.subplot(1,2,1)
        for i in range(10):
            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, random_choice)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.regrets,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, func)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.regrets,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)
        plt.title(&amp;#39;Regret - &amp;#39; + t)

        plt.subplot(1,2,2)
        for i in range(10):
            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, random_choice)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Random Choice&amp;#39;,color=&amp;#39;steelblue&amp;#39;,alpha=0.4,lw=3)

            bandits = Bandits(v[:])
            strat = BanditStrategy(bandits, func)
            strat.sample_bandits(size)
            ppl.plot(range(strat.N),strat.percentOpt,label=&amp;#39;Max Mean&amp;#39;+str(i),alpha=0.4,color=&amp;#39;indianred&amp;#39;,lw=3)
        plt.title(&amp;#39;Percent Optimal - &amp;#39; + t)
        plt.show()

make_multi_plots(1000,max_mean)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_30_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Max-Mean appear to work well in cases where there is a single clear best option, but does not consistently find optimal results in the other options.&lt;/p&gt;
&lt;h2&gt;Epsilon Greedy&lt;/h2&gt;
&lt;p&gt;The epsilon greed strategy is a alternative to the max mean where some fraction of the time, say 10%, it will explore instead of exploit.   Well look at it for the same cases as the max mean method.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,epsilon_greedy)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_32_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This strategy has the benefit that if it does start to exploit a sub-optimal strategy, it will eventually find its way out.   This is clear in the "Slightly Best" plots where as the number of trials increase, bandits are leaving the sub-optimal strategy.  &lt;/p&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;p&gt;The soft max is another strategy that balances exploitation vs exploration by use of a parameter.   It has an anology to temperature is method of statistical mechanics.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x: softmax(x,tau=0.5)
make_multi_plots(1000,func)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_34_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The tau parameter of 0.5, in this example, seems to provide random solutions.   If we reduce the size of tau to 0.005, we should get solutions close to max mean.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x: softmax(x,tau=0.005)
make_multi_plots(1000,func)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_36_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Inbetween, the soft max is to reduce the initial effect of outliers, but on these arrays we get less then optimal soltuions.&lt;/p&gt;
&lt;h2&gt;UCB1&lt;/h2&gt;
&lt;p&gt;The UCB1 picks the strategy based on the number of trials and its success rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,ucb1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_38_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This method worked great on one standout best, but behaved randomly on the other options&lt;/p&gt;
&lt;h2&gt;Bayesian Bandit&lt;/h2&gt;
&lt;p&gt;This method uses the ration of success and failures in a trial to pick options based on the beta distribution.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;make_multi_plots(1000,bayesian_bandit)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_40_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;This method wored well on a clear best and eventually found the optimal solution for a ranked system.  The slight best behaved randomly.&lt;/p&gt;
&lt;h2&gt;Annealing&lt;/h2&gt;
&lt;p&gt;This method is working with a softmax that starts with a 'hot' system that explores randomly and cools to a 'cold' system that exploits.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func = lambda x:annealing(x,discount=0.90,tau=0.5)
make_multi_plots(1000,annealing)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_42_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see evidence of the cooling because the random values are starting to move away from random, but we would need to see more trials to see the evenatual discovery of optimal solutions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;color_scheme = [(227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),(188, 189, 34),(23, 190, 207)] # (219, 219, 141)]#, (]
color_scheme = [ (x/255.,y/255.,z/255.) for x,y,z in color_scheme]

func1 = lambda x: softmax(x,tau=0.5)
strategies = [max_mean,random_choice,epsilon_greedy,func1,ucb1,bayesian_bandit]

size=10000
values = [[0.1, 0.1, 0.1, 0.1, 0.9],[0.1, 0.1, 0.1, 0.1, 0.12],[0.1, 0.2, 0.3, 0.4, 0.5]]
titles = [&amp;#39;One Stand Out Best&amp;#39;,&amp;#39;One Slightly Best&amp;#39;, &amp;quot;A clear rank&amp;quot;]
for v,t in zip(values,titles): 
    plt.figure(figsize=(14,5))
    for color,func in zip(color_scheme,strategies):
        plt.subplot(1,2,1)
        bandits = Bandits(v[:])
        strat = BanditStrategy(bandits, func)
        strat.sample_bandits(size)
        ppl.plot(range(strat.N),strat.regrets,label=func.__name__,alpha=1,color=color,lw=4)
        plt.title(&amp;#39;Regret - &amp;#39; + t)
        plt.legend(loc=2)


        plt.subplot(1,2,2)
        bandits = Bandits(v[:])
        strat = BanditStrategy(bandits, func)
        strat.sample_bandits(size)
        ppl.plot(range(strat.N),strat.percentOpt,label=func.__name__,alpha=1,color=color,lw=4)
        plt.title(&amp;#39;Percent Optimal - &amp;#39; + t)

    plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D5/output_44_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the epsilon greed strategy and bayesian bandit algorithms have the best overall performance and avoid getting stuck in non-optimal solutions.   The soft max and annealing both require turning, so they are not fairly compared.   It is nice to see that both epsilon greed and bayesian bandit solutions offer easy to implment and robust use.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="multi-arm bandit"></category></entry><entry><title>Galvanize - Week 02 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-04/" rel="alternate"></link><updated>2015-06-11T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-11:galvanize/galvanize-data-science-02-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 4&lt;/h2&gt;
&lt;p&gt;Our morning quiz was fun.  It was the first time that we built off a prevous quiz.  We were required to build a random variable class that used our probability mass function from the day before.   The end result was being able to simulate a random value from any distrubiton able to be defined by a PMF.  &lt;/p&gt;
&lt;p&gt;Our morning lecture was about power and sample size, as was our individual sprint.  The afternoon lecture was on Bayesian Inference, and the afternoon paired sprint was investigating evolving likelihood functions as we gained more information/data.  &lt;/p&gt;
&lt;h2&gt;Power&lt;/h2&gt;
&lt;p&gt;-Suppose you are interested in testing if on average a bottle of coke weighs 20.4 ounces. You have collected
simple random samples of 130 bottles of coke and weighed them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/coke_weights.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="mi"&gt;130&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;State your null and alternative hypothesis.&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;H0: The mean weight of coke bottles is 20.4 oz&lt;/strong&gt;&lt;br /&gt;
2.&lt;strong&gt;HA: The mean weight of coke bottles is different form 20.4 oz&lt;/strong&gt;     &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the mean and standard error of the sample. State why you are able to apply the Central
   Limit Theorem here to approximate the sample distribution to be normal.&lt;/p&gt;
&lt;p&gt;mean = data.mean()
std = data.std()
se = sc.sem(data)
print "Sample Mean: ", mean
print "Sample STD", std
print "Sample Standard Error",se
print "Sample Size: ", len(data)&lt;/p&gt;
&lt;p&gt;Sample Mean:  20.519861441
Sample STD 0.957682215104
Sample Standard Error 0.084319217426
Sample Size:  130&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;We can use the CLT on this problem because the sample size is 130, much greater than 30.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can make a simulation of the sampling distribution of the null hypthosis, and we can make a sampling distirubiton for another believe, say the true value is the sample man.   If this is true, we can ask questions about how powerful our test is at discovering the mean coke bottle weight is not 20.4, but 20.52.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def power_graph(mu1,std1,n1,mu2,std2,n2,alpha=0.05,two_sided=True):
    x = np.array([sc.norm.rvs(loc=mu1,scale=std1,size=n1).mean() for i in range(10000)])
    y = np.array([sc.norm.rvs(loc=mu2,scale=std2,size=n2).mean() for i in range(10000)])
    plt.figure()
    plt.hist(x,normed=True,color=&amp;#39;lightseagreen&amp;#39;,edgecolor=&amp;#39;lightseagreen&amp;#39;,alpha=0.4,bins=30,label=&amp;quot;Null&amp;quot;)
    plt.hist(y,normed=True,color=&amp;#39;lightsalmon&amp;#39;,edgecolor=&amp;#39;lightsalmon&amp;#39;,alpha=0.4,bins=30,label=&amp;quot;W=20.52&amp;quot;)
    if two_sided:
        x95 = np.percentile(x,100-100*alpha/2.)
    else:
        x95 = np.percentile(x,100-100*alpha)

    plt.axvline(x95,0,10,color=&amp;#39;gray&amp;#39;,lw=2,linestyle=&amp;#39;--&amp;#39;,alpha=0.8)
    plt.legend()
    plt.show()
    print &amp;quot;Value Threshold For Significance: &amp;quot;, x95
    power = len(y[y&amp;gt;x95])/len(y)
    print &amp;quot;Power of Finding True Positive: &amp;quot;,power
    return power

power_graph(20.4,data.std(),130,data.mean(),data.std(),130)
power_graph(20.4,data.std(),130,data.mean(),data.std(),130,two_sided=False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_6_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5657024198
Power of Finding True Positive:  0.2899
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_6_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5387014434
Power of Finding True Positive:  0.4037





0.4037
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These powers are 30% for the two sided, and 42% for the onsided.   It deends if our originaly hypothesis test is a not equal or greater than.   &lt;/p&gt;
&lt;p&gt;We can also do this analytically since we are using the centeral limit.  For a two sided test, we find the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(20.4-4*se,20.4+4*se,100)

y1 = sc.norm.pdf(x,loc=20.4,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.4,scale=se)
y2 = sc.norm.pdf(x,loc=data.mean(),scale=se)
x975 = 20.4+1.96*se
x025 = 20.4-1.95*se
print x025,x975
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.show()

20.235577526 20.5652656662
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_8_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can see that we would not reject the null hypothesis in favor of the alternative because the peak of the blue curve is to the left of the bounding of significance.  The area under the red cuver outside of the black boundaries is 0.05, the signifcance level.   The power of detecing a signficant difference assuming the data's mean is the true value is the blue area.   In this case it is ~ 30%&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The probability of making a type II error (false negative) is called beta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = sc.norm.cdf(x975,loc=data.mean(),scale=se)-sc.norm.cdf(x025,loc=data.mean(),scale=se)
print &amp;quot;Beta (Prob of Type II Error) Assuming data value is the true value&amp;quot;, beta

Beta (Prob of Type II Error) Assuming data value is the true value 0.704503425135
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The power is always 1 minus the false negative rate:&lt;/p&gt;
&lt;p&gt;$$\mbox{Power} = 1 - \beta$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Power: &amp;quot;,1-beta

Power:  0.295496574865
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Statistical power is affected by a number of factors, including the &lt;strong&gt;sample size&lt;/strong&gt;, the &lt;strong&gt;effect size (difference
between the sample statistic and the statistic formulated under the null)&lt;/strong&gt;, and the &lt;strong&gt;significance level&lt;/strong&gt;. Here
we are going to explore the effect of these factors on power.&lt;/p&gt;
&lt;p&gt;If we assuming that we have a different null hypothesis, we can find the power of detecting anther effect size.  Lets stick with the sample data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def explore_power(null_mu,sample_size,effect_size,null_standard_dev,significance_level=0.95,two_sided=True):
    if two_sided:
        critical_z = sc.norm.isf((1-significance_level)/2.)
        se = np.sqrt(null_standard_dev**2/(sample_size-1))
        x025 = null_mu-critical_z*se
        x975 = null_mu+critical_z*se
        alt_mean = null_mu+effect_size
        return (1-np.abs(sc.norm.cdf(x975,loc=alt_mean,scale=se)-sc.norm.cdf(x025,loc=alt_mean,scale=se)))*100
    else:
        critical_z = sc.norm.isf(significance_level)
        se = np.sqrt(null_standard_dev**2/(sample_size-1))
        x95 = null_mu-critical_z*se
        alt_mean = null_mu+effect_size
        return 100*(np.abs(sc.norm.cdf(x95,loc=alt_mean,scale=se)))

print explore_power(20.4,130,data.mean()-20.4,data.std())
explore_power(20.2,130,data.mean()-20.2,data.std())

29.5495708063





96.663546292685481
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The power increased when we assumed the distributions were more different.  This makes sense because the two values are father appart, and the sampling distributions have less overlap.  We can see that by calling power_graph, or calling the analyical functions because we have access to the central limit theorem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;power_graph(20.2,data.std(),130,data.mean(),data.std(),130)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_17_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.364830609
Power of Finding True Positive:  0.9698





0.9698




x = np.linspace(20.2-5*se,20.2+6*se,100)

y1 = sc.norm.pdf(x,loc=20.2,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.2,scale=se)
y2 = sc.norm.pdf(x,loc=data.mean(),scale=se)
x975 = 20.2+1.96*se
x025 = 20.2-1.95*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_18_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;This makes me wonder how the power changes with the effect size.  I image that bigger effects are easier to detect.  It makes good sense&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.plot(np.linspace(0.1,1.2,20),[explore_power(20.2,130,x,data.std()) for x in np.linspace(0.0,1.2,20)],&amp;#39;bo--&amp;#39;,
        color=&amp;#39;lightseagreen&amp;#39;,alpha=0.8,label=&amp;quot;Power&amp;quot;)
plt.ylim([0,110])
plt.xlabel(&amp;quot;Effect Size&amp;quot;)
plt.ylabel(&amp;quot;Power&amp;quot;)
plt.legend(loc=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_20_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The power of a study also depends on the sample size.  As the sample size increases, the standard error decreases by the central limit theorem.   This will make two different distributions overlap less. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;power_graph(20.4,data.std(),130,data.mean(),data.std(),130)
power_graph(20.4,data.std(),1000,data.mean(),data.std(),1000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_22_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.5657528314
Power of Finding True Positive:  0.2857
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_22_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Value Threshold For Significance:  20.4590858498
Power of Finding True Positive:  0.9802





0.9802




data2 = np.loadtxt(&amp;#39;data/coke_weights_1000.txt&amp;#39;)
mean = data2.mean()
std = data2.std()
se = sc.sem(data2)
x = np.linspace(20.4-5*se,20.4+6*se,100)

y1 = sc.norm.pdf(x,loc=20.4,scale=se)
cumy1 = sc.norm.cdf(x,loc=20.4,scale=se)
y2 = sc.norm.pdf(x,loc=data2.mean(),scale=se)
x975 = 20.4+1.96*se
x025 = 20.4-1.95*se

plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;Data&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
beta = sc.norm.cdf(x975,loc=data2.mean(),scale=se)-sc.norm.cdf(x025,loc=data2.mean(),scale=se)
print &amp;quot;Beta (Prob of Type II Error) Assuming data value is the true value&amp;quot;, beta
print &amp;quot;Power:&amp;quot;,1-beta
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_23_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Beta (Prob of Type II Error) Assuming data value is the true value 0.271504963539
Power: 0.728495036461
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can also see how chaning the significance level affects the power of a study.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.plot(np.linspace(0.01,0.3,40),[explore_power(20.4,130,0.01,data.std(),significance_level=x) for x in np.linspace(0.01,0.3,40)],&amp;#39;bo--&amp;#39;,
        color=&amp;#39;lightseagreen&amp;#39;,alpha=0.6,label=&amp;quot;Power&amp;quot;)
plt.ylim([0,110])
plt.xlabel(&amp;quot;Significance Level&amp;quot;)
plt.ylabel(&amp;quot;Power&amp;quot;)
plt.legend(loc=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_25_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Power Calculations for A/B testing&lt;/h2&gt;
&lt;p&gt;We continued yesterday's case study with Esty to find the power needed.  It looked like our Etsy Tuesday Landing Page experiment was under-powered.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;etsy = pd.read_csv(&amp;#39;data/experiment.csv&amp;#39;)
old_data = etsy[etsy[&amp;#39;landing_page&amp;#39;] == &amp;#39;old_page&amp;#39;][&amp;#39;converted&amp;#39;]
new_data = etsy[etsy[&amp;#39;landing_page&amp;#39;] == &amp;#39;new_page&amp;#39;][&amp;#39;converted&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will set up the following hypthesis test.&lt;/p&gt;
&lt;p&gt;X ~ p_new - p_old&lt;/p&gt;
&lt;p&gt;H0: X = 0.001
H1: X &amp;gt; 0.001&lt;/p&gt;
&lt;p&gt;We need to set up the proportions of conversions and find the standard error for this experiment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;po = old_data.mean()
pn = new_data.mean()
p = (old_data.mean()*len(old_data)+new_data.mean()*len(new_data))/(len(old_data)+len(new_data))
se = np.sqrt(p*(1-p)/len(old_data)+p*(1-p)/len(new_data))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can make the same plots as before and compared null to our data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(-5*se,5*se,100)

y1 = sc.norm.pdf(x,loc=0.001,scale=se)
cumy1 = sc.norm.cdf(x,loc=0.001,scale=se)
y2 = sc.norm.pdf(x,loc=(pn-po),scale=se)
x95 = 0.001+1.68*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;New Data&amp;#39;)
plt.axvline(x95,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x95],y2[x&amp;gt;=x95],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x95],y1[x&amp;gt;=x95],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see our data is not statistically different of 0.001, but if we assume the data represents the true mean, we have a very weak test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;critical_z = sc.norm.ppf(0.95)
x95 = 0.001+critical_z*se
1-sc.norm.cdf(x95,loc=(pn-po),scale=se)




0.005391084734824525
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We have a power of less than 1/2% from our data.   &lt;/p&gt;
&lt;p&gt;Increasing the sample size will weaken the power in this case, because our data is less than the null hypthesis, and we are constructing a one sided t-test.  In this case we accept the results and fail to reject the null hypthesis. &lt;/p&gt;
&lt;p&gt;We were told that Etsy decided the pilot is a plausible enough representation of the company's daily  traffic. As a result, Esty decided on a two-tailed test instead, which is as follows:&lt;/p&gt;
&lt;p&gt;```
   X ~ p_new - p_old&lt;/p&gt;
&lt;p&gt;H0: X = 0.001
   H1: X != 0.001
   ```&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(-5*se,5*se,100)

y1 = sc.norm.pdf(x,loc=0.001,scale=se)
cumy1 = sc.norm.cdf(x,loc=0.001,scale=se)
y2 = sc.norm.pdf(x,loc=(pn-po),scale=se)
x025 = 0.001-1.96*se
x975 = 0.001+1.96*se
plt.plot(x,y1,color=&amp;#39;indianred&amp;#39;,label=&amp;#39;Null Hypthesis&amp;#39;)
plt.plot(x,y2,color=&amp;#39;steelblue&amp;#39;,label=&amp;#39;New Data&amp;#39;)
plt.axvline(x025,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.axvline(x975,0,10,color=&amp;#39;black&amp;#39;,lw=3,linestyle=&amp;#39;--&amp;#39;)
plt.fill_between(x[x&amp;gt;=x975],y2[x&amp;gt;=x975],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y2[x&amp;lt;=x025],color=&amp;#39;steelblue&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;gt;=x975],y1[x&amp;gt;=x975],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.fill_between(x[x&amp;lt;=x025],y1[x&amp;lt;=x025],color=&amp;#39;indianred&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_36_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;2-Sided Power: &amp;quot;, sc.norm.cdf(x025,loc=(pn-po),scale=se)+(1-sc.norm.cdf(x975,loc=(pn-po),scale=se))

2-Sided Power:  0.14775925242
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Minimum Sample Size&lt;/h3&gt;
&lt;p&gt;When desigining an experiment, or conducting a test, it is important to get enough data to measure what we are looking for.  In the case of Etsy, its a 1% lift in conversions.  To try to figure out the sample size we might considered our desired false postive and false negative rates, and the Z values associated with them.&lt;/p&gt;
&lt;p&gt;$$\alpha = \mbox{False Positive Rate}, \ Z_{\alpha}$$&lt;/p&gt;
&lt;p&gt;$$\beta = \mbox{False Negative Rate}, \ Z_{\beta}$$&lt;/p&gt;
&lt;p&gt;In an experiment we will have one values, and the Z's will be related to it by the following:&lt;/p&gt;
&lt;p&gt;$$\mu_{exp} = \mbox{Experimental Result}$$&lt;/p&gt;
&lt;p&gt;$$s_{sample} = \mbox{Sampling Erroring}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} = \frac{\mu_{exp} \ - \ \mu_{Null}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\beta} = \frac{\mu_{exp} \ - \ \mu_{Alternative}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;It is worth noting that one of these Z's can be positive, while the other can be negative.  You will find equations that differ from what is done here because of that.  Taking the difference between these two equations give:&lt;/p&gt;
&lt;p&gt;$$\mbox{Effect Size} = \mu_{Alternative} - \mu_{Null}$$&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} \ - \ Z_{\beta} = \frac{\mbox{Effect Size}}{s_{sample}}$$&lt;/p&gt;
&lt;p&gt;We can use the eqauation for the sampling distribution for two sample proproption test of equal size for the sampling error.&lt;/p&gt;
&lt;p&gt;$$s_{sample} = \sqrt{\frac{ p \ (1 - p) \ 2}{n}}$$&lt;/p&gt;
&lt;p&gt;Where p is the weighted proportion of the two groups.  This gives the previous equation as.&lt;/p&gt;
&lt;p&gt;$$Z_{\alpha} \ - \ Z_{\beta} = \frac{\mbox{Effect Size}}{\sqrt{\frac{ p \ (1 - p) \ 2}{n}}}$$&lt;/p&gt;
&lt;p&gt;Squaring both sides we get:&lt;/p&gt;
&lt;p&gt;$$(Z_{\alpha} \ - \ Z_{\beta})^2 = \frac{\mbox{Effect Size}^2 \ n}{ p \ (1 - p) \ 2}$$&lt;/p&gt;
&lt;p&gt;So the needed sample size should be&lt;/p&gt;
&lt;p&gt;$$n = \frac{2 \ (Z_{\alpha} \ - \ Z_{\beta})^2 \ p \ (1-p)}{\mbox{Effect Size}^2}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def calc_min_sample_size(old,new,effect_size,sig=0.05,pow=0.8,one_tail=False):
    pn = new.mean()
    po = old.mean()
    no = len(old)
    nn = len(new)
    dp = pn-po
    p = ( po*no+pn*nn ) / (nn+no)
    se = np.sqrt(p*(1-p)*(1/nn+1/no))
    if one_tail:
        z_null = sc.norm.ppf((1-sig))
    else:
        z_null = sc.norm.ppf((1-sig/2))
    z_pow = sc.norm.ppf(1-pow)


    return (z_null-z_pow)**2*2*p*(1-p)/effect_size**2


calc_min_sample_size(old_data,new_data,0.001,one_tail=False)




1410314.3210247809
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So if we want a power of 80% for our Etsy experiment, meaning its likely for us to detect results that are different from a lift of 1%, we need to have 1.4 million users in each sample.  Our results are definately underpowered.&lt;/p&gt;
&lt;h2&gt;Afternoon - Bayes&lt;/h2&gt;
&lt;p&gt;We covered Bayes' Theorem and updating priors with likelihoods to produce a posterior distribution of sample paramters given data.   We started with a created class, and explore the results of these distributions given data.&lt;/p&gt;
&lt;p&gt;$$ P(Parameters \ | \ Data) = \frac{P(Data \ | \ Parameters) * P(Paramters)}{P(Data)} $$&lt;/p&gt;
&lt;p&gt;$$ \mbox{Posterior} = \frac{\mbox{Likelihood} \ \times \ \mbox{Prior}}{\mbox{Normalization}} $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;Bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;object&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;    INPUT:&lt;/span&gt;
&lt;span class="s1"&gt;        prior (dict): key is the value (e.g. 4-sided die),&lt;/span&gt;
&lt;span class="s1"&gt;                      value is the probability&lt;/span&gt;

&lt;span class="s1"&gt;        likelihood_func (function): takes a new piece of data and the value and&lt;/span&gt;
&lt;span class="s1"&gt;                                    outputs the likelihood of getting that data&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;likelihood_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;prior&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;likelihood_func&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;likelihood_func&lt;/span&gt;


    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        INPUT: None&lt;/span&gt;
&lt;span class="s1"&gt;        OUTPUT: None&lt;/span&gt;

&lt;span class="s1"&gt;        Makes the sum of the probabilities equal 1.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="nx"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;total&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        INPUT:&lt;/span&gt;
&lt;span class="s1"&gt;            data (int or str): A single observation (data point)&lt;/span&gt;

&lt;span class="s1"&gt;        OUTPUT: None&lt;/span&gt;

&lt;span class="s1"&gt;        Conduct a bayesian update. Multiply the prior by the likelihood and&lt;/span&gt;
&lt;span class="s1"&gt;        make this the new prior.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;likelihood_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;print_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        Print the current posterior probability.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;        Plot the current prior.&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;color&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nx"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;color&lt;/span&gt;
        &lt;span class="nx"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="nx"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self.&lt;/span&gt;&lt;span class="nx-Member"&gt;prior&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;likelihood_dice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kr"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;likelihood_coin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;H&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kr"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;A box contains a 4-sided die, a 6-sided die, an 8-sided die,a 12-sided die, and a 20-sided die. A die is selected at random, and the rest are destroyed.  &lt;/p&gt;
&lt;p&gt;What is the prior?&lt;br /&gt;
&lt;strong&gt;All Dice Equally Likely&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Say I roll an 8. After one bayesian update, what is the probability that I chose each of the dice?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b.update(8)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We know that we do not have a 4 or 6 sided dice, and the 8 is most likly because the 8 has a 1 in 8 chance of getting an 8, the 12 has a 1 in 12 chance of rolling and 8, and the 20 sided dice has a 1 in 20 chance of rolling an 8.&lt;/p&gt;
&lt;p&gt;Comment on the difference in the posteriors if I had rolled the die 50 times instead of 1.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[b.update(8) for i in range(49)]&lt;/span&gt;
&lt;span class="err"&gt;b.plot()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_49_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;However unlikely it is, the posterier suggest that the post likely culperite of 50 roles all coming to 8 is an 8 sided dice.&lt;/p&gt;
&lt;p&gt;Which one of these two sets of data gives you a more certain posterior and why?
&lt;code&gt;[1, 1, 1, 3, 1, 2]&lt;/code&gt; or &lt;code&gt;[10, 10, 10, 10, 8, 8]&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in [1,1,1,3,1,2]]
b.plot()
plt.show()
b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in [10,10,10,10,8,8]]
b.plot()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_52_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_52_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We are most certain in the second case becasue 3 of the dice have been ruled out, and a 1/12 per roll is much bigger than a 1/20.&lt;/p&gt;
&lt;p&gt;Say the prior of the dice is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;```
4-sided die: 8%
6-sided die: 12%
8-sided die: 16%
12-sided die: 24%
20-sided die: 40%
```
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What are posteriors for each die after rolling the 8?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.08,&amp;#39;06&amp;#39;:0.12,&amp;#39;08&amp;#39;:0.16,&amp;#39;12&amp;#39;:0.24,&amp;#39;20&amp;#39;:0.40},likelihood_dice)
b.update(8)
b.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_55_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The postior makes 8 sided, 12 sided, and 20 sided dice equally likely after 1 roll of an 8.  &lt;/p&gt;
&lt;p&gt;Say you keep the same prior and you roll the die 50 times and get values 1-8 every time. What would you expect of the posterior? How different do you think it would be if you'd used the uniform prior?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b = Bayes({&amp;#39;04&amp;#39;:0.2,&amp;#39;06&amp;#39;:0.2,&amp;#39;08&amp;#39;:0.2,&amp;#39;12&amp;#39;:0.2,&amp;#39;20&amp;#39;:0.2},likelihood_dice)
[b.update(x) for x in np.random.randint(1,8,size=50)]
b.plot()
plt.show()
b = Bayes({&amp;#39;04&amp;#39;:0.08,&amp;#39;06&amp;#39;:0.12,&amp;#39;08&amp;#39;:0.16,&amp;#39;12&amp;#39;:0.24,&amp;#39;20&amp;#39;:0.40},likelihood_dice)
[b.update(x) for x in np.random.randint(1,8,size=50)]
b.plot()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_58_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_58_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;With enough data, the difference in priors do not matter.   They converge to the same value.  &lt;/p&gt;
&lt;h2&gt;Bayes and Coin Flips&lt;/h2&gt;
&lt;p&gt;We can consider a random flip of a coin and ask what is the believe about our belief of the probability the coin resulting in heads.   Before any flip we would assume a unifor distrubiton.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p = np.linspace(0,0.99,100)
prior = dict()
for v in p:
    prior[str(v)] = 0.01
flips = [[&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;],[&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;H&amp;#39;],[&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;T&amp;#39;,&amp;#39;H&amp;#39;]]

plt.figure(figsize=(15,10))
for i,f in enumerate(flips):
    plt.subplot(4,2,i+1)
    b = Bayes(prior.copy(),likelihood_coin)
    for x in f:
         b.update(x)
    b.plot(label=str(f))
    plt.legend(loc=&amp;#39;best&amp;#39;)
    plt.xticks(rotation=70,fontsize=4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_61_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We were given a cone class, with a unknown probability.  I am going to plot the update in my posteriers after fixed number of flips to see if a Bayesian would believe this is a biased coin&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;coin&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Coin&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Coin&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;10&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;coral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;50&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lightseagreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;250&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;skyblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;500&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;purple&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;likelihood_coin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;249&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;499&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;999&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; Flips&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;70&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D4/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;As we update our believes, upto 1000 flips, we see that 50 looks to be in the edge of our posterior space, but the distribution is centered around 0.53, making this the maximum likelihood value after 1000 flips.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="statistics"></category><category term="hypothesis testing"></category></entry><entry><title>Galvanize - Week 02 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-03/" rel="alternate"></link><updated>2015-06-10T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-10:galvanize/galvanize-data-science-02-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 3&lt;/h2&gt;
&lt;p&gt;Today we had a miniquiz on making a python package that could contain an arbitrary probability mass function as a dictionary.  It had to allow new values to be set, maintain a normalized pmf, and return probabilities for values in the dictionary, None otherwise.   &lt;/p&gt;
&lt;h2&gt;Morning&lt;/h2&gt;
&lt;p&gt;The morning lecture was on Hypthesis testing and multiple testing corrections.  We reviewed the languaged, phrasing, caveots, and particulars about the frame works.  Our sprint involved investigating some questions involving multiple testing and clickthru rates&lt;/p&gt;
&lt;h2&gt;Multiple Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A study attempted to measure the influence of patients' astrological signs on their risk for heart failure.
   12 groups of patients (1 group for each astrological sign) were reviewed and the incidence of heart failure in each group was recorded. For each of the 12 groups, the researchers performed a z-test comparing the incidence of heart failure in one group to the incidence among the patients of all the other groups (i.e. 12 tests). The group with the highest rate of heart failure was Pisces, which had a p-value of .026 when assessing the null hypothesis that it had the same heart failure rate as all the other groups. What is the the problem with concluding from this p-value that Pisces have a higher rate of heart failure at a significance level of 0.05? How might you correct this p-value?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We have 12 tests, with a 5% false positive rate.  Using the Bernoulli Distribution we can see the chance of getting x number of false positive.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;


&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;indianred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;False Positive Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Probability of False Positive Count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability Of False Positives For 12 Tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 false postiive with 12 tests&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of 1 or More false positives with 12 test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;postiive&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="n"&gt;tests&lt;/span&gt; &lt;span class="mf"&gt;0.341280055366&lt;/span&gt;
&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;More&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt; &lt;span class="n"&gt;positives&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="kp"&gt;test&lt;/span&gt; &lt;span class="mf"&gt;0.459639912337&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_1_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is a 34% chance that one of the 12 tests will be a false positive value.  Instead, it would make more sense to chose a false positive rate such that the chance of having false positives out of 12 tests would be less than 0.05&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p = np.linspace(0,1,10000)
cdf = 1-sc.binom.pmf(0,12,p)
plt.plot(p,cdf,label=&amp;quot;CDF of P&amp;quot;,color=&amp;#39;indianred&amp;#39;)
plt.xlabel(&amp;#39;False Positive Rate For 12 Tests&amp;#39;)
plt.ylabel(&amp;#39;Prob of 1+ False Positives&amp;#39;)
plt.fill_between(p, cdf, where=cdf&amp;lt;0.05, interpolate=True, color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.show()
print &amp;quot;New Significant Level: &amp;quot;, p[cdf &amp;lt; 0.05].max()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_3_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;New Significant Level:  0.004200420042
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;In this case we would want to have a false positive rate for a single test of 0.0042.  By this criteria the p-value of 0.026 is not significant for the relation between Picese and heart failure. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Bonferonni correct suggests taking the significant goal of an individual test and divide it by the number of tests.  This gives a value of 0.000417.  This is very close to the above results&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Significance: &amp;quot;, 0.05/12

Significance:  0.00416666666667
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Click Through Rate&lt;/h2&gt;
&lt;p&gt;We will use hypothesis testing to analyze &lt;strong&gt;Click Through Rate (CTR)&lt;/strong&gt; on the New York Times website.
CTR is defined as the number of clicks the user make per impression that is made upon the user.
We are going to determine if there is statistically significant difference between the mean CTR for
the following groups:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Signed in users v.s. Not signed in users&lt;/li&gt;
&lt;li&gt;Male v.s. Female&lt;/li&gt;
&lt;li&gt;Each of 7 age groups against each other (7 choose 2 = 21 tests)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Because we are construction 23 hypothesis tests on this data set, we can use the Bonferroni Correction of dividing the 5% false error rate by 23 to get:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$\alpha_{23} = 0.00217 $$&lt;/p&gt;
&lt;p&gt;We can now load the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt = pd.read_csv(&amp;quot;../ab-testing/data/nyt1.csv&amp;quot;)
nyt.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 458441 entries, 0 to 458440
Data columns (total 5 columns):
Age            458441 non-null int64
Gender         458441 non-null int64
Impressions    458441 non-null int64
Clicks         458441 non-null int64
Signed_In      458441 non-null int64
dtypes: int64(5)
memory usage: 21.0 MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are not any null values in the data set, but we need to construct a click through rate for each user.   We will do this by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Removing users without Impressions&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dividing the Clicks by Impressions&lt;/p&gt;
&lt;p&gt;nyt = nyt[nyt.Impressions!=0]
nyt['CTR'] = np.nan
nyt.CTR = nyt.Clicks.astype(float)/nyt2.Impressions.astype(float)
nyt.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Impressions&lt;/th&gt;
      &lt;th&gt;Clicks&lt;/th&gt;
      &lt;th&gt;Signed_In&lt;/th&gt;
      &lt;th&gt;CTR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have the click through lets look at the distributions of variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;nyt2.hist(figsize=(15,8),bins=30,color=&amp;#39;indianred&amp;#39;,alpha=0.5)




array([[&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088e9f10&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088d9a50&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x108944150&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x104598750&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x10882f990&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1088cafd0&amp;gt;]], dtype=object)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_11_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The age's with zero appear to be users that are not signed in by the height of the bars in the age and signed_in graphs.   The number of total clicks is less then 50,000, and the click through rates are small are mostly zero. &lt;/p&gt;
&lt;p&gt;To answer question 1, we need to split the date between signed-in users and signed-out users.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==1]
nosign = nyt[nyt[&amp;#39;Signed_In&amp;#39;]==0]
plt.figure()
sign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
nosign.hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed32690&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_13_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x108bbc950&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_13_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can construct a hypothesis test on the data where we have the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H0: The mean click through rate between signed in users and signed-out users are the same&lt;/li&gt;
&lt;li&gt;HA: The mean click through rates between signed-in users and signed-out users aer different&lt;/li&gt;
&lt;li&gt;We requrire a p-value less than 0.00217 to rejec the Null Hypothesis in favor for the Alterative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do this with a Weltch's (Non Equal Variance Assigned) t-test results in the following results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign.CTR, nosign.CTR, equal_var=False)




(array(-55.37611793427461), 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The results of the test states that the test statistic to measure the difference is -55, given a p-value very close to zero, which is less than 0.00217.   In this case we can see there is a mean difference in the click through rates between the two populations.  In this case the signed-out users have a higher CTR than signed-in users.&lt;/p&gt;
&lt;p&gt;Question two has to do with the difference in click through rates between genders.   This requires that we only investigate the users that are signed in, because signed out users do not have a gender variable.   Lets look at the data and perform a Wetch's t-test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
sign[sign.Gender==0].hist(figsize=(15,8),bins=10,color=&amp;#39;indianred&amp;#39;,alpha=0.5)
plt.show()
plt.figure()
sign[sign.Gender==1].hist(figsize=(15,8),bins=10,color=&amp;#39;blue&amp;#39;,alpha=0.5)
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10ed3a550&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_17_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;matplotlib.figure.Figure at 0x109a90610&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_17_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;A visual inspect of the data shows that men and women have similar distirubtions, with the exceptions fo the impressions distributions.  Men seem to have a wider range of impressons compared to women.  &lt;/p&gt;
&lt;p&gt;The hypthesis test we are constructing is:&lt;/p&gt;
&lt;p&gt;H0:  Signed-in Men and Women have the same mean CTR&lt;br /&gt;
HA:  Signed-in Men and Women have different mean CTR&lt;br /&gt;
&lt;em&gt;significance is 0.00217&lt;/em&gt;  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sc.ttest_ind(sign[sign.Gender==0].CTR, sign[sign.Gender==1].CTR, equal_var=False)




(array(3.2897560659373846), 0.0010028527313066396)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our test possted a t-value of 3.29 and a p-value of 0.001, which is significant.  We reject the null in favor of the alternative, concluding that men and women have different mean click through rates.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Click through rates (SI Men, SI Women, NOT Si)&amp;quot;,sign[sign.Gender==1].CTR.mean(),sign[sign.Gender==1].CTR.mean(),nosign.CTR.mean()

Click through rates (SI Men, SI Women, NOT Si) 0.0139185242976 0.0139185242976 0.0283549070617
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The difference in the mean CTR between men and women is significant, but not large. The difference between both groups and not-signed-in users are much more different.&lt;/p&gt;
&lt;p&gt;Now we will construct a set of hypothesis tests comparing different age groups against each other.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sign[&amp;#39;AgeGroup&amp;#39;] = np.nan
sign.loc[:,&amp;#39;AgeGroup&amp;#39;] = pd.cut(sign.Age, [0,7, 18, 24, 34, 44, 54, 64, 1000])
df = pd.DataFrame(columns=[&amp;#39;group1&amp;#39;,&amp;#39;group2&amp;#39;,&amp;#39;meanCTR1&amp;#39;,&amp;#39;meanCTR2&amp;#39;,&amp;#39;p_value&amp;#39;,&amp;#39;mean_difference&amp;#39;])
k = 0
AG = sign.AgeGroup.unique()
for i,x in enumerate(AG):
    for j,y in enumerate(AG):
        if x != &amp;#39;(0, 7]&amp;#39; and y!=&amp;#39;(0, 7]&amp;#39; and i &amp;gt; j:
            g1 = sign[sign.AgeGroup==x].CTR
            g2 = sign[sign.AgeGroup==y].CTR
            p = sc.ttest_ind(g1,g2, equal_var=False)[1]
            diff = g1.mean()-g2.mean()
            df.loc[k] = [x,y,g1.mean(),g2.mean(),p,diff]
            k += 1

df = df[df.p_value &amp;lt; 0.00217]
df = df.sort(&amp;#39;mean_difference&amp;#39;).reset_index().drop(&amp;#39;index&amp;#39;,axis=1)
df

/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  if __name__ == &amp;#39;__main__&amp;#39;:
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group1&lt;/th&gt;
      &lt;th&gt;group2&lt;/th&gt;
      &lt;th&gt;meanCTR1&lt;/th&gt;
      &lt;th&gt;meanCTR2&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
      &lt;th&gt;mean_difference&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;2.458627e-272&lt;/td&gt;
      &lt;td&gt;-0.020082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;1.430923e-295&lt;/td&gt;
      &lt;td&gt;-0.019845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;7.860398e-285&lt;/td&gt;
      &lt;td&gt;-0.019656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;6.900980e-144&lt;/td&gt;
      &lt;td&gt;-0.016865&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;9.214903e-56&lt;/td&gt;
      &lt;td&gt;-0.009496&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;8.273993e-20&lt;/td&gt;
      &lt;td&gt;-0.006278&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;3.563408e-05&lt;/td&gt;
      &lt;td&gt;-0.003218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;7.523228e-144&lt;/td&gt;
      &lt;td&gt;0.010020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;5.668132e-141&lt;/td&gt;
      &lt;td&gt;0.010160&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;2.525271e-151&lt;/td&gt;
      &lt;td&gt;0.010349&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;(54, 64]&lt;/td&gt;
      &lt;td&gt;(18, 24]&lt;/td&gt;
      &lt;td&gt;0.020307&lt;/td&gt;
      &lt;td&gt;0.009720&lt;/td&gt;
      &lt;td&gt;1.007813e-130&lt;/td&gt;
      &lt;td&gt;0.010586&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;4.575147e-146&lt;/td&gt;
      &lt;td&gt;0.016299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(24, 34]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.010146&lt;/td&gt;
      &lt;td&gt;7.449266e-146&lt;/td&gt;
      &lt;td&gt;0.016439&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;(7, 18]&lt;/td&gt;
      &lt;td&gt;(44, 54]&lt;/td&gt;
      &lt;td&gt;0.026585&lt;/td&gt;
      &lt;td&gt;0.009958&lt;/td&gt;
      &lt;td&gt;4.014382e-151&lt;/td&gt;
      &lt;td&gt;0.016628&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;(64, 1000]&lt;/td&gt;
      &lt;td&gt;(34, 44]&lt;/td&gt;
      &lt;td&gt;0.029803&lt;/td&gt;
      &lt;td&gt;0.010286&lt;/td&gt;
      &lt;td&gt;5.245541e-288&lt;/td&gt;
      &lt;td&gt;0.019516&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We itereated through all 21 groups and found 14 pairs who's mean CTR are different from each other with a signicance less than 0.00217.   These results can be summarized in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;People older than 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 54 adn 64 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;li&gt;People between 7 and 18 have a statistically significant difference in mean CTR from all other age groups&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The order of CTR seems to be older than 65, 7-18 year olds, and then 54-64 year olds.  &lt;/p&gt;
&lt;p&gt;Its interest that the high click through rate of non-signed-in users matches those of the older groups.   &lt;/p&gt;
&lt;h2&gt;Afternoon&lt;/h2&gt;
&lt;p&gt;In the afternoon we learned about AB testing, and our paired programming assignment had to do with analyzing the results of simulated data for &lt;a href="http://etsy.com"&gt;Etsy&lt;/a&gt;.  We were asked to analysis the data from a given Tuesday where an AB test was performed on a website between two pages.   The goal is to change the conversion rate from 10% to 10.1% with this new page, a lift of 1%.  We are told that the weekend users are different than weeday users, and most of Etsy's revenue is made on the weekend.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df = pd.read_csv(&amp;#39;../ab-testing/data/experiment.csv&amp;#39;)
df.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4040615247&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4365389205&lt;/td&gt;
      &lt;td&gt;1356998400&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4256174578&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8122359922&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6077269891&lt;/td&gt;
      &lt;td&gt;1356998402&lt;/td&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)]
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)]
print dfa.user_id.nunique(),dfa.user_id.count()
print dfb.user_id.nunique(),dfb.user_id.count()
print dfc.user_id.nunique(),dfc.user_id.count()
print dfd.user_id.nunique(),dfd.user_id.count()

95574 95574
90814 90815
0 0
4759 4759
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Right away we have a problem in the data.  We have 4759 users that are in treatment group that have seen the new page and old page.  There is also one person in the control group that appear's twice.  &lt;/p&gt;
&lt;p&gt;In my minds, a good experiment outlines how to handle missing data, mistakes, and errors in the analysis before they a found.  This is not the case for this assignment, so we will have to decided what to do ourselves.  I am going to exam the 4759 users.  First I will get a datetime in the dataframe, and then I will try to figure out what is happening with this group.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df[&amp;#39;dt&amp;#39;] = np.nan
df[&amp;#39;dt&amp;#39;] = pd.to_datetime(df.ts,unit=&amp;#39;s&amp;#39;)
dfa = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfb = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfc = df[(df.ab==&amp;#39;control&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;new_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)
dfd = df[(df.ab==&amp;#39;treatment&amp;#39;)&amp;amp;(df.landing_page==&amp;#39;old_page&amp;#39;)].sort(&amp;#39;user_id&amp;#39;)

dfd.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189992&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084275&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151793&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066970&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114751&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050318&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99066&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043251&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104055&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045528&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfa[dfa.user_id.isin(dfd.user_id)].head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189991&lt;/th&gt;
      &lt;td&gt;1033628&lt;/td&gt;
      &lt;td&gt;1357084274&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:51:14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;151790&lt;/th&gt;
      &lt;td&gt;1891740&lt;/td&gt;
      &lt;td&gt;1357066969&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 19:02:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;114746&lt;/th&gt;
      &lt;td&gt;4557110&lt;/td&gt;
      &lt;td&gt;1357050317&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 14:25:17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;99064&lt;/th&gt;
      &lt;td&gt;5534964&lt;/td&gt;
      &lt;td&gt;1357043250&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:27:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;104052&lt;/th&gt;
      &lt;td&gt;6180378&lt;/td&gt;
      &lt;td&gt;1357045527&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 13:05:27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;In these 5 cases we see that the users saw the new landing page, then one second later saw the old landing page.  None of these users displayed converted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print dfd[dfd.converted==1].user_id.count()
dfd[dfd.converted==1].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100082&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043708&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67975&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029144&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42109&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017491&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22040&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008350&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87355&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037889&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;old_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:09&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].user_id.count()
dfa[dfa.user_id.isin(dfd[dfd.converted==1].user_id)].head()

501
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100079&lt;/th&gt;
      &lt;td&gt;10792592&lt;/td&gt;
      &lt;td&gt;1357043707&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 12:35:07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;67973&lt;/th&gt;
      &lt;td&gt;13223933&lt;/td&gt;
      &lt;td&gt;1357029143&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 08:32:23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;42108&lt;/th&gt;
      &lt;td&gt;27727121&lt;/td&gt;
      &lt;td&gt;1357017490&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:18:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22038&lt;/th&gt;
      &lt;td&gt;34535851&lt;/td&gt;
      &lt;td&gt;1357008349&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 02:45:49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;87352&lt;/th&gt;
      &lt;td&gt;85676035&lt;/td&gt;
      &lt;td&gt;1357037888&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 10:58:08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We have 501 users that saw the new page, then 1 second later saw the old page and converted.  This does not make sense interms of our project/assignment, so I think the safest thing to do for this analysis is throw out these mistakes, and only do the analysis with the untainted results.  If this was a real experiment, I would definatley investigate the details of the test.&lt;/p&gt;
&lt;p&gt;The goal of the experiment is to have a new pages that has a conversion lift of 1 percent.   With that goal in mine we define the following test:&lt;/p&gt;
&lt;p&gt;H0:  The lift in conversions from the new page and old page is equal to 1%
HA:  the lift if conversions from the new page to the old page is less than 1%&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def z_test(old_conversion, new_conversion, old_nrow, new_nrow,
           effect_size=0., two_tailed=True, alpha=.05):
    &amp;quot;&amp;quot;&amp;quot;z-test&amp;quot;&amp;quot;&amp;quot;
    conversion = (old_conversion * old_nrow + new_conversion * new_nrow) / \
                 (old_nrow + new_nrow)

    se = np.sqrt(conversion * (1 - conversion) * (1 / old_nrow + 1 / new_nrow))

    z_score = (new_conversion - old_conversion - effect_size) / se

    if not two_tailed:
        p_val = 1 - sc.norm.cdf(abs(z_score))
    else:
        p_val = (1 - sc.norm.cdf(abs(z_score))) * 2

    reject_null = p_val &amp;lt; alpha
    #print &amp;#39;z-score: %s, p-value: %s, reject null: %s&amp;#39; % (z_score, p_val, reject_null)
    return z_score, p_val, reject_null



conA,cntA = dfa.converted.mean(),dfa.converted.count()
conB,cntB = dfb.converted.mean(),dfb.converted.count()
print conA,conB,cntA,cntB,0.01*conB
z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)

0.0996819218616 0.0996421296041 95574 90815 0.000996421296041





(-0.74648172622270292, 0.22768823318094589, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this frame there results are not significantly different from a 1% left that we can rule them out.  But we could have also tested if there is a difference between them.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;z_test(conA,conB,cntA,cntB,two_tailed=True,effect_size=0.0)




(-0.028666091979081442, 0.9771308999283459, False)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is also not signifcant difference between the groups either.  The results of our Tuesday experiment are really inconclusive.  Ultimately we are concerned with the effect on weekend users, because they are responsible for most of Etsy's revenue.   We were told this user base is different from the weekend users.  &lt;/p&gt;
&lt;p&gt;AirBNB had a talks (&lt;a href="http://nerds.airbnb.com/experiments-airbnb/"&gt;here&lt;/a&gt; and &lt;a href="http://nerds.airbnb.com/experiments-at-airbnb/"&gt;here&lt;/a&gt;) about looking at the hourly change in a p-vale, and examing if and when it level's off as the 'true' p-value for an experiment.   We are going to explore this method.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_values = []
effect = 0
last_effect=0
for i in range(23):

    # Grab the hour
    df_houra = dfa[dfa.dt.dt.hour&amp;lt;=i]
    df_hourb = dfb[dfb.dt.dt.hour&amp;lt;=i]

    conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
    conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

    p_values.append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*conB)[1] )

plt.figure()
plt.plot(range(23),p_values,color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_38_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;From this method we see that the p-value is still changing, making me believe that our experiment could be under-powered.&lt;/p&gt;
&lt;p&gt;There is additional data about the country of each user.  It could be interesting to look that these results by country.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;countries = pd.read_csv(&amp;quot;../ab-testing/data/country.csv&amp;quot;)
countries.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;9160993935&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5879439034&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;8915383273&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2917824565&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3980216975&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries,on=&amp;#39;user_id&amp;#39;)
dfbc = dfb.merge(countries,on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
print dfbc.user_id.count(),dfbc.user_id.nunique()
dfac.head()

97151 92554
87927 87924
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;ts&lt;/th&gt;
      &lt;th&gt;ab&lt;/th&gt;
      &lt;th&gt;landing_page&lt;/th&gt;
      &lt;th&gt;converted&lt;/th&gt;
      &lt;th&gt;dt&lt;/th&gt;
      &lt;th&gt;country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23267&lt;/td&gt;
      &lt;td&gt;1357066015&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 18:46:55&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;79973&lt;/td&gt;
      &lt;td&gt;1357018111&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 05:28:31&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;338650&lt;/td&gt;
      &lt;td&gt;1357083484&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:38:04&lt;/td&gt;
      &lt;td&gt;UK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;340147&lt;/td&gt;
      &lt;td&gt;1357083599&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2013-01-01 23:39:59&lt;/td&gt;
      &lt;td&gt;US&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;382429&lt;/td&gt;
      &lt;td&gt;1357002072&lt;/td&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;new_page&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2013-01-01 01:01:12&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;dfac.groupby(&amp;#39;user_id&amp;#39;).country.count().max()




2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see that some users have two countries listed.  Since the user country data is not time sensitive, we have to drop them.  We do not know which country they were in at the type of the experiment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dfac = dfa.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfac.user_id.count(),dfac.user_id.nunique()
dfbc = dfb.merge(countries.drop_duplicates(&amp;#39;user_id&amp;#39;),on=&amp;#39;user_id&amp;#39;)
print dfbc.user_id.count(),dfbc.user_id.nunique()

92554 92554
87925 87924




p_values = {&amp;#39;US&amp;#39;:[],&amp;#39;CA&amp;#39;:[],&amp;#39;UK&amp;#39;:[]}
effect = {&amp;#39;US&amp;#39;:0,&amp;#39;CA&amp;#39;:0,&amp;#39;UK&amp;#39;:0}
last_effect=0
for i in range(23):
    for country in dfac.country.unique():
    # Grab the hour
        df_houra = dfac[(dfac.dt.dt.hour&amp;lt;=i)&amp;amp;(dfac.country==country)]
        df_hourb = dfbc[(dfbc.dt.dt.hour&amp;lt;=i)&amp;amp;(dfbc.country==country)]

        conA,cntA = df_houra.converted.mean(),df_houra.converted.count()
        conB,cntB = df_hourb.converted.mean(),df_hourb.converted.count()

        p_values[country].append( z_test(conA,conB,cntA,cntB,two_tailed=False,effect_size=0.01*effect[country])[1] )
        effect[country] = conB

plt.figure()
plt.plot(range(23),p_values[&amp;#39;US&amp;#39;],color=&amp;#39;indianred&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;US&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;CA&amp;#39;],color=&amp;#39;blue&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;CA&amp;#39;)
plt.plot(range(23),p_values[&amp;#39;UK&amp;#39;],color=&amp;#39;green&amp;#39;,alpha=0.8,lw=2,label=&amp;#39;UK&amp;#39;)
plt.hlines(0.05,0,23,color=&amp;#39;red&amp;#39;,alpha=0.6,lw=2,linestyle=&amp;#39;--&amp;#39;)
plt.xlabel(&amp;quot;Hours&amp;quot;)
plt.ylabel(&amp;quot;P-values&amp;quot;)
plt.ylim([0,1])
plt.xlim([0,23])
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we constructed a hypthoesis at the end of day on Tuesday, the US would look like they were responsive to the new page, but the time plot shows that this could be a cherry-picked value.  If the experiment was allowed to run for more time, or we collect more data, the value would have changed.   It is clear from these plots that the sample sizes are not large enough that a additional data does not heavily influence the result.   I would be hesitant to make any strong conclusions about the new page based on these results.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="ab testing"></category><category term="statistics"></category><category term="hypthesis testing"></category></entry><entry><title>Galvanize - Week 02 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-02/" rel="alternate"></link><updated>2015-06-09T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-09:galvanize/galvanize-data-science-02-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 2&lt;/h2&gt;
&lt;p&gt;Today we started with a mini-quiz, had a lecture on sampling methods, were given a talk about searching for a job, and finished the day with lecture on estimations/bootstraping and a reinforcement paired programming section.&lt;/p&gt;
&lt;h2&gt;Mini-Quiz&lt;/h2&gt;
&lt;p&gt;The mini-quiz is interesting because it involved using pandas, which I have found to be great and flexible, while also mysterious.&lt;/p&gt;
&lt;p&gt;We were given a salary dataset and asked to make some changes to it and answer some questions.  The first was to read in the data and convert the names to a human readiable text and transform variables to the correct type.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../estimation-sampling/data/salary_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;info&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nc"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;core&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;Int64Index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;32159&lt;/span&gt;
&lt;span class="n"&gt;Data&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;          &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;job_title&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;department&lt;/span&gt;    &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;salary&lt;/span&gt;        &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;Join&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;     &lt;span class="mi"&gt;32160&lt;/span&gt; &lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;
&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;



&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;job_title&lt;/th&gt;
      &lt;th&gt;department&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.columns = [&amp;#39;Name&amp;#39;,&amp;#39;Position Title&amp;#39;,&amp;#39;Department&amp;#39;,&amp;#39;Employee Annual Salary&amp;#39;,&amp;#39;Join Date&amp;#39;]
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This is how I traditionally have renamed columns.   I learned a new way that involed using pandas' 'rename' function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;../estimation-sampling/data/salary_data.csv&amp;#39;)
salary.rename(columns={&amp;#39;name&amp;#39;: &amp;#39;Name&amp;#39;,
                  &amp;#39;job_title&amp;#39;: &amp;#39;Position Title&amp;#39;,
                  &amp;#39;department&amp;#39;:&amp;#39;Department&amp;#39;,
                  &amp;#39;salary&amp;#39;:&amp;#39;Employee Annual Salary&amp;#39;,
                   &amp;#39;join_data&amp;#39;: &amp;#39;Join Date&amp;#39;},
                   inplace=True)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$87228.0&lt;/td&gt;
      &lt;td&gt;2000-09-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-08-04 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;$75372.0&lt;/td&gt;
      &lt;td&gt;2000-01-20 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;$80916.0&lt;/td&gt;
      &lt;td&gt;2000-04-27 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;$99648.0&lt;/td&gt;
      &lt;td&gt;2000-02-11 00:00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;I personally do not like these names for the columns because they involve spaces.   That removes the ability to us the pd.variable notation.   &lt;/p&gt;
&lt;p&gt;I have also found multiple ways to update a variable type.  I am still not sure if there is a better method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary[&amp;#39;Employee Annual Salary&amp;#39;] = salary[&amp;#39;Employee Annual Salary&amp;#39;].str.replace(&amp;quot;$&amp;quot;,&amp;quot;&amp;quot;).astype(float)
salary[&amp;#39;Join Date&amp;#39;] = pd.to_datetime(salary[&amp;#39;Join Date&amp;#39;])
salary.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 32160 entries, 0 to 32159
Data columns (total 5 columns):
Name                      32160 non-null object
Position Title            32160 non-null object
Department                32160 non-null object
Employee Annual Salary    32160 non-null float64
Join Date                 32160 non-null datetime64[ns]
dtypes: datetime64[ns](1), float64(1), object(3)
memory usage: 1.5+ MB
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have the data in the correct format, we can now answer questions about the dataset.  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the top 5 paying job titles?&lt;/li&gt;
&lt;li&gt;How many people have "Police" in their title?&lt;/li&gt;
&lt;li&gt;What fraction of the people in 2 are a 'Police Officer'&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How many people were hired from July 30, 2000 to Aug 08, 2000&lt;/p&gt;
&lt;p&gt;salary.groupby('Position Title')['Employee Annual Salary'].mean().order(ascending=False).head()&lt;/p&gt;
&lt;p&gt;Position Title
SUPERINTENDENT OF POLICE          260004
MAYOR                             216210
FIRE COMMISSIONER                 202728
FIRST DEPUTY SUPERINTENDENT       188316
FIRST DEPUTY FIRE COMMISSIONER    188316
Name: Employee Annual Salary, dtype: float64&lt;/p&gt;
&lt;p&gt;print "Contains 'POLICE': ", salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].count()
salary[salary['Position Title'].str.contains('POLICE')]['Position Title'].value_counts(normalize=True)&lt;/p&gt;
&lt;p&gt;Contains 'POLICE':  11141&lt;/p&gt;
&lt;p&gt;POLICE OFFICER                                      0.847051
POLICE OFFICER (ASSIGNED AS DETECTIVE)              0.076025
POLICE COMMUNICATIONS OPERATOR II                   0.019747
POLICE COMMUNICATIONS OPERATOR I                    0.012925
POLICE OFFICER / FLD TRNG OFFICER                   0.010232
POLICE OFFICER (ASSIGNED AS EVIDENCE TECHNICIAN)    0.006463
POLICE OFFICER/EXPLSV DETECT K9 HNDLR               0.003590
POLICE OFFICER (ASGND AS MARINE OFFICER)            0.002783
POLICE CADET                                        0.002603
ELECTRICAL MECHANIC-AUTO-POLICE MTR MNT             0.002423
MACHINIST (AUTO) POLICE MOTOR MAINT                 0.002244
POLICE OFFICER (ASSIGNED AS CANINE HANDLER)         0.001885
POLICE OFFICER (ASSIGNED AS TRAFFIC SPECIALIST)     0.001795
SUPERVISING POLICE COMMUNICATIONS OPERATOR          0.001616
POLICE OFFICER (ASGND AS MOUNTED PATROL OFFICER)    0.001346
POLICE OFFICER (ASSIGNED AS SECURITY SPECIALIST)    0.001346
POLICE AGENT                                        0.001167
POLICE FORENSIC INVESTIGATOR I                      0.001077
POLICE OFFICER(ASGND AS LATENT PRINT EX)            0.000987
POLICE OFFICER (PER ARBITRATION AWARD)              0.000898
POLICE TECHNICIAN                                   0.000539
POLICE LEGAL OFFICER II                             0.000359
POLICE LEGAL OFFICER I                              0.000269
DIR OF POLICE RECORDS                               0.000090
MANAGER OF POLICE PAYROLLS                          0.000090
POLICE OFFICER(ASGND AS SUPVG LATENT PRINT EX)      0.000090
EXECUTIVE DIR - POLICE BOARD                        0.000090
SUPERINTENDENT OF POLICE                            0.000090
ASST SUPVSR OF POLICE RECORDS                       0.000090
MANAGER OF POLICE PERSONNEL                         0.000090
dtype: float64&lt;/p&gt;
&lt;p&gt;salary.set_index('Join Date',inplace=True)
salary.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Position Title&lt;/th&gt;
      &lt;th&gt;Department&lt;/th&gt;
      &lt;th&gt;Employee Annual Salary&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Join Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-09-27&lt;/th&gt;
      &lt;td&gt;AARON,  ELVIA J&lt;/td&gt;
      &lt;td&gt;WATER RATE TAKER&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;87228&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-08-04&lt;/th&gt;
      &lt;td&gt;AARON,  JEFFERY M&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-20&lt;/th&gt;
      &lt;td&gt;AARON,  KARINA&lt;/td&gt;
      &lt;td&gt;POLICE OFFICER&lt;/td&gt;
      &lt;td&gt;POLICE&lt;/td&gt;
      &lt;td&gt;75372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-04-27&lt;/th&gt;
      &lt;td&gt;AARON,  KIMBERLEI R&lt;/td&gt;
      &lt;td&gt;CHIEF CONTRACT EXPEDITER&lt;/td&gt;
      &lt;td&gt;GENERAL SERVICES&lt;/td&gt;
      &lt;td&gt;80916&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-02-11&lt;/th&gt;
      &lt;td&gt;ABAD JR,  VICENTE M&lt;/td&gt;
      &lt;td&gt;CIVIL ENGINEER IV&lt;/td&gt;
      &lt;td&gt;WATER MGMNT&lt;/td&gt;
      &lt;td&gt;99648&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary.ix[&amp;#39;2000-07-13&amp;#39; : &amp;#39;2000-08-13&amp;#39;].count()




Name                      2866
Position Title            2866
Department                2866
Employee Annual Salary    2866
dtype: int64
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Morning Sprint&lt;/h2&gt;
&lt;p&gt;The individual morning sprint covered sampling and estimation.   We were given a dataset on rain fall and attempted to use &lt;a href="http://en.wikipedia.org/wiki/Method_of_moments_%28statistics%29"&gt;Method of Moments&lt;/a&gt; estimates on the data to approximate the distributions.  We then followed up by looking at &lt;a href="http://en.wikipedia.org/wiki/Maximum_likelihood"&gt;Maximum Likelihood Estimates&lt;/a&gt; of the parameters.  &lt;/p&gt;
&lt;p&gt;I first looked at the data for January rainfall over the course of several years.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = pd.read_csv(&amp;quot;../estimation-sampling/data/rainfall.csv&amp;quot;)
print data.info()

&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 140 entries, 0 to 139
Data columns (total 13 columns):
Year    140 non-null int64
Jan     140 non-null float64
Feb     140 non-null float64
Mar     140 non-null float64
Apr     140 non-null float64
May     140 non-null float64
Jun     140 non-null float64
Jul     140 non-null float64
Aug     140 non-null float64
Sep     140 non-null float64
Oct     140 non-null float64
Nov     140 non-null float64
Dec     140 non-null float64
dtypes: float64(12), int64(1)
memory usage: 15.3 KB
None



data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Jan&lt;/th&gt;
      &lt;th&gt;Feb&lt;/th&gt;
      &lt;th&gt;Mar&lt;/th&gt;
      &lt;th&gt;Apr&lt;/th&gt;
      &lt;th&gt;May&lt;/th&gt;
      &lt;th&gt;Jun&lt;/th&gt;
      &lt;th&gt;Jul&lt;/th&gt;
      &lt;th&gt;Aug&lt;/th&gt;
      &lt;th&gt;Sep&lt;/th&gt;
      &lt;th&gt;Oct&lt;/th&gt;
      &lt;th&gt;Nov&lt;/th&gt;
      &lt;th&gt;Dec&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1871&lt;/td&gt;
      &lt;td&gt;2.76&lt;/td&gt;
      &lt;td&gt;4.58&lt;/td&gt;
      &lt;td&gt;5.01&lt;/td&gt;
      &lt;td&gt;4.13&lt;/td&gt;
      &lt;td&gt;3.30&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;0.95&lt;/td&gt;
      &lt;td&gt;1.31&lt;/td&gt;
      &lt;td&gt;2.13&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1872&lt;/td&gt;
      &lt;td&gt;2.32&lt;/td&gt;
      &lt;td&gt;2.11&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;5.91&lt;/td&gt;
      &lt;td&gt;3.09&lt;/td&gt;
      &lt;td&gt;5.17&lt;/td&gt;
      &lt;td&gt;6.10&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;4.50&lt;/td&gt;
      &lt;td&gt;1.58&lt;/td&gt;
      &lt;td&gt;2.25&lt;/td&gt;
      &lt;td&gt;2.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1873&lt;/td&gt;
      &lt;td&gt;2.96&lt;/td&gt;
      &lt;td&gt;7.14&lt;/td&gt;
      &lt;td&gt;4.11&lt;/td&gt;
      &lt;td&gt;3.59&lt;/td&gt;
      &lt;td&gt;6.31&lt;/td&gt;
      &lt;td&gt;4.20&lt;/td&gt;
      &lt;td&gt;4.63&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
      &lt;td&gt;4.28&lt;/td&gt;
      &lt;td&gt;4.36&lt;/td&gt;
      &lt;td&gt;5.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1874&lt;/td&gt;
      &lt;td&gt;5.22&lt;/td&gt;
      &lt;td&gt;9.23&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;11.84&lt;/td&gt;
      &lt;td&gt;1.49&lt;/td&gt;
      &lt;td&gt;2.87&lt;/td&gt;
      &lt;td&gt;2.65&lt;/td&gt;
      &lt;td&gt;3.52&lt;/td&gt;
      &lt;td&gt;3.12&lt;/td&gt;
      &lt;td&gt;2.63&lt;/td&gt;
      &lt;td&gt;6.12&lt;/td&gt;
      &lt;td&gt;4.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1875&lt;/td&gt;
      &lt;td&gt;6.15&lt;/td&gt;
      &lt;td&gt;3.06&lt;/td&gt;
      &lt;td&gt;8.14&lt;/td&gt;
      &lt;td&gt;4.22&lt;/td&gt;
      &lt;td&gt;1.73&lt;/td&gt;
      &lt;td&gt;5.63&lt;/td&gt;
      &lt;td&gt;8.12&lt;/td&gt;
      &lt;td&gt;1.60&lt;/td&gt;
      &lt;td&gt;3.79&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
      &lt;td&gt;5.46&lt;/td&gt;
      &lt;td&gt;4.30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_16_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;To me this looks like it could be well fitted by a &lt;a href="http://en.wikipedia.org/wiki/Poisson_distribution"&gt;Poisson distribution&lt;/a&gt; or a &lt;a href="http://en.wikipedia.org/wiki/Gamma_distribution"&gt;Gamma distribution&lt;/a&gt;.   A poisson distribution models random events occuring in a fixed time interval, and is a discreate distribution.  Gamma distributions are a continuous distribution that model how long one must way for N events to happen.   That seems like a better framework to think about rain fall.&lt;/p&gt;
&lt;p&gt;The mean and variance for a Poisson distribution is the lambda parameter:&lt;/p&gt;
&lt;p&gt;$$\mu = \lambda$$&lt;/p&gt;
&lt;p&gt;So we will use this to see the fit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;

&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Jan&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Jan Rain&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Poisson Fit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Fall In Janary For All Years&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Rain Values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mf"&gt;4.54457142857&lt;/span&gt; &lt;span class="mf"&gt;6.91677463515&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_18_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This gives a fair fit to the distribution for january.   I want to compare this with the gama distribution.&lt;/p&gt;
&lt;p&gt;The gamma function is given by:&lt;/p&gt;
&lt;p&gt;$$X = Gamma(\alpha \ ,\beta) = \frac{\beta^\alpha \ x^{\alpha-1} \ e^{-\beta x}}{\Gamma(\alpha)}$$&lt;/p&gt;
&lt;p&gt;The mean and variance of the gamma distribution is given by&lt;/p&gt;
&lt;p&gt;$$\mu = \frac{\alpha}{\beta}$$&lt;/p&gt;
&lt;p&gt;$$\sigma^2 = \frac{\alpha}{\beta^2}$$&lt;/p&gt;
&lt;p&gt;So the estimate of alpha and beta are given by:&lt;/p&gt;
&lt;p&gt;$$\beta = \frac{\mu}{\sigma^2}$$&lt;/p&gt;
&lt;p&gt;$$\alpha = \frac{\mu^2}{\sigma^2}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;beta = mean/var
alpha = mean**2/var
print alpha,beta
x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1/beta)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x,y,color=&amp;#39;red&amp;#39;,linestyle=&amp;#39;--&amp;#39;,label=&amp;#39;Poisson Fit&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_20_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The Gamma distribution fit matches the distribution's peak and tail better than the Poisson distribution fit.  There are method's to test the relative fit but I saved that for another day.&lt;/p&gt;
&lt;p&gt;Now lets look at the Gamma fits for all months.  The reason we bin by months is that we have the prior that rain and weather is season.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;f, axarr = plt.subplots(3, 4,figsize=(14, 8))

x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    y = sc.gamma.pdf(x,alpha,scale=1/beta)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;Gamma Fit&amp;quot;,color=&amp;#39;red&amp;#39;)
    label = &amp;#39;alpha = %.2f\nbeta = %.2f&amp;#39; % (alpha, beta)
    axarr[i/4,i%4].annotate(label, xy=(4, 0.25))

plt.tight_layout()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_22_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have a Method of Moments fit of the gamma distribution of rainfall for each month.   Now lets doo Maximum Likely Hood.  &lt;/p&gt;
&lt;p&gt;First we need to make a funciton.  In order to test the method I will try it on a poisson generated dataset.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def poisson_likelihood(x, lam):
    return sc.poisson.pmf(x,lam)

##produces the probabilty of of each lambda(lam) given a value of 6
plt.plot(range(1,20),[poisson_likelihood(6, lam) for lam in range(1,20)])
plt.xlabel(&amp;quot;Lambda Value&amp;quot;)
plt.ylabel(&amp;quot;Probability of Lambda Value Given x=6&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c5d0290&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;This make sense because the maximum likelihood is 6, but there are still changes that the value is different.  Lets run this on the data now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;p_data = pd.read_csv(&amp;#39;../estimation-sampling/data/poisson.txt&amp;#39;,header=None)
p_data.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;x_values = np.linspace(1,20,1000)
likelihoods = np.array([np.log(poisson_likelihood(p_data.values[:,0],i)).sum() for i in x_values])
plt.plot(x_values,likelihoods)
plt.ylabel(&amp;quot;Log Likelyhood For Lambda Fro Data&amp;quot;)
plt.xlabel(&amp;quot;Lambda Values&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x10c67f090&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_27_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We want to compare the maximum likelihood (argmax) of this distribution to the mean fo the data since the mean of Poisson distribution should be the lambda parameter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x_values[likelihoods.argmax()],p_data.mean()




(5.0510510510510516, 0    5.0437
 dtype: float64)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The maximum likelihood estimate of the lambda parameter is very close to the sample mean, which also matches the value of lambda used to generated the data ($$\lambda = 5$$).&lt;/p&gt;
&lt;p&gt;Scipy Stats has a fit function for each of the distributions that uses the maximum likelihood method.   I want to compare the plots the difference between the fits of the Method of Moments and the Maximum Likelihood.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mean = data.Jan.mean()
var = data.Jan.var()
beta = mean/var
alpha = mean**2/var

x1 = np.linspace(0,16,100)
y1 = sc.gamma.pdf(x1,alpha,scale=1./beta)
alpha_MLE,loc_MFL,one_over_beta_MLE = sc.gamma.fit(data.Jan,floc=0) #Loc = 0 - no rainfall minimum
y2 = sc.gamma.pdf(x1,alpha_MLE,scale=one_over_beta_MLE)
plt.figure()
data.Jan.hist(bins=30,color=&amp;#39;red&amp;#39;,alpha=.2,normed=True,label=&amp;#39;Jan Rain&amp;#39;)
plt.plot(x1,y1,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MoM)&amp;#39;,linestyle=&amp;#39;--&amp;#39;)
plt.plot(x1,y2,color=&amp;#39;red&amp;#39;,label=&amp;#39;Gamma Fit (MLE)&amp;#39;,linestyle=&amp;#39;-&amp;#39;)
plt.title(&amp;quot;Rain Fall In Janary For All Years&amp;quot;)
plt.xlabel(&amp;quot;Rain Values&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.legend()
plt.show()
print alpha, alpha_MLE
print beta, beta_MLE

2.98594801173 0.65703621533
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_31_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;2.98594801173 3.25219914651
0.65703621533 1.39738411574
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the two distributions are similar, but slightly different.   The MLE is more skewed right, and the MLE fits are larger then the method of moments.  I just want to finish this section with a plot of all the months.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    print month, alpha, alpha_fit, beta, beta_fit
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()

Jan 2.98594801173 3.25219914651 0.65703621533 0.715622847528
Feb 3.0418721755 3.0803224672 0.740681272732 0.750043734186
Mar 4.67867768543 4.64576013378 0.946813252137 0.94015180285
Apr 4.28032831959 4.25175417646 1.01660157558 1.00981505905
May 3.53902182175 3.82055049055 0.815644176548 0.880528551612
Jun 2.97083704473 2.89974494499 0.765862938963 0.747535846757
Jul 3.98358361758 3.6314371778 1.02531889482 0.934681309897
Aug 3.02948804114 3.20185713476 0.907886646459 0.959542766645
Sep 2.29238948802 2.14489278382 0.678766821038 0.635093671449
Oct 2.46786112353 1.96116795936 0.945359556993 0.751261428601
Nov 3.69539855825 3.30306402837 1.00014653216 0.893962581139
Dec 3.23590721109 3.52396472416 0.77216125712 0.840898349041



&amp;lt;matplotlib.figure.Figure at 0x10c387e10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_33_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;For each moths the distributions are similar, but like january the alpha and beta values are different. &lt;/p&gt;
&lt;h2&gt;Kernal Density Estimates&lt;/h2&gt;
&lt;p&gt;The last topic we had was to use the non-parametric method for fitting a distributions using gaussian kernal density estimates.   The idea is that each data point is fit with a gausian for some unknown variance, and the variance is shared for each such gaussian.   The variance paramenter is adjusted until an 'optimal' fit is found.&lt;/p&gt;
&lt;p&gt;We can do this with an example by convoluting two gaussian data sets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data2 = [sc.norm.rvs(loc=0,scale=2) for x in range(500)]+[sc.norm.rvs(loc=4,scale=1) for x in range(400)]
plt.figure()
plt.hist(data,bins=30,color=&amp;#39;red&amp;#39;,alpha=0.2)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_35_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fit = sc.gaussian_kde(data2)
plt.figure()
plt.hist(data,bins=30,normed=True,alpha=0.2)
x=np.linspace(-6,8,100)
plt.plot(x,fit(x),color=&amp;#39;blue&amp;#39;,label=&amp;#39;KDE Fit&amp;#39;)
plt.xlabel(&amp;quot;Values&amp;quot;)
plt.ylabel(&amp;quot;Counts&amp;quot;)
plt.legend()
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_36_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;The fit function can be used for a density estimate when we do not wish to model the data with a particlar model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plt.figure()
f, axarr = plt.subplots(3, 4,figsize=(14, 10))
x = np.linspace(0,16,100)
for i,month in enumerate(data.columns[1:]):
    mean = data[month].mean()
    var = data[month].var()
    alpha = mean**2/var
    beta = mean/var
    fits = sc.gamma.fit(data[month],floc=0)
    alpha_fit = fits[0]
    beta_fit = 1./fits[2]
    y = sc.gamma.pdf(x,alpha,scale=1./beta)
    y1 = sc.gamma.pdf(x,alpha_fit,scale=1./beta_fit)
    gfit = sc.gaussian_kde(data[month])
    yg = gfit(x)
    axarr[i/4,i%4].hist(data[month],bins=20,color=&amp;#39;red&amp;#39;,alpha=0.25,normed=True)
    axarr[i/4,i%4].set_xlim([0,20])
    axarr[i/4,i%4].set_ylim([0,.35])
    axarr[i/4,i%4].set_xlabel(&amp;quot;Rain Fall&amp;quot;)
    axarr[i/4,i%4].set_ylabel(&amp;quot;Prob Density&amp;quot;)
    axarr[i/4,i%4].set_title(month)
    axarr[i/4,i%4].plot(x,y,label=&amp;quot;MOD&amp;quot;,linestyle=&amp;#39;-&amp;#39;,color=&amp;#39;red&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y1,label=&amp;quot;MLE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;green&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].plot(x,y2,label=&amp;quot;KDE&amp;quot;,linestyle=&amp;#39;--&amp;#39;,color=&amp;#39;blue&amp;#39;,lw=3,alpha=0.5)
    axarr[i/4,i%4].legend()

plt.tight_layout()
plt.show()


&amp;lt;matplotlib.figure.Figure at 0x10c65e490&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_38_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;In each case the non-KDE function seem tot fit the data better, but it could be used to for other estimates if we did not have a model.&lt;/p&gt;
&lt;h2&gt;Paired Programming&lt;/h2&gt;
&lt;p&gt;In the afternoon session we had to investigate the centeral limit theorem, produce confidence intervals, and attempt some bootstrapping estimates&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_draws(distribution, parameters, size):
    &amp;#39;&amp;#39;&amp;#39;
        returns distribrution or None if valid distribution is not selected
    &amp;#39;&amp;#39;&amp;#39;    
    dist = None

    if distribution.lower() == &amp;#39;binomial&amp;#39;:
        n, p = parameters[&amp;#39;n&amp;#39;], parameters[&amp;#39;p&amp;#39;]
        dist = sc.binom(n, p).rvs(size)

    elif distribution.lower() == &amp;#39;exponential&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.expon(scale = l).rvs(size)

    elif distribution.lower() == &amp;#39;poisson&amp;#39;:
        l = parameters[&amp;#39;lambda&amp;#39;]
        dist = sc.poisson(mu=l).rvs(size)

    elif distribution.lower() == &amp;#39;gamma&amp;#39;:
        a, b = parameters[&amp;#39;alpha&amp;#39;],parameters[&amp;#39;beta&amp;#39;]
        dist = sc.gamma(a=a,scale=1./b).rvs(size)

    elif distribution.lower() == &amp;#39;normal&amp;#39;:
        mean, var = parameters[&amp;#39;mean&amp;#39;], parameters[&amp;#39;var&amp;#39;]
        dist = sc.norm(loc=mean, scale=var).rvs(size)

    elif distribution.lower() == &amp;#39;uniform&amp;#39;:
        low, high = parameters[&amp;#39;low&amp;#39;], parameters[&amp;#39;high&amp;#39;]
        dist = sc.uniform(loc=low, scale=(high-low)).rvs(size)

    return dist

def plot_means(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).mean())
    plt.figure()
    plt.title(&amp;quot;Centeral Limit Theorem: &amp;quot; + distribution + &amp;quot; for N = &amp;quot; + str(size))
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_41_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_42_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 10, 5000)
plot_means(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_43_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_44_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 10, 5000)
plot_means(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_45_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 10, 5000)
plot_means(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:10,&amp;#39;high&amp;#39;:20}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_46_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these distirubtions we see that for N = 10, espeicaly for the discrete distriubiton, that the sampling distirubiton of the mean is not normal.   If the underlying distribution is skewed, so is the sampling distribution.  If we look at means of samples of size 200, the central limit theorm holds and the sampling distribution is normal even if the underlying distribution is skewed or discrete.  &lt;/p&gt;
&lt;p&gt;The central limit theorm does not hold for all statistics.  We can look at the max, for instance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def plot_max(distribution, parameters, size, repeat):
    arr = []
    for r in range(repeat):
        arr.append(make_draws(distribution, parameters, size).max())
    plt.figure()
    plt.hist(arr, normed=1,bins=100)
    plt.show()

plot_max(&amp;#39;poisson&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;binomial&amp;#39;, {&amp;#39;n&amp;#39;:10,&amp;#39;p&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;exponential&amp;#39;, {&amp;#39;lambda&amp;#39;:10}, 200, 5000)
plot_max(&amp;#39;gamma&amp;#39;, {&amp;#39;alpha&amp;#39;:10,&amp;#39;beta&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;normal&amp;#39;, {&amp;#39;mean&amp;#39;:10,&amp;#39;var&amp;#39;:0.1}, 200, 5000)
plot_max(&amp;#39;uniform&amp;#39;, {&amp;#39;low&amp;#39;:5, &amp;#39;high&amp;#39;:10}, 200, 5000)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_48_5.png" /&gt;&lt;/p&gt;
&lt;p&gt;These distributions are clearly not normally distributed, and the discrete distribution results remain discrete.&lt;/p&gt;
&lt;h3&gt;Population Inference and Confidence Interval&lt;/h3&gt;
&lt;p&gt;Our next section had to do with constructing confidence intervals on means for different situations.   We were given some lunch data, and attempted to construct the confidence interval for the mean lunch break.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lunch_hour = np.loadtxt(&amp;#39;../estimation-sampling/data/lunch_hour.txt&amp;#39;)
plt.figure()
plt.hist(lunch_hour)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_50_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;There are 25 data points in the data.   Even though this distrubtion is not normal, the sampling distribution of the mean should be approching a normal distribution.  &lt;/p&gt;
&lt;p&gt;The standard deviation of the sampling distirubiton is suppose to be well approximated by the standard error of the sample.&lt;/p&gt;
&lt;p&gt;$$s = \sqrt{ \frac{\Sigma_{i}(x_i \ - \ \bar{x})^2}{N-1} }$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = lunch_hour.std(ddof=1) / np.sqrt( len(lunch_hour) )
se, sc.sem(lunch_hour) ##scipy standard error comparison




(0.040925827625524797, 0.040925827625524797)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using this standard error we can attempt to construct the confidence interval on the population mean.  We choose Z=1.96 for a 95% confidence interval&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;lm = lunch_hour.mean()
(ci_95_low, ci_95_hi) = (lm-1.96*se,lm+1.96*se)
(ci_95_low, ci_95_hi)




(2.1042853778539716, 2.264714622146029)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The 95% confidence interval interpretation is that 95% of the confidence intervals constructed through this method will contrain the population mean lunch hour.   If the sample size was smaller, the both the standard error and normal approximation would change in a way that would not allow this method to work.   &lt;/p&gt;
&lt;p&gt;For smaller sample sizes we would want to try another method.  Bootstrapping could be effective.&lt;/p&gt;
&lt;p&gt;Bootstrapping does not assume normality, or more any assumptions about the underlying distribution.   It is a non-parametric method of constructiong an confidence interval.   If the distribution is well approximateldy by some common distribution, the bootstrapped CI will overestimate the boundaries compared to this distribution.  &lt;/p&gt;
&lt;p&gt;We will try this for another data set involving productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;productivity = np.loadtxt(&amp;#39;../estimation-sampling/data/productivity.txt&amp;#39;)
productivity




array([-19.1, -15.2, -12.4, -15.4,  -8.7,  -6.7,  -5.9,  -3.5,  -3.1,
        -2.1,   4.2,   6.1,   7. ,   9.1,  10. ,  10.3,  13.2,  10.1,
        14.1,  14.4,  20.1,  26.3,  27.7,  22.2,  23.4])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because the sample size is large enough, we would expect the centeral limit to hold.  Lets see if the boot strapping gives similar values.  If we do not know the population variance, we should us the t-distribution.   We will also check that.  The two results should be close&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;se = sc.sem(productivity)
mean = productivity.mean()
ci95lo, ci95hi = mean-1.96*se, mean+1.96*se
print ci95lo, ci95hi

-0.330202770421 10.4182027704



ci95lo, ci95hi = mean+sc.t.ppf(0.025,24)*se, mean+sc.t.ppf(0.975,24)*se
print ci95lo, ci95hi

-0.615086412127 10.7030864121
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;These intervals are very close.   Bootstrapping should give a similar result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt; &lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;np.random.randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

    &lt;span class="nx"&gt;statistic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;stat_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;bootstrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;B&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;array&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;means&lt;/span&gt;
    &lt;span class="nx"&gt;low&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="nx"&gt;high&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;low&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;high&lt;/span&gt;

&lt;span class="nx"&gt;bootstrap_ci&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;productivity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;




&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.49619999999999997&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.265000000000001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is in line with the previous estimates.   Lets look at the histogram of bootstrapped means.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bootstrap_plot_means(sample, iterations=1000):
    samples = bootstrap(sample, iterations)
    means = np.apply_along_axis(np.mean, 1, samples)
    plt.figure()
    plt.hist(means,normed=True,color=&amp;#39;red&amp;#39;,alpha=0.1)
    plt.xlabel(&amp;#39;Mean Values&amp;#39;)
    plt.ylabel(&amp;#39;Probability Density&amp;#39;)
    plt.show()

bootstrap_plot_means(productivity)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_63_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Even though the results are not a statistically significant difference from zero, the results suggest that the population value is likely to be different from zero.   The uncertainty from the sample does not allow us to know that it is not zero, but we do know that it does not significantly harm productivity.&lt;/p&gt;
&lt;h3&gt;Bootstraping Correlation&lt;/h3&gt;
&lt;p&gt;We can bootstrap other variables.   We will try it for the correlation between LSAT and GPA from law data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;law_sample = np.loadtxt(&amp;#39;../estimation-sampling/data/law_sample.txt&amp;#39;)
plt.scatter(law_sample[:,0],law_sample[:,1])
plt.xlabel(&amp;quot;LSAT Score&amp;quot;)
plt.ylabel(&amp;quot;GPA&amp;quot;)
print sc.pearsonr(law_sample[:,0],law_sample[:,1])

(0.77637449128940705, 0.00066510201110281625)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_65_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data = bootstrap(law_sample,B=10000)
corrs = np.array([sc.pearsonr(mat[:,0],mat[:,1])[0] for mat in data])
plt.figure()
plt.hist(corrs)
plt.show()
np.percentile(corrs,2.5),np.percentile(corrs,97.5)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW02D2/output_66_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.45069334540504685, 0.96239529249176581)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bootstrapping the correlation between the variables from the sampple finds that a 95% confidence interval estimates the population correlation of LSAT with GPA should be between 0.45 and 0.96.   Thankfully we have the full dataset from which this sample was pulled.   Lets compare.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;all_law = np.loadtxt(&amp;#39;../estimation-sampling/data/law_all.txt&amp;#39;)
sc.pearsonr(all_law[:,0],all_law[:,1])[0]




0.75999785550389798
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is smack in the middle of the confidence interval.   Pretty cool.   &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="bootstraping"></category><category term="statistics"></category><category term="confidence interval"></category></entry><entry><title>Galvanize - Week 02 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-02-01/" rel="alternate"></link><updated>2015-06-08T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-08:galvanize/galvanize-data-science-02-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 2 - Day 1&lt;/h2&gt;
&lt;p&gt;Since this is the first day of the week we started with an hour long assessment on what we did last week.  The assessment was straight forward, and very doable with a modest understanding of the previous material.   &lt;/p&gt;
&lt;p&gt;After the test, we started a lecture on probability, which we finished in the afternoon.  There was the individual sprint after the morning lecture, and a paired spring as the afternoon lecture&lt;/p&gt;
&lt;h2&gt;Conditional Probabilities&lt;/h2&gt;
&lt;p&gt;We started with some simple questions like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose two cards are drawn from a standard 52 card deck.  What's the probability that the first is a queen and the second is a king?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(K,Q\right) = P\left(K|Q\right)*P\left(Q\right) = \frac{4}{51} * \frac{4}{52} = \frac{16}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 16./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00603318250377&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that both cards are queens?&lt;/p&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q|Q\right)*P\left(Q\right) = \frac{3}{51} * \frac{4}{52} = \frac{12}{2652}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 12./2652&lt;/p&gt;
&lt;p&gt;Answer:  0.00452488687783&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose that before the second card was drawn, the first was inserted back into the deck and the deck reshuffled. What's the probability that both cards are queens?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P\left(Q\right) = \frac{4}{52}$$&lt;/p&gt;
&lt;p&gt;$$P\left(Q,Q\right) = P\left(Q\right)*P\left(Q\right) = \frac{4}{52} * \frac{4}{52} = \frac{16}{2705}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 16./2705

Answer:  0.00591497227357
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We had similar questions about tables of data:&lt;/p&gt;
&lt;p&gt;A Store Manager wants to understand how his customers use different payment methods, and suspects that the size of the purchase is a major deciding factor. He organizes the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="right"&gt;Cash&lt;/th&gt;
&lt;th align="right"&gt;Debit&lt;/th&gt;
&lt;th align="right"&gt;Credit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Under 20&lt;/td&gt;
&lt;td align="right"&gt;400&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;td align="right"&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20 - 50&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;1200&lt;/td&gt;
&lt;td align="right"&gt;800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Over 50&lt;/td&gt;
&lt;td align="right"&gt;100&lt;/td&gt;
&lt;td align="right"&gt;600&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer spent over $50, what's the probability that the customer used a credit card?&lt;/p&gt;
&lt;p&gt;$$P\left(C|S&amp;gt;$50\right) = \frac{1400}{100+600+1400}$$ &lt;/p&gt;
&lt;p&gt;print "Answer: ", 1400./(100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.666666666667&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given that a customer paid in cash, what's the probability that the customer spent less than $20?&lt;/p&gt;
&lt;p&gt;$$P\left(S&amp;lt;20|Cash\right) = \frac{400}{400+200+100}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+200+100)&lt;/p&gt;
&lt;p&gt;Answer:  0.571428571429&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's the probability that a customer spent under $20 using cash?&lt;/p&gt;
&lt;p&gt;$$P\left(S &amp;lt; $20,Cash\right) = \frac{400}{400+150+150+200+1200+800+100+600+1400}$$&lt;/p&gt;
&lt;p&gt;print "Answer: ", 400./(400+150+150+200+1200+800+100+600+1400)&lt;/p&gt;
&lt;p&gt;Answer:  0.08&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also had a question about job offers - something near and dear to our hearts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A gSchool grad is looking for her first job!  Given that she is freaked out, her chances of not getting an offer are 70%.  Given that she isn't freaked out, her chances of not getting an offer are 30%.  Suppose that the probability that she's freaked out is 80%. What's the probability that she gets an offer?&lt;/p&gt;
&lt;p&gt;$$P\left(Offer|Freak Out\right) = 0.7$$  &lt;/p&gt;
&lt;p&gt;$$P\left(Offer|No Freak Oout\right) = 0.3$$&lt;/p&gt;
&lt;p&gt;$$P\left(Freak Out\right) = 0.8$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ P\left(A\right) = \Sigma_{B} P\left(A|B\right) $$&lt;/p&gt;
&lt;p&gt;$$ P\left(Offer\right) = P\left(Offer|Freak Out\right) * P\left(Freak Out\right) + P\left(Offer|No Freak Out\right) * P\left(No Freak Out\right)$$&lt;/p&gt;
&lt;p&gt;$$P\left(Offer\right) = 0.7 * 0.8 + 0.3 * 0.2$$ &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.7 * 0.8 + 0.3 * 0.2

Answer:  0.62
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also tackled the deep issue go heroin use at Google: &lt;/p&gt;
&lt;p&gt;*. Google decides to do random drug tests for heroin on their employees.
   They know that 3% of their population uses heroin. The drug test has the
   following accuracy: The test correctly identifies 95% of the
   heroin users (sensitivity) and 90% of the non-users (specificity).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test Results&lt;/th&gt;
&lt;th&gt;Uses heroin&lt;/th&gt;
&lt;th&gt;Doesn't use heroin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tests positive&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tests negative&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alice gets tested and the test comes back positive. What is the probability
   that she uses heroin?&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test)}$$&lt;/p&gt;
&lt;p&gt;$$P(Heroin \ | \ Positive Test) = \frac{P(Positive Test|Heroin) P(Heroin)}{P(Positive Test|Heroin) P(Heroin) + P(Positive Test|No Heroin) P(No Heroin)}$$ &lt;/p&gt;
&lt;p&gt;$$ = \frac{0.95 \ 0.03}{0.95 \ 0.03 + 0.1 \ 0.97}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 0.95*0.03/(0.95*0.03 + 0.1*0.97)

Answer:  0.227091633466
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally we had the manditory birthday problem:   &lt;/p&gt;
&lt;p&gt;*The Birthday Problem.  Suppose there are 23 people in a data science class, lined up in a single file line.&lt;br /&gt;
Let A_i be the probability that the i'th person doesn't have the same birthday as the j'th person for any j &amp;lt; i.&lt;br /&gt;
Use the chain rule from probability to calculate the probability that at least 2 people share the same birthday. &lt;/p&gt;
&lt;p&gt;$$P(1,2,3,...,23) = \mbox{Probability that 23 people do not have the same birthday}$$&lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2, \ 3, \ ..., \ 23) = P(1) \ P(2|1) \ P(3 \ | \ 2 \ , \ 1) \ ... \ P(23|22 \ , \ ... \ , \ 2, \ 1)$$&lt;/p&gt;
&lt;p&gt;Given that 2 people don't have the same birthday, there are 363 days that are not taken that a new person could have:&lt;/p&gt;
&lt;p&gt;$$P(3|2,1) = \frac{363}{365}$$&lt;/p&gt;
&lt;p&gt;Similarly, given that 3 people don't have the same birthday, then there are 362 days not occupied:&lt;/p&gt;
&lt;p&gt;$$P(4|3,2,1) = \frac{362}{365}$$&lt;/p&gt;
&lt;p&gt;Extending this to the problem we have the probability of no matching birthdays being &lt;/p&gt;
&lt;p&gt;$$P( \ 1, \ 2,\ 3, \ ..., \ 23) = \frac{1 \ 364 \ 363 \ ... \ (365-23)}{365^{23}}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def bday(N=23):
    prob = 1
    for i in range(1,N+1):
        prob = prob*(365.0-i+1)/365.
    return prob

print bday()

0.492702765676
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The probability of having 2 or more matches is us the inverse of this&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1 - P(No Maches|23)$$&lt;/p&gt;
&lt;p&gt;$$P(Matches &amp;gt;= 2| \ 23) = 1-0.4927$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, round(100-100*bday(),1)

Answer:  50.7
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Distributions&lt;/h2&gt;
&lt;p&gt;The afternoon paired programming assignment involved developing an intuition for and using various distributions.&lt;/p&gt;
&lt;h3&gt;Discrete:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model one instance of a success or failure trial (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binomial&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of successes out of a number of trials (n), each with probability of success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Poisson&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model the number of events occurring in a fixed interval&lt;/li&gt;
&lt;li&gt;Events occur at an average rate (lambda) independently of the last event&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Geometric&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence of Bernoulli trials until first success (p)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Continuous:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uniform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any of the values in the interval of a to b are equally likely&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Commonly occurring distribution shaped like a bell curve&lt;/li&gt;
&lt;li&gt;Often comes up because of the Central Limit Theorem (to be discussed later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exponential&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model time between Poisson events&lt;/li&gt;
&lt;li&gt;Events occur continuously and independently&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of our questions involved being given examples and identify the distribution that describes it.&lt;/p&gt;
&lt;p&gt;Often we have to identify what distribution we should use to model a real-life
situation. This exercise is designed to equip you with the ability to do so.&lt;/p&gt;
&lt;p&gt;*. A typist makes on average 2 mistakes per page.  What is the probability of a particular page having no errors on it?&lt;/p&gt;
&lt;p&gt;$$X = Poisson(\lambda = 2 \frac{mistakes}{page})$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Mistakes on a Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Probability&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Prob of No Mistakes: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt; &lt;span class="n"&gt;Mistakes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;span class="mf"&gt;0.135335283237&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_24_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Components are packed in boxes of 20. The probability of a component being
   defective is 0.1.  What is the probability of a box containing 2 defective components?&lt;/p&gt;
&lt;p&gt;$$ X = Binomial(p=0.1,k=2,n=20) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(20)
y = sc.binom.pmf(x,20,.1)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of Defective Components&amp;quot;)
plt.ylabel(&amp;quot;Probability&amp;quot;)
print &amp;quot;Prob of 2 Defects: &amp;quot;, sc.binom.pmf(2,20,.1)

Prob of 2 Defects:  0.285179807064
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_26_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. Patrons arrive at a local bar at a mean rate of 30 per hour.  What is the probability that the bouncer has to wait more than 3 minutes to card the next patron?&lt;/p&gt;
&lt;p&gt;$$X = Exponential(\lambda = .5 \frac{cards}{minute})$$&lt;/p&gt;
&lt;p&gt;$$ P(t \ &amp;gt; \ 3min) = \int_{t=3}^{t=\infty} Exponential(\lambda = .5 \frac{cards}{minute}) $$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, sc.expon.cdf(1e10,scale=0.5)-sc.expon.cdf(3,scale=0.5)


 Anser:  0.00247875217667



x = np.linspace(0,5,1000)
y = sc.expon.pdf(x,scale = 0.5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
plt.ylim([0,0.1])
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=3, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_29_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. A variable is normally distributed with a mean of 120 and a standard
   deviation of 5. One score is randomly sampled. What is the probability the score is above 127?&lt;/p&gt;
&lt;p&gt;$$Z = (127-120)/5 = 7/5 = 1.4$$&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;gt;1.4) = 1 - .91924 \ \mbox{(area to left)}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.linspace(100,140,1000)
y = sc.norm.pdf(x,loc=120,scale=5)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Variable Value&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=127, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_31_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;*. You need to find a tall person, at least 6 feet tall, to help you reach
   a cookie jar. 8% of the population is 6 feet or taller.  If you wait on the sidewalk, how many people would you expect to have passed you by before you'd have a candidate to reach the jar?&lt;/p&gt;
&lt;p&gt;$$X = Geometric(p=0.08)$$&lt;/p&gt;
&lt;p&gt;$$average = \frac{1}{p} = \frac{1}{0.08} = 12.5$$&lt;/p&gt;
&lt;p&gt;We round up - 13th person is expected to be it - 12 people pass.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.pmf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106020790&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_33_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;x = np.arange(40)
y = sc.geom.cdf(x,.08)
plt.bar(x,y,color=&amp;#39;red&amp;#39;,alpha=.2)
plt.xlabel(&amp;quot;Number of People&amp;quot;)
plt.ylabel(&amp;quot;Cumlative Probability Person is Above 6ft&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x106451c50&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_34_1.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A harried passenger will be several minutes late for a scheduled 10 A.M.
   flight to NYC. Nevertheless, he might still make the flight, since boarding
   is always allowed until 10:10 A.M., and boarding is sometimes
   permitted up to 10:30 AM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assuming the extended boarding time is &lt;strong&gt;uniformly distributed&lt;/strong&gt; over the above
   limits, find the probability that the passenger will make his flight,
   assuming he arrives at the boarding gate at 10:25.&lt;/p&gt;
&lt;p&gt;$$X = Uniform(0,30)$$&lt;/p&gt;
&lt;p&gt;$$P( x &amp;gt; 25 ) = \int_{25}^{30} \frac{dx}{30}$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Answer: &amp;quot;, 5./30

Answer:  0.166666666667



x = np.linspace(0,30,1000)
y = sc.uniform.pdf(x,loc=0,scale=30)
plt.figure()
plt.plot(x,y)
plt.xlabel(&amp;quot;Minutes Late&amp;quot;)
plt.ylabel(&amp;quot;Probability Density&amp;quot;)
d = np.zeros(len(y))
plt.fill_between(x, y, where=x&amp;gt;=25, interpolate=True, color=&amp;#39;blue&amp;#39;,alpha=0.4)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_37_0.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Covariance and Joint Distribution&lt;/h2&gt;
&lt;p&gt;Suppose a university wants to look for factors that are correlated with the GPA of the students that they
are going to admit. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;admissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;../probability/data/admissions.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;addmissions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;covariance&lt;/code&gt; function and compute the covariance matrix of the dataframe. Check your results 
   with &lt;code&gt;df.cov()&lt;/code&gt;. Make sure you understand what each of the numbers in the matrix represents&lt;/p&gt;
&lt;p&gt;def make_cov(df):
    N = len(df)
    cols = df.columns
    return [[(df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;from pprint import pprint
pprint (make_cov(addmissions))&lt;/p&gt;
&lt;p&gt;[[332910756.59847927, 4014.9337921708066, -1226.2147143883631],
 [4014.9337921708066, 0.087883196618951942, -0.028782641179958546],
 [-1226.2147143883631, -0.028782641179958546, 112]]&lt;/p&gt;
&lt;p&gt;addmissions.cov()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;3.329410e+08&lt;/td&gt;
      &lt;td&gt;4015.299085&lt;/td&gt;
      &lt;td&gt;-1226.326280&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;4.015299e+03&lt;/td&gt;
      &lt;td&gt;0.087891&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-1.226326e+03&lt;/td&gt;
      &lt;td&gt;-0.028785&lt;/td&gt;
      &lt;td&gt;112.977442&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Implement a &lt;code&gt;normalize&lt;/code&gt; function that would compute the correlation matrix from the covariance matrix.
   Check your results with &lt;code&gt;df.corr()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def make_corr(df):
    N = len(df)
    cols = df.columns
    return [[((df[x]&lt;em&gt;df[y]).sum()/(N)-(df[x].sum()&lt;/em&gt;df[y].sum())/(N**2))/(df[x].std() * df[y].std()) for y in cols] for x in cols]&lt;/p&gt;
&lt;p&gt;pprint (make_corr(addmissions))&lt;/p&gt;
&lt;p&gt;[[0.99990902474526921, 0.74220186205662952, -0.0063224730309758576],
 [0.74220186205662952, 0.99990902474528243, -0.0091340229188836969],
 [-0.0063224730309758576, -0.0091340229188836969, 0.99134834685640671]]&lt;/p&gt;
&lt;p&gt;addmissions.corr()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;td&gt;0.742269&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;td&gt;-0.006323&lt;/td&gt;
      &lt;td&gt;-0.009135&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;You should identify &lt;code&gt;family_income&lt;/code&gt; as being the most correlated with GPA. The university wants to make
   an effort to make sure people of all family income are being fairly represented in the admissions process.
   In order to achieve that, different GPA thresholds will be set according to family income. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The low, medium and high family income groups are &lt;code&gt;0 to 26832&lt;/code&gt;, &lt;code&gt;26833 to 37510&lt;/code&gt; and &lt;code&gt;37511 to 51112&lt;/code&gt; respectively. 
   Implement a function that would plot the distribution of GPA scores for each family income category. These are the 
   conditional probability distributions of &lt;code&gt;gpa&lt;/code&gt; given certain levels of &lt;code&gt;family_income&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def make_hist(df):
    low = df[df.family_income &amp;lt;= 26832]
    med = df[(df.family_income &amp;gt; 26832) &amp;amp; (df.family_income &amp;lt;= 37519)]
    high = df[(df.family_income &amp;gt; 37519) &amp;amp; (df.family_income &amp;lt;= 51112)]
    low.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;blue&amp;quot;,label=&amp;#39;Low Income&amp;#39;)
    med.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;green&amp;quot;,label=&amp;#39;Medium Income&amp;#39;)
    high.gpa.plot(kind=&amp;quot;kde&amp;quot;, color=&amp;quot;red&amp;quot;,label=&amp;#39;High Income&amp;#39;)
    plt.xlim([2.0, 4.0])
    plt.legend()
    plt.show()

make_hist(addmissions)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_47_0.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the university decides to accept students with GPA above the 90th percentile within the respective family 
   income categories, what are the GPA thresholds for each of the categories?&lt;/p&gt;
&lt;p&gt;low = addmissions[addmissions.family_income &amp;lt;= 26832]
med = addmissions[(addmissions.family_income &amp;gt; 26832) &amp;amp; (addmissions.family_income &amp;lt;= 37519)]
high = addmissions[(addmissions.family_income &amp;gt; 37519) &amp;amp; (addmissions.family_income &amp;lt;= 51112)]
print "Low 90th Percentile", low.gpa.quantile(.9)
print "Medium 90th Percentile", med.gpa.quantile(.9)
print "High 90th Percentile", high.gpa.quantile(.9)&lt;/p&gt;
&lt;p&gt;Low 90th Percentile 3.01
Medium 90th Percentile 3.26
High 90th Percentile 3.36&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pearson Correlation vs Spearman Correlation&lt;/h2&gt;
&lt;p&gt;The Pearson correlation evaluates the linear relationship between two continuous 
variables. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables
without assuming linearity of the variables. Spearman correlation is often more robust in capturing non-linear relationship
between variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In addition to the &lt;code&gt;family_income&lt;/code&gt; and &lt;code&gt;parent_avg_age&lt;/code&gt;, you are also given data about the number of hours the 
   students studied. Load the new data in from &lt;code&gt;data/admissions_with_study_hrs_and_sports.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;studydf = pd.read_csv('../probability/data/admissions_with_study_hrs_and_sports.csv')
studydf.head()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;family_income&lt;/th&gt;
      &lt;th&gt;gpa&lt;/th&gt;
      &lt;th&gt;family_income_cat&lt;/th&gt;
      &lt;th&gt;parent_avg_age&lt;/th&gt;
      &lt;th&gt;hrs_studied&lt;/th&gt;
      &lt;th&gt;sport_performance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;31402&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;49.463745&lt;/td&gt;
      &lt;td&gt;0.033196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;32247&lt;/td&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;16.414467&lt;/td&gt;
      &lt;td&gt;0.000317&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;34732&lt;/td&gt;
      &lt;td&gt;2.85&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;4.937079&lt;/td&gt;
      &lt;td&gt;0.021845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53759&lt;/td&gt;
      &lt;td&gt;3.39&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;160.210286&lt;/td&gt;
      &lt;td&gt;0.153819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;50952&lt;/td&gt;
      &lt;td&gt;3.10&lt;/td&gt;
      &lt;td&gt;medium&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;36.417860&lt;/td&gt;
      &lt;td&gt;0.010444&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make a scatter plot of the &lt;code&gt;gpa&lt;/code&gt; against &lt;code&gt;hrs_studied&lt;/code&gt;. Make the points more transperant so you can see the density
   of the points. Use the following command get the slope and intercept of a straight line to fit the data.&lt;/p&gt;
&lt;p&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.hrs_studied)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept&lt;/p&gt;
&lt;p&gt;studydf.plot(kind='scatter',x='gpa',y='hrs_studied',alpha=0.01)
plt.plot(x,y,color='red')
plt.xlabel("GPA")
plt.ylabel("Hours Studied")
plt.show()&lt;/p&gt;
&lt;p&gt;494.329335528 -1400.63719543 0.475940264662 0.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_53_1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the functions &lt;code&gt;scipy.stats.pearsonr&lt;/code&gt; and &lt;code&gt;scipy.stats.spearmanr&lt;/code&gt; to compute the Pearson and Spearman correlation&lt;/p&gt;
&lt;p&gt;print sc.pearsonr(studydf.gpa,studydf.hrs_studied)
print sc.spearmanr(studydf.gpa,studydf.hrs_studied)&lt;/p&gt;
&lt;p&gt;(0.47594026466220946, 0.0)
(0.98495916559333341, 0.0)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat step &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; for &lt;code&gt;gpa&lt;/code&gt; and &lt;code&gt;sport_performance&lt;/code&gt;. Is there a strong relationship between the two variables?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;slope, intercept, r_value, p_value, std_err = sc.linregress(studydf.gpa,studydf.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(studydf.gpa.min(),studydf.gpa.max(),100)
y = slope*x+intercept

studydf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()

0.00979813693421 0.0585103217504 0.0238485969548 0.0124044928587
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_57_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;print sc.pearsonr(studydf.gpa,studydf.sport_performance)
print sc.spearmanr(studydf.gpa,studydf.sport_performance)

(0.023848596954761905, 0.012404492858691094)
(0.0022881402736224248, 0.81043264616449484)



temp = studydf[studydf.gpa &amp;gt; 3.0]
slope, intercept, r_value, p_value, std_err = sc.linregress(temp.gpa,temp.sport_performance)
print slope, intercept, r_value, p_value
x = np.linspace(temp.gpa.min(),temp.gpa.max(),100)
y = slope*x+intercept

temp.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;gpa&amp;#39;,y=&amp;#39;sport_performance&amp;#39;,alpha=0.1)
plt.plot(x,y,color=&amp;#39;black&amp;#39;)
plt.show()
print sc.pearsonr(temp.gpa,temp.sport_performance)
print sc.spearmanr(temp.gpa,temp.sport_performance)

0.65608660543 -2.03523616554 0.945140987506 0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_59_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;(0.94514098750633013, 0.0)
(1.0, 0.0)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Part 4: Distribution Simulation&lt;/h2&gt;
&lt;p&gt;Often times in real life applications, we can specify the values of a variable to be of a particular distribution,
for example the number of sales made in the next month can be modeled as a uniform distribution over the range of
5000 and 6000.&lt;/p&gt;
&lt;p&gt;In this scenario, we are modeling &lt;code&gt;profit&lt;/code&gt; as a product of &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt;,
where &lt;code&gt;number of views&lt;/code&gt;, &lt;code&gt;conversion&lt;/code&gt; and &lt;code&gt;profit per sale&lt;/code&gt; can be modeled as probabilistic distributions.
By randomly drawing values from these distributions, we are able to get a distribution of the range of &lt;code&gt;profit&lt;/code&gt; 
based on the uncertainties in the other variables.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Profit = Number of views * Conversion * Profit per sale&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Assumptions:
- &lt;code&gt;Number of views&lt;/code&gt; is a uniform distribution over the range of 5000 and 6000
- &lt;code&gt;Conversion is a binomial distribution where the probability of success is&lt;/code&gt;0.12&lt;code&gt;for each sale among the&lt;/code&gt;Number on views made 
- &lt;code&gt;Profit per sale&lt;/code&gt; has &lt;code&gt;0.2&lt;/code&gt; probability of taking the value &lt;code&gt;50&lt;/code&gt; (for wholesale) and &lt;code&gt;0.8&lt;/code&gt; of 
  taking the value &lt;code&gt;60&lt;/code&gt; (non-wholesale)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given the distributions of each of variables, use scipy to write a function that would draw random values from each of the distributions to simulate a distribution for &lt;code&gt;profit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;def get_profit():
    num_views = np.round(sc.uniform.rvs(loc=5000,scale=1000,size=1),0)
    conversions = sc.binom.rvs(num_views,0.12)
    wholesale = sc.binom.rvs(conversions,0.2)
    return wholesale&lt;em&gt;50+(conversions-wholesale)&lt;/em&gt;60&lt;/p&gt;
&lt;p&gt;profits = np.array([get_profit() for i in xrange(100000)])
plt.hist(profits)
plt.show()
print "Low: ",np.percentile(profits,2.5)
print "High: ",np.percentile(profits,97.5)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/GW2D1/output_61_0.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Low&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;33800.0&lt;/span&gt;
&lt;span class="n"&gt;High&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;42920.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 5</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-05/" rel="alternate"></link><updated>2015-06-05T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-05:galvanize/galvanize-data-science-01-05/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 5&lt;/h2&gt;
&lt;p&gt;This morning we started with a reflection about the week, and completed a survey about our progress and thoughts on the program.   I think it is a very strong, hands-on program so far.&lt;/p&gt;
&lt;p&gt;The morning lesson and sprint was on using (pandas)[http://pandas.pydata.org/].   We were given some hospital data in a CSV format, read in the data, and answered a number of questions about most common diseases, most expensive procedures, the post profitable hospitals, and various subsets of these question on different conditions.  It was a simple exercise that gave us practice making new variables, grouping, and subsetting to look massage the data into a form that allowed us to answer the questions.&lt;/p&gt;
&lt;p&gt;During our lunch we had a presentation on learning, and the approach and attitudes that facilitate the bests learning.  It was partly motivational and partly reflective.  If you are familiar with Carol Dweck's work and the research it created, then you have a feeling for the talk.&lt;/p&gt;
&lt;p&gt;After lunch we did a fun assignment that was to recreate the MoneyBall movie where we are trying to get a set of three players that in aggregate replace the 3 key players that were just lost.   &lt;/p&gt;
&lt;p&gt;The data we used is hosted (here)[http://www.seanlahman.com/baseball-archive/statistics/]&lt;/p&gt;
&lt;h2&gt;Money Ball&lt;/h2&gt;
&lt;p&gt;We first started by downloading the dataset and loading it into Pandas.  We started with the batting data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;batting&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/baseball-csvs/Batting.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SB&lt;/th&gt;
      &lt;th&gt;CS&lt;/th&gt;
      &lt;th&gt;BB&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SFN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHN&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CHA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BOS&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;aardsda01&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows  24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We then loaded the salaryd data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;salary = pd.read_csv(&amp;#39;data/baseball-csvs/Salaries.csv&amp;#39;)
salary.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;teamID&lt;/th&gt;
      &lt;th&gt;lgID&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;murraed02&lt;/td&gt;
      &lt;td&gt;1472819&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lynnfr01&lt;/td&gt;
      &lt;td&gt;1090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;ripkeca01&lt;/td&gt;
      &lt;td&gt;800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;lacyle01&lt;/td&gt;
      &lt;td&gt;725000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1985&lt;/td&gt;
      &lt;td&gt;BAL&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;flanami01&lt;/td&gt;
      &lt;td&gt;641667&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;salary[salary.yearID==2001].salary.describe()




count         860.000000
mean      2279841.061628
std       2907710.250521
min        200000.000000
25%        269375.000000
50%        925000.000000
75%       3250000.000000
max      22000000.000000
Name: salary, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the year 2001, the year we are concerned with, the minimum salary was $200,000.&lt;/p&gt;
&lt;p&gt;The next thing we did was merged the two dataframes and limited the data to 2001.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf = batting.merge(salary,on=[&amp;#39;playerID&amp;#39;,&amp;#39;yearID&amp;#39;],how=&amp;#39;left&amp;#39;)
mergeddf = mergeddf[mergeddf.yearID==2001]
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows  27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can see some of the salaries are missing.  There are players that can be aquired, but are not on a payroll.   If we pick them up we have to pay them 200,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.salary = mergeddf.salary.fillna(200000)
mergeddf.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;yearID&lt;/th&gt;
      &lt;th&gt;stint&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;lgID_x&lt;/th&gt;
      &lt;th&gt;G&lt;/th&gt;
      &lt;th&gt;G_batting&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;R&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SO&lt;/th&gt;
      &lt;th&gt;IBB&lt;/th&gt;
      &lt;th&gt;HBP&lt;/th&gt;
      &lt;th&gt;SH&lt;/th&gt;
      &lt;th&gt;SF&lt;/th&gt;
      &lt;th&gt;GIDP&lt;/th&gt;
      &lt;th&gt;G_old&lt;/th&gt;
      &lt;th&gt;teamID_y&lt;/th&gt;
      &lt;th&gt;lgID_y&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;abadan01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;abbotje01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;FLO&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44&lt;/th&gt;
      &lt;td&gt;abbotku01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
      &lt;td&gt;600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;abbotpa01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SEA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;1700000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;abernbr01&lt;/td&gt;
      &lt;td&gt;2001&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;AL&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;304&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;200000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows  27 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now we need to make some new variables.   The number of times they were on First Base,the Batting Average (BA), the On Base Percentage (OBP), and the Slugg (SLG)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf[&amp;#39;BA&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf.BA.describe()




count    1044.000000
mean        0.202532
std         0.140697
min         0.000000
25%         0.117647
50%         0.235227
75%         0.274705
max         1.000000
Name: BA, dtype: float64




mergeddf[&amp;#39;1B&amp;#39;] = mergeddf[&amp;#39;H&amp;#39;]-mergeddf[&amp;#39;2B&amp;#39;]-mergeddf[&amp;#39;3B&amp;#39;]-mergeddf[&amp;#39;HR&amp;#39;]
mergeddf[&amp;#39;1B&amp;#39;].describe()




count    1237.000000
mean       23.185125
std        34.327716
min         0.000000
25%         0.000000
50%         4.000000
75%        33.000000
max       192.000000
Name: 1B, dtype: float64




mergeddf[&amp;#39;SLG&amp;#39;]=(mergeddf[&amp;#39;1B&amp;#39;]+2*mergeddf[&amp;#39;2B&amp;#39;]+3*mergeddf[&amp;#39;3B&amp;#39;] \
                 +4*mergeddf[&amp;#39;HR&amp;#39;])/mergeddf[&amp;#39;AB&amp;#39;]
mergeddf[&amp;#39;SLG&amp;#39;].describe()




count    1044.000000
mean        0.303628
std         0.214569
min         0.000000
25%         0.142857
50%         0.337722
75%         0.436874
max         2.000000
Name: SLG, dtype: float64




mergeddf[&amp;#39;OBP&amp;#39;]=(mergeddf[&amp;#39;H&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]) \
/(mergeddf[&amp;#39;AB&amp;#39;]+mergeddf[&amp;#39;BB&amp;#39;]+mergeddf[&amp;#39;HBP&amp;#39;]+mergeddf[&amp;#39;SF&amp;#39;])
mergeddf[&amp;#39;OBP&amp;#39;].describe()




count    1047.000000
mean        0.254084
std         0.159932
min         0.000000
25%         0.162162
50%         0.293103
75%         0.338235
max         1.000000
Name: OBP, dtype: float64
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The A's lost Jason Giambi (&lt;code&gt;giambja01&lt;/code&gt;), Johnny Damon (&lt;code&gt;damonjo01&lt;/code&gt;), Jason Isringhausen (&lt;code&gt;isrinja01&lt;/code&gt;), and Rainer Gustavo "Ray" Olmedo (&lt;code&gt;'saenzol01'&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These player need to replaced with similar player that bat, in total, as much as these guys, get on base as often as these guys, and can be payed less than these guys.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;my_mask = mergeddf[&amp;#39;playerID&amp;#39;].isin([&amp;#39;giambja01&amp;#39;,&amp;#39;damonjo01&amp;#39;,&amp;#39;isrinja01&amp;#39;,&amp;#39;saenzol01&amp;#39;])
lostboysdf = mergeddf[my_mask]
imp_var = [&amp;#39;playerID&amp;#39;, &amp;#39;teamID_x&amp;#39;,&amp;#39;AB&amp;#39;,&amp;#39;HR&amp;#39;, &amp;#39;OBP&amp;#39;, &amp;#39;SLG&amp;#39;, &amp;#39;salary&amp;#39;]
lostboysdf[imp_var]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;playerID&lt;/th&gt;
      &lt;th&gt;teamID_x&lt;/th&gt;
      &lt;th&gt;AB&lt;/th&gt;
      &lt;th&gt;HR&lt;/th&gt;
      &lt;th&gt;OBP&lt;/th&gt;
      &lt;th&gt;SLG&lt;/th&gt;
      &lt;th&gt;salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;7065&lt;/th&gt;
      &lt;td&gt;damonjo01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;644&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.323529&lt;/td&gt;
      &lt;td&gt;0.363354&lt;/td&gt;
      &lt;td&gt;7100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10836&lt;/th&gt;
      &lt;td&gt;giambja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;520&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0.476900&lt;/td&gt;
      &lt;td&gt;0.659615&lt;/td&gt;
      &lt;td&gt;4103333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14911&lt;/th&gt;
      &lt;td&gt;isrinja01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27408&lt;/th&gt;
      &lt;td&gt;saenzol01&lt;/td&gt;
      &lt;td&gt;OAK&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.291176&lt;/td&gt;
      &lt;td&gt;0.383607&lt;/td&gt;
      &lt;td&gt;290000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print &amp;quot;Avg OBP Needed to be replaced:&amp;quot;, 3*lostboysdf[imp_var].OBP.mean()
print &amp;quot;Total Bats needed to be replaced:&amp;quot;, lostboysdf[imp_var].AB.sum()

Avg OBP Needed to be replaced: 1.09160603138
Total Bats needed to be replaced: 1469.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We would ideally like to get every combination of 3 players that are available.  That would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;factorial&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;nCr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;1335&lt;/span&gt;
&lt;span class="mi"&gt;395654395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is almost 400,000,000 combinations to search through.   Less make some reasonable assumptions about about the minimum At Bats and On Base Percentages&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mergeddf.plot(kind=&amp;#39;scatter&amp;#39;,x=&amp;#39;AB&amp;#39;,y=&amp;#39;OBP&amp;#39;)




&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1063f3f10&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://www.bryantravissmith.com/img/gw1d5_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the variance does not change much above 200 AB.  We probably want them to bat about 500 times a season, however.  We also want the average to be above 0.33 for the OPB, so that seems like a reasonable initial cutoff.  We also want there average salary to be less than 5000000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;size = len(mergeddf[(mergeddf.AB &amp;gt; 400) &amp;amp; (mergeddf.OBP &amp;gt; 0.33)])
nCr(size,3)




246905L
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us 11,480 combinations to search through.  Lets do it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#mdf = mergeddf[(mergeddf.yearID==2001)&amp;amp;(mergeddf.AB&amp;gt;50)] #.salary.describe()&lt;/span&gt;
&lt;span class="c"&gt;#mdf.salary = mdf.salary.fillna(200000)&lt;/span&gt;
&lt;span class="c"&gt;#mdf[imp_var].head()&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mergeddf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;
&lt;span class="n"&gt;good_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;giambja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;damonjo01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;isrinja01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;saenzol01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;player3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_AB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_OBP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;salary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_AB&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;playerID&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OBP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;15000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0961&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;total_AB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_obp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_salary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;total_salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;player1&lt;/th&gt;
      &lt;th&gt;player2&lt;/th&gt;
      &lt;th&gt;player3&lt;/th&gt;
      &lt;th&gt;total_AB&lt;/th&gt;
      &lt;th&gt;total_OBP&lt;/th&gt;
      &lt;th&gt;total_salary&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;28&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1.261767&lt;/td&gt;
      &lt;td&gt;5338333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;32&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1754&lt;/td&gt;
      &lt;td&gt;1.264850&lt;/td&gt;
      &lt;td&gt;5455000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;martied01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1637&lt;/td&gt;
      &lt;td&gt;1.256603&lt;/td&gt;
      &lt;td&gt;6005000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;edmonji01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1667&lt;/td&gt;
      &lt;td&gt;1.243410&lt;/td&gt;
      &lt;td&gt;6838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;38&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;olerujo01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1739&lt;/td&gt;
      &lt;td&gt;1.234375&lt;/td&gt;
      &lt;td&gt;7205000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gilesbr02&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1743&lt;/td&gt;
      &lt;td&gt;1.236756&lt;/td&gt;
      &lt;td&gt;7838333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;alomaro01&lt;/td&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1742&lt;/td&gt;
      &lt;td&gt;1.247866&lt;/td&gt;
      &lt;td&gt;8255000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;43&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;thomeji01&lt;/td&gt;
      &lt;td&gt;1693&lt;/td&gt;
      &lt;td&gt;1.249345&lt;/td&gt;
      &lt;td&gt;8380000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;55&lt;/th&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;pujolal01&lt;/td&gt;
      &lt;td&gt;1786&lt;/td&gt;
      &lt;td&gt;1.263189&lt;/td&gt;
      &lt;td&gt;9983333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;berkmla01&lt;/td&gt;
      &lt;td&gt;gonzalu01&lt;/td&gt;
      &lt;td&gt;heltoto01&lt;/td&gt;
      &lt;td&gt;1773&lt;/td&gt;
      &lt;td&gt;1.290459&lt;/td&gt;
      &lt;td&gt;10088333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;print len(gc)

66
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This allowed us to find 66 combination of players that would, in aggregate, have better statistics that the players that were lost.  It also turns out to be cheaper to do that.   This is the story of money ball.  It was a fun project.  &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="pandas"></category><category term="money ball"></category></entry><entry><title>Galvanize - Week 01 - Day 4</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-04/" rel="alternate"></link><updated>2015-06-04T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-04:galvanize/galvanize-data-science-01-04/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 4&lt;/h2&gt;
&lt;p&gt;The day started out with a mini-quiz on object-oriented programming, and that was followed by an introduction to git and sophisticated join queries.   Our instructor for the day used to work at Facebook, and she walked us through some the queries she would do on the job.  &lt;/p&gt;
&lt;p&gt;She then gave us a simulated data set that match the structure, but not the content, of Facebook tables and we had an individual sprint attempting to complete 10 queries in 2 hours.&lt;/p&gt;
&lt;p&gt;After lunch we had a lecture on pyscopg2, a python library to use to connect and interact with a PostgreSQL server.   We ran a server locally, loaded with the same data as the morning, and were given an assignment to construct a pipeline that we could run each day to give us an updated status of our users.   We were to check on results for today being set to Aug 14, 2014. &lt;/p&gt;
&lt;p&gt;Our resulting script is below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;psycopg2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;2014-08-14&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;%Y-%M-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;CREATE TABLE logins_7d_%s AS&lt;/span&gt;
&lt;span class="sd"&gt;    WITH&lt;/span&gt;
&lt;span class="sd"&gt;    main AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        r.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        tmstmp::date AS reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        CASE WHEN optout.userid IS NULL then 0 ELSE 1 END AS opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM registrations r&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN optout&lt;/span&gt;
&lt;span class="sd"&gt;    ON r.userid = optout.userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY r.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid,&lt;/span&gt;
&lt;span class="sd"&gt;        MAX(tmstmp::date) AS last_login&lt;/span&gt;
&lt;span class="sd"&gt;    FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid&lt;/span&gt;
&lt;span class="sd"&gt;    ORDER BY userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7d&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7m AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT t.userid, COUNT(t.dt) AS logins_7m&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;mobile&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    last7w AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        t.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(t.dt) AS logins_7w&lt;/span&gt;
&lt;span class="sd"&gt;    FROM (&lt;/span&gt;
&lt;span class="sd"&gt;        SELECT&lt;/span&gt;
&lt;span class="sd"&gt;            DISTINCT userid,&lt;/span&gt;
&lt;span class="sd"&gt;            tmstmp::date AS dt&lt;/span&gt;
&lt;span class="sd"&gt;        FROM logins&lt;/span&gt;
&lt;span class="sd"&gt;        WHERE&lt;/span&gt;
&lt;span class="sd"&gt;            logins.tmstmp &amp;gt; timestamp &amp;#39;2014-08-14&amp;#39; - interval &amp;#39;7 days&amp;#39; AND&lt;/span&gt;
&lt;span class="sd"&gt;            logins.type = &amp;#39;web&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;        GROUP BY userid, tmstmp::date&lt;/span&gt;
&lt;span class="sd"&gt;        ORDER BY userid) t&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY t.userid),&lt;/span&gt;
&lt;span class="sd"&gt;    uf1 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT * FROM friends)&lt;/span&gt;
&lt;span class="sd"&gt;    UNION ALL&lt;/span&gt;
&lt;span class="sd"&gt;    (SELECT userid2, userid1 FROM friends)),&lt;/span&gt;
&lt;span class="sd"&gt;    uf2 AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT DISTINCT *&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf1),&lt;/span&gt;
&lt;span class="sd"&gt;    friend_cnt AS (&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        userid1 AS userid,&lt;/span&gt;
&lt;span class="sd"&gt;        COUNT(1) AS num_friends&lt;/span&gt;
&lt;span class="sd"&gt;    FROM uf2&lt;/span&gt;
&lt;span class="sd"&gt;    GROUP BY userid)&lt;/span&gt;
&lt;span class="sd"&gt;    SELECT&lt;/span&gt;
&lt;span class="sd"&gt;        main.userid,&lt;/span&gt;
&lt;span class="sd"&gt;        reg_date,&lt;/span&gt;
&lt;span class="sd"&gt;        last_login,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7d,0) AS logins_7d,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7m,0) AS logins_7d_mobile,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(logins_7w,0) AS logins_7d_web,&lt;/span&gt;
&lt;span class="sd"&gt;        coalesce(num_friends,0) AS num_friends,&lt;/span&gt;
&lt;span class="sd"&gt;        opt_out&lt;/span&gt;
&lt;span class="sd"&gt;    FROM main&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7m&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7m.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN last7w&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = last7w.userid&lt;/span&gt;
&lt;span class="sd"&gt;    LEFT OUTER JOIN friend_cnt&lt;/span&gt;
&lt;span class="sd"&gt;    ON main.userid = friend_cnt.userid;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;timestamp&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;commit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also learned how pull data and load the data into a pandas dataframe.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sql&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas.io.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;socialmedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;postgres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sql&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;SELECT * FROM logins_7d_1389686880 LIMIT 20;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;userid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coerce_float&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reg_date&lt;/th&gt;
      &lt;th&gt;last_login&lt;/th&gt;
      &lt;th&gt;logins_7d&lt;/th&gt;
      &lt;th&gt;logins_7d_mobile&lt;/th&gt;
      &lt;th&gt;logins_7d_web&lt;/th&gt;
      &lt;th&gt;num_friends&lt;/th&gt;
      &lt;th&gt;opt_out&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;userid&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2014-06-23&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-12-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2014-04-18&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-12-17&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2014-08-09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2013-08-18&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2014-03-21&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2014-05-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2014-06-06&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2013-08-31&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2013-08-16&lt;/td&gt;
      &lt;td&gt;2014-08-10&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2013-09-12&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2014-07-29&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2013-11-03&lt;/td&gt;
      &lt;td&gt;2014-08-11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2013-10-09&lt;/td&gt;
      &lt;td&gt;2014-08-13&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2014-02-16&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;2014-04-20&lt;/td&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;2014-07-02&lt;/td&gt;
      &lt;td&gt;2014-08-12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;2014-08-14&lt;/td&gt;
      &lt;td&gt;2014-05-10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Today was a very intense day.   But the programming is delivering on what it promised: Hands On Learning From Experience Professions!&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category><category term="psycopg2"></category></entry><entry><title>Galvanize - Week 01 - Day 3</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-03/" rel="alternate"></link><updated>2015-06-03T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-03:galvanize/galvanize-data-science-01-03/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 3&lt;/h2&gt;
&lt;p&gt;Today was an 'introduction' to SQL and PostgreSQL.  I put introduction in quotes because it does not properly describe what we did.  The pre-reading was to complete all 9 (1-9) tutorials on &lt;a href="http://sqlzoo.net/"&gt;SQLZoo&lt;/a&gt;.  This took me about 5 hours.   During lecture we have a review of the order of operation of SQL queries, as well as a detailed explanation of joins.    &lt;/p&gt;
&lt;p&gt;The sprint for the day involved install &lt;a href="http://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; locally, loading a database into it, then completing ~25 basic and 10 advance (extra credit) queries.   Our database had 3 tables with 300k, 500k, and 5k entries respectively.&lt;/p&gt;
&lt;p&gt;These were a great set of assignment because of how they 'leveled-up'.  Even the few among us that were sophisticated with SQL had difficulty with the advance problems.&lt;/p&gt;
&lt;p&gt;Now that I have completed ~10 hours of SQL queries today, I am going to end this post now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT * 
FROM bryan JOIN bed 
ON bryan.location=bed.location 
AND bryan.state=&amp;#39;sleep&amp;#39; 
AND bed.state=&amp;#39;comfy&amp;#39;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="sql"></category><category term="postgresql"></category></entry><entry><title>Galvanize - Week 01 - Day 2</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-02/" rel="alternate"></link><updated>2015-06-02T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-02:galvanize/galvanize-data-science-01-02/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 2&lt;/h2&gt;
&lt;p&gt;Today was he first 'regular' day in the program.  I showed up about 90 minutes before the mini-quiz to review the readings for the day's lecture on Object Oriented Programming (OOP).   At 9:30 we started the mini-quiz on SQL statements and results.  I found it rather simple.  We were given 30 minutes to complete it, and I finished in about 10 minutes.  Most the topics involved analogs in pandas that I am familiar with, so I think that's why I finished rather quickly.&lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;We had two lectures today.  The first lecture was on object oriented structures, and how to implement them in python.  The afternoon lecture was on scoping in python, and a little bit of debugging.  We were introduced to pdb, but told that the use of debuggers is not well integrated in the data science community.&lt;/p&gt;
&lt;h3&gt;LEGB&lt;/h3&gt;
&lt;p&gt;We were told the variables are looked for in the order of local, enclosing function, global, and python build-in.   I made a set of functions to try to illustrate it for myself.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;locally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;span class="nx"&gt;printer1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;built&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer4&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;encapsulating&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;globally&lt;/span&gt; &lt;span class="nx"&gt;finds&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt;
    &lt;span class="nx"&gt;innerprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;printer6&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Paired Programming Sprint&lt;/h2&gt;
&lt;p&gt;Today's project involved programming a text based game of black jack with a dealer and 1 number of players.  We also had the extra credit options adding n-players, AI/Bot players, double down, and split.  I am happy to report that we that my partner and I were able to complete the first three, but ran out of time before implementing split.&lt;/p&gt;
&lt;p&gt;We started off with pencil and paper using the noun,verb method of abstraction.   We settled on making a Deck, a Player, and Hand, and Game, and an AI.    Its clear at the end that we should have abstracted the game more, and given the Hand class more responsibilities to best implement the split method.&lt;/p&gt;
&lt;p&gt;After we finished we had our dealer hit until 17 or above, while our AI bot hit until soft 17 below.   We also had it implement a doubling bettering strategy.   In our sample, the AI agent one more often then not came out ahead from this setup.   It added a little credence to the ways dealer's seem to play in Las Vegas.&lt;/p&gt;
&lt;p&gt;The repo is currently private, because it could be a project for future cohorts.   I do not want to make a copy public, but will if they give permission.&lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry><entry><title>Galvanize - Week 01 - Day 1</title><link href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-01/" rel="alternate"></link><updated>2015-06-01T10:30:00-07:00</updated><author><name>Bryan Smith</name></author><id>tag:www.bryantravissmith.com,2015-06-01:galvanize/galvanize-data-science-01-01/</id><summary type="html">&lt;h1&gt;Galvanize Immersive Data Science&lt;/h1&gt;
&lt;h2&gt;Week 1 - Day 1&lt;/h2&gt;
&lt;p&gt;Today is my first day attending Galvanize's Immersive Data Science Program in San Francisco, CA.   The program is a 12 week program that is approximately 10 hours a day of learning and activities to reinforce and refine the learning.   I am very excited to be a part of this program.&lt;/p&gt;
&lt;h2&gt;My Background&lt;/h2&gt;
&lt;p&gt;I have a Ph.d in Theoretical High Energy Particle Physics and Cosmology, earned a Data Analysis Nano-degree from Udacity.com, and also am current working on a M.S. in Computer Science from GA Tech.    I have also spent the last 8 years teaching high school physics and robotics.   &lt;/p&gt;
&lt;p&gt;I will likely have some strength with math and theory, but I have no doubt that my programming will significantly improve over the next 12 weeks.   Everyone in the program is well educated and intelligent, and each one of them have strengths in some areas and room for improvements in others.  It seems to be a strength for this program.   No matter your weakness, there are students that have that as a strength.&lt;/p&gt;
&lt;h2&gt;Summary of the Day&lt;/h2&gt;
&lt;p&gt;The first half of the day is getting to know our instructors, hour cohort, and the amazingly nice galvanize complex.  It is a 5 story building filled with startup companies, work spaces, galvanize students, and other tech visitors.   I found this to be an impressive building.&lt;/p&gt;
&lt;p&gt;After about an hour of getting to know everyone, we were given a presentation.  That was followed by a tour of the galvanize building.  We were then given an assessment on the pre-course material that was given to us before we showed up.&lt;/p&gt;
&lt;p&gt;After lunch, we had an 90 minute lecture, then worked on a paired sprint assignment for about 3 hours.   This assignment involved...&lt;/p&gt;
&lt;p&gt;After we finished the sprint, we were invited to a Galvanize happy hour to socialize over beer and wine.   I feel very lucky to be apart of this program, and have been impressed with my cohort, the instructors, and Galvanize.  &lt;/p&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;The pre-course material required us to complete material on the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Numpy/Pandas&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Web Awareness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The initial assessment we were given was a 120 minute test on the first 5 topics.  It was an 'open book' test, but that does not mean it was easy.  A majority of my peers did not finish within the allotted time.   &lt;/p&gt;
&lt;p&gt;I am not going to post details on the test because I would hate to ruin the thrill of discovery for potential future students.&lt;/p&gt;
&lt;h2&gt;LUNCH!!!!&lt;/h2&gt;
&lt;p&gt;They provide us a lunch on the first day, but most days we have an 75 minute break for lunch.  There are kitchen, fridges, storage for us to use if we wish.   The lunch was nice, from a local Thai place.   &lt;/p&gt;
&lt;h2&gt;Lecture&lt;/h2&gt;
&lt;p&gt;The lecture, in my opinion, was a little redundant with the course material.  It seemed structure under the assumption that you didn't read or review the python pre-course materials.  I understand its important that everyone is on the same starting point, but I wish we got to jump in a little deeper.&lt;/p&gt;
&lt;p&gt;I did learn and see the importance of using generators when possible.   It save both memory and time.   &lt;/p&gt;
&lt;h2&gt;Paired programming&lt;/h2&gt;
&lt;p&gt;After the lecture we grouped up for a paired programming assignment.   We trade off roles of being the driver and the navigator in 20 to 30 minute rotations for a 3 hour block of programming.  We start of by forking the day's assignment from a Github repo and cloning it locally.  Today we then worked two projects.  The first project was completing a list of functions based on a description of the function, including inputs and outputs.  The second project was fixing inefficiently running code.&lt;/p&gt;
&lt;p&gt;A simple example is checking if a key is in a dictionary.   Before today I might have checked to see if it was in the keys() results, but we can see that for medium size dictionaries that it is almost 40x slower than just using in in the dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;randint&lt;/span&gt;

&lt;span class="n"&gt;cnt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;

&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.17&lt;/span&gt; &lt;span class="err"&gt;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="mi"&gt;10000000&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;111&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We saw similar results for iter compared to iteritems, range to xrange, and izip to zip.   It was a useful assignment for the content and the practice of collaborating with someone else.&lt;/p&gt;
&lt;h2&gt;After reception&lt;/h2&gt;
&lt;p&gt;Galvanize SF now runs two cohorts 6 weeks apart.  We had a mixer with previous cohort, enjoying beer, wine, and conversation on the roof of the building.  &lt;/p&gt;
&lt;p&gt;After 11 hours at Galvanize, I decided it was time to head home.   Definitely looking forward to day 2. &lt;/p&gt;</summary><category term="data-science"></category><category term="galvanize"></category><category term="python"></category></entry></feed>