<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-02/index.html" />

    <title>  Bryan Travis Smith, Ph.D &mdash; Galvanize - Week 04 - Day 2
</title>




    <link rel="stylesheet" href="http://www.bryantravissmith.com/theme/css/style.css">

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-24340005-3', 'auto');
    ga('send', 'pageview');

  </script>

    <meta name="author" content="Bryan Smith">
    <meta name="description" content="Today we covered random forests.">
  <meta name="tags" contents="data-science, galvanize, Random Forests, sklearn, ">
</head>

<body>
<header class="header">
  <div class="container">
      <div class="header-image pull-left">
        <a class="nodec" href="http://www.bryantravissmith.com"><img src=http://www.bryantravissmith.com/img/bryan.jpeg></a>
      </div>
    <div class="header-inner">
      <h1 class="header-name">
        <a class="nodec" href="http://www.bryantravissmith.com">Bryan Travis Smith, Ph.D</a>
      </h1>
      <h3 class="header-text">Physicist, Data Scientist, Martial Artist, & Life Enthusiast</h3>
      <ul class="header-menu list-inline">
              <li class="muted">|</li>
            <li><a class="nodec" href="http://www.bryantravissmith.com/about/">About</a></li>
              <li class="muted">|</li>
          <li><a class="nodec icon-mail-alt" href="mailto:bryantravissmith@gmail.com"></a></li>
          <li><a class="nodec icon-github" href="https://github.com/bryantravissmith"></a></li>
      </ul>
    </div>
  </div>
</header> <!-- /.header -->  <div class="container">
  <div class="post full-post">
    <h1 class="post-title">
      <a href="/galvanize/galvanize-data-science-04-02/" title="Permalink to Galvanize - Week 04 - Day 2">Galvanize - Week 04 - Day 2</a>
    </h1>
    <ul class="list-inline">
      <li class="post-date">
        <a class="text-muted" href="/galvanize/galvanize-data-science-04-02/" title="2015-06-23T10:20:00-07:00">Tue 23 June 2015</a>
      </li>
      <li class="muted">&middot;</li>
      <li class="post-category">
        <a href="http://www.bryantravissmith.com/category/galvanize.html">Galvanize</a>
      </li>
        <li class="muted">&middot;</li>
        <li>
          <address class="post-author">
            By <a href="http://www.bryantravissmith.com/author/bryan-smith.html">Bryan Smith</a>
          </address>
        </li>
    </ul>
    <div class="post-content">
      <h1>Galvanize Immersive Data Science</h1>
<h2>Week 4 - Day 2</h2>
<p>Today's quiz a an online interview questions that involves picking a random value from a stream in order 1 memory use. 
After that was a lecture on Bootstrap Aggregate (Bagging) ML methods, focused on decisions trees, then the random forest algorithm.   We implemented a random forest using our decision trees we made from yesterday.</p>
<h2>Random Forest Class</h2>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">DecisionTree</span> <span class="kn">import</span> <span class="n">DecisionTree</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">RandomForest</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;A Random Forest class&#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_trees</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">           num_trees:  number of trees to create in the forest:</span>
<span class="sd">        num_features:  the number of features to consider when choosing the</span>
<span class="sd">                           best split for each node of the decision trees</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_trees</span> <span class="o">=</span> <span class="n">num_trees</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forest</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        X:  two dimensional numpy array representing feature matrix</span>
<span class="sd">                for test data</span>
<span class="sd">        y:  numpy array representing labels for test data</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_forest</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_forest</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_trees</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Return a list of num_trees DecisionTrees.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">forest</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">size_feature</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">num_features</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">size_feature</span> <span class="o">=</span> <span class="n">num_features</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trees</span><span class="p">):</span>

            <span class="c">#features = np.random.choice(range(len(X[0,:])),size=size_feature,replace=False)</span>
            <span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTree</span><span class="p">(</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span><span class="p">)</span>
            <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span><span class="kp">size</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>
            <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">indexes</span><span class="p">,:],</span><span class="n">y</span><span class="p">[</span><span class="n">indexes</span><span class="p">])</span>
            <span class="n">forest</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">forest</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Return a numpy array of the labels predicted for the given test data.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forest</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forest</span><span class="p">)):</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">hstack</span><span class="p">((</span><span class="n">predictions</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">forest</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)))</span>

        <span class="n">final_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
            <span class="n">labels</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">unique</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">final_predictions</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="kp">argmax</span><span class="p">(</span><span class="n">counts</span><span class="p">)])</span>

        <span class="k">return</span> <span class="n">final_predictions</span>

    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Return the accuracy of the Random Forest for the given test data and</span>
<span class="sd">        labels.</span>

<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="kp">sum</span><span class="p">(</span><span class="n">prediction</span><span class="o">==</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="kp">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<h2>Checking Our Random Forest</h2>
<div class="highlight"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">DecisionTree</span> <span class="kn">import</span> <span class="n">DecisionTree</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;../data/playgolf.csv&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&#39;Result&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTree</span><span class="p">()</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span> <span class="n">dt</span>

<span class="mi">0</span>
  <span class="o">|-&gt;</span> <span class="n">overcast</span><span class="p">:</span>
  <span class="o">|</span>     <span class="n">Play</span>
  <span class="o">|-&gt;</span> <span class="n">no</span> <span class="n">overcast</span><span class="p">:</span>
  <span class="o">|</span>     <span class="mi">1</span>
  <span class="o">|</span>     <span class="o">|-&gt;</span> <span class="o">&lt;</span> <span class="mi">71</span><span class="p">:</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="n">Play</span>
  <span class="o">|</span>     <span class="o">|-&gt;</span> <span class="o">&gt;=</span> <span class="mi">71</span><span class="p">:</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="mi">2</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="o">|-&gt;</span> <span class="o">&lt;</span> <span class="mi">80</span><span class="p">:</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="o">|</span>     <span class="n">Play</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="o">|-&gt;</span> <span class="o">&gt;=</span> <span class="mi">80</span><span class="p">:</span>
  <span class="o">|</span>     <span class="o">|</span>     <span class="o">|</span>     <span class="n">Don</span><span class="s">&#39;t Play</span>
</pre></div>


<p>Our Decision tree is still functioning.  Since the random forest depends on that I wanted to just show that it functions.  The idea is that if we have a weak classifier we can use it a large number of them to vote on a decisions and find, on average, a better answer.  </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">RandomForest</span> <span class="kn">import</span> <span class="n">RandomForest</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForest</span><span class="p">(</span><span class="n">num_trees</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predict</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;score:&quot;</span><span class="p">,</span> <span class="n">rf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span><span class="n">y_predict</span><span class="p">,</span><span class="n">y_test</span>


<span class="n">score</span><span class="p">:</span> <span class="mf">0.5</span> <span class="p">[</span><span class="s">&#39;Play&#39;</span><span class="p">,</span> <span class="s">&#39;Play&#39;</span><span class="p">,</span> <span class="s">&#39;Play&#39;</span><span class="p">,</span> <span class="s">&#39;Play&#39;</span><span class="p">]</span> <span class="p">[</span><span class="s">&#39;Play&#39;</span> <span class="s">&#39;Play&#39;</span> <span class="s">&quot;Don&#39;t Play&quot;</span> <span class="s">&quot;Don&#39;t Play&quot;</span><span class="p">]</span>
</pre></div>


<p>The random forest, unsurprisingly, does not do very well on the golf data set.  It is very small, and the rules are complicated.   Lets try it on some congress data.</p>
<h2>Comparision with Sklearn</h2>
<div class="highlight"><pre><span class="n">congress</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;../data/congressional_voting.csv&quot;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">congress</span> <span class="o">=</span> <span class="n">congress</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;republican&#39;</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;democrat&#39;</span><span class="p">,</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;y&#39;</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;n&#39;</span><span class="p">,</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;?&#39;</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">congress</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">congress</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">cv_score</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">skcv_score</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="p">:</span>
    <span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForest</span><span class="p">(</span><span class="n">num_trees</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">skrf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">])</span>
    <span class="n">skrf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">])</span>
    <span class="n">cv_score</span> <span class="o">+=</span> <span class="n">rf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">])</span><span class="o">/</span><span class="mf">10.</span>
    <span class="n">skcv_score</span> <span class="o">+=</span> <span class="n">skrf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">])</span><span class="o">/</span><span class="mf">10.</span>

<span class="k">print</span> <span class="s">&quot;                       CV Accuracy    Test Accuracy&quot;</span>
<span class="k">print</span> <span class="s">&quot;My RF Scores:         &quot;</span><span class="p">,</span> <span class="n">cv_score</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="k">print</span> <span class="s">&quot;My Sklearn RF Scores: &quot;</span><span class="p">,</span> <span class="n">skcv_score</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">skrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>


                       <span class="n">CV</span> <span class="n">Accuracy</span>    <span class="n">Test</span> <span class="n">Accuracy</span>
<span class="n">My</span> <span class="n">RF</span> <span class="n">Scores</span><span class="p">:</span>          <span class="mf">0.958441558442</span> <span class="mf">0.94495412844</span>
<span class="n">My</span> <span class="n">Sklearn</span> <span class="n">RF</span> <span class="n">Scores</span><span class="p">:</span>  <span class="mf">0.958658008658</span> <span class="mf">0.95871559633</span>
</pre></div>


<p>Sklearn's random forest classifier and my random forest implementation give similar results in cross validation and on the test set.  On small datasets, it works in similar time.   </p>
<h2>Afternoon</h2>
<p>The afternoon paired sprint involved using and exploring sklearn's random forest classifier on a cell phone plan dataset attempting to predict churn.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;../data/churn.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>State</th>
      <th>Account Length</th>
      <th>Area Code</th>
      <th>Phone</th>
      <th>Int'l Plan</th>
      <th>VMail Plan</th>
      <th>VMail Message</th>
      <th>Day Mins</th>
      <th>Day Calls</th>
      <th>Day Charge</th>
      <th>...</th>
      <th>Eve Calls</th>
      <th>Eve Charge</th>
      <th>Night Mins</th>
      <th>Night Calls</th>
      <th>Night Charge</th>
      <th>Intl Mins</th>
      <th>Intl Calls</th>
      <th>Intl Charge</th>
      <th>CustServ Calls</th>
      <th>Churn?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KS</td>
      <td>128</td>
      <td>415</td>
      <td>382-4657</td>
      <td>no</td>
      <td>yes</td>
      <td>25</td>
      <td>265.1</td>
      <td>110</td>
      <td>45.07</td>
      <td>...</td>
      <td>99</td>
      <td>16.78</td>
      <td>244.7</td>
      <td>91</td>
      <td>11.01</td>
      <td>10.0</td>
      <td>3</td>
      <td>2.70</td>
      <td>1</td>
      <td>False.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>OH</td>
      <td>107</td>
      <td>415</td>
      <td>371-7191</td>
      <td>no</td>
      <td>yes</td>
      <td>26</td>
      <td>161.6</td>
      <td>123</td>
      <td>27.47</td>
      <td>...</td>
      <td>103</td>
      <td>16.62</td>
      <td>254.4</td>
      <td>103</td>
      <td>11.45</td>
      <td>13.7</td>
      <td>3</td>
      <td>3.70</td>
      <td>1</td>
      <td>False.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NJ</td>
      <td>137</td>
      <td>415</td>
      <td>358-1921</td>
      <td>no</td>
      <td>no</td>
      <td>0</td>
      <td>243.4</td>
      <td>114</td>
      <td>41.38</td>
      <td>...</td>
      <td>110</td>
      <td>10.30</td>
      <td>162.6</td>
      <td>104</td>
      <td>7.32</td>
      <td>12.2</td>
      <td>5</td>
      <td>3.29</td>
      <td>0</td>
      <td>False.</td>
    </tr>
    <tr>
      <th>3</th>
      <td>OH</td>
      <td>84</td>
      <td>408</td>
      <td>375-9999</td>
      <td>yes</td>
      <td>no</td>
      <td>0</td>
      <td>299.4</td>
      <td>71</td>
      <td>50.90</td>
      <td>...</td>
      <td>88</td>
      <td>5.26</td>
      <td>196.9</td>
      <td>89</td>
      <td>8.86</td>
      <td>6.6</td>
      <td>7</td>
      <td>1.78</td>
      <td>2</td>
      <td>False.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>OK</td>
      <td>75</td>
      <td>415</td>
      <td>330-6626</td>
      <td>yes</td>
      <td>no</td>
      <td>0</td>
      <td>166.7</td>
      <td>113</td>
      <td>28.34</td>
      <td>...</td>
      <td>122</td>
      <td>12.61</td>
      <td>186.9</td>
      <td>121</td>
      <td>8.41</td>
      <td>10.1</td>
      <td>3</td>
      <td>2.73</td>
      <td>3</td>
      <td>False.</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 21 columns</p>
</div>

<div class="highlight"><pre>for i in df.columns:
    print i, df[i].nunique()

State 51
Account Length 212
Area Code 3
Phone 3333
Int&#39;l Plan 2
VMail Plan 2
VMail Message 46
Day Mins 1667
Day Calls 119
Day Charge 1667
Eve Mins 1611
Eve Calls 123
Eve Charge 1440
Night Mins 1591
Night Calls 120
Night Charge 933
Intl Mins 162
Intl Calls 21
Intl Charge 162
CustServ Calls 10
Churn? 2
</pre></div>


<p>We are going to clean up the data a little.   Replace boolean type values with 1 or 0, and drop some information that will not work with the classifier. </p>
<div class="highlight"><pre>df[&#39;Int\&#39;l Plan&#39;]=np.where(df[&#39;Int\&#39;l Plan&#39;]==&#39;yes&#39;,1,0)
df[&#39;VMail Plan&#39;]=np.where(df[&#39;VMail Plan&#39;]==&#39;yes&#39;,1,0)
df[&#39;Churn?&#39;]=np.where(df[&#39;Churn?&#39;]==&#39;True.&#39;,1,0)
df = df[[u&#39;Account Length&#39;, u&#39;Int\&#39;l Plan&#39;, u&#39;VMail Plan&#39;, u&#39;VMail Message&#39;, u&#39;Day Mins&#39;, u&#39;Day Calls&#39;, u&#39;Day Charge&#39;, u&#39;Eve Mins&#39;, u&#39;Eve Calls&#39;, u&#39;Eve Charge&#39;, u&#39;Night Mins&#39;, u&#39;Night Calls&#39;, u&#39;Night Charge&#39;, u&#39;Intl Mins&#39;, u&#39;Intl Calls&#39;, u&#39;Intl Charge&#39;, u&#39;CustServ Calls&#39;, u&#39;Churn?&#39;]]
df.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Account Length</th>
      <th>Int'l Plan</th>
      <th>VMail Plan</th>
      <th>VMail Message</th>
      <th>Day Mins</th>
      <th>Day Calls</th>
      <th>Day Charge</th>
      <th>Eve Mins</th>
      <th>Eve Calls</th>
      <th>Eve Charge</th>
      <th>Night Mins</th>
      <th>Night Calls</th>
      <th>Night Charge</th>
      <th>Intl Mins</th>
      <th>Intl Calls</th>
      <th>Intl Charge</th>
      <th>CustServ Calls</th>
      <th>Churn?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>128</td>
      <td>0</td>
      <td>1</td>
      <td>25</td>
      <td>265.1</td>
      <td>110</td>
      <td>45.07</td>
      <td>197.4</td>
      <td>99</td>
      <td>16.78</td>
      <td>244.7</td>
      <td>91</td>
      <td>11.01</td>
      <td>10.0</td>
      <td>3</td>
      <td>2.70</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>107</td>
      <td>0</td>
      <td>1</td>
      <td>26</td>
      <td>161.6</td>
      <td>123</td>
      <td>27.47</td>
      <td>195.5</td>
      <td>103</td>
      <td>16.62</td>
      <td>254.4</td>
      <td>103</td>
      <td>11.45</td>
      <td>13.7</td>
      <td>3</td>
      <td>3.70</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>137</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>243.4</td>
      <td>114</td>
      <td>41.38</td>
      <td>121.2</td>
      <td>110</td>
      <td>10.30</td>
      <td>162.6</td>
      <td>104</td>
      <td>7.32</td>
      <td>12.2</td>
      <td>5</td>
      <td>3.29</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>84</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>299.4</td>
      <td>71</td>
      <td>50.90</td>
      <td>61.9</td>
      <td>88</td>
      <td>5.26</td>
      <td>196.9</td>
      <td>89</td>
      <td>8.86</td>
      <td>6.6</td>
      <td>7</td>
      <td>1.78</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>75</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>166.7</td>
      <td>113</td>
      <td>28.34</td>
      <td>148.3</td>
      <td>122</td>
      <td>12.61</td>
      <td>186.9</td>
      <td>121</td>
      <td>8.41</td>
      <td>10.1</td>
      <td>3</td>
      <td>2.73</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df.info()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 3333 entries, 0 to 3332
Data columns (total 18 columns):
Account Length    3333 non-null int64
Int&#39;l Plan        3333 non-null int64
VMail Plan        3333 non-null int64
VMail Message     3333 non-null int64
Day Mins          3333 non-null float64
Day Calls         3333 non-null int64
Day Charge        3333 non-null float64
Eve Mins          3333 non-null float64
Eve Calls         3333 non-null int64
Eve Charge        3333 non-null float64
Night Mins        3333 non-null float64
Night Calls       3333 non-null int64
Night Charge      3333 non-null float64
Intl Mins         3333 non-null float64
Intl Calls        3333 non-null int64
Intl Charge       3333 non-null float64
CustServ Calls    3333 non-null int64
Churn?            3333 non-null int64
dtypes: float64(8), int64(10)
memory usage: 494.7 KB
</pre></div>


<p>So now we have a clean dataset with only ints and floats. Lets prepare our test and training</p>
<div class="highlight"><pre>y = df[&#39;Churn?&#39;].values
x = df.drop(&#39;Churn?&#39;,axis=1).values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)
model = RandomForestClassifier()
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
print &quot;Base Accuracy: &quot;, model.score(x_test,y_test)
print &quot;Guessing No Churn:&quot;, np.sum(y_test==0).astype(float)/len(y_test)

Base Accuracy:  0.939
Guessing No Churn: 0.862
</pre></div>


<p>The random forest is giving a better than guessing result!</p>
<div class="highlight"><pre>print confusion_matrix(y_test, model.predict(x_test))

[[853   9]
 [ 52  86]]



print &quot;Recall: &quot;, recall_score(y_test,y_predict)
print &quot;Precision: &quot;, precision_score(y_test,y_predict)

Recall:  0.623188405797
Precision:  0.905263157895
</pre></div>


<p>The model is having the worst problem predicting that people who do churn will churn.   Only ~60% of those that churn were predicted to do so.  There is room for imporovement in the model here.</p>
<p>Sklearn's random forest classifiery has an out of bag error estimate as well as a estimator of feature importance.  We are going to use these next.</p>
<div class="highlight"><pre>model_oob = RandomForestClassifier(oob_score=True)
model_oob.fit(x_train, y_train)
oob_score = model_oob.score(x_test, y_test)
print &#39;Accuracy (old/new): &#39;, model.score(x_test,y_test), oob_score

print &quot;OBB Estimate: &quot;, model_oob.oob_score_

y_predict_oob = model_oob.predict(x_test)

print &#39;Precision (old, new)&#39;, precision_score(y_test, y_predict), precision_score(y_test, y_predict_oob)
print &#39;Recall (old, new)&#39;, recall_score(y_test, y_predict), recall_score(y_test, y_predict_oob)

Accuracy (old/new):  0.939 0.943
OBB Estimate:  0.921560222889
Precision (old, new) 0.905263157895 0.917525773196
Recall (old, new) 0.623188405797 0.644927536232
</pre></div>


<p>The difference between the old and new model is not statistically significant.  They are just the natural variation in this model.   The OBB estimate is very close to the accuracy on the test set.  That is promissing.  Lets see if we can find the most important features:</p>
<div class="highlight"><pre>features = model.feature_importances_
print df.columns[model.feature_importances_ &gt;= sorted(features)[-5]]
features = model_oob.feature_importances_
print df.columns[model_oob.feature_importances_ &gt;= sorted(features)[-5]]

Index([u&#39;Int&#39;l Plan&#39;, u&#39;Day Mins&#39;, u&#39;Day Charge&#39;, u&#39;Eve Charge&#39;, u&#39;CustServ Calls&#39;], dtype=&#39;object&#39;)
Index([u&#39;Int&#39;l Plan&#39;, u&#39;Day Mins&#39;, u&#39;Day Charge&#39;, u&#39;Eve Charge&#39;, u&#39;CustServ Calls&#39;], dtype=&#39;object&#39;)
</pre></div>


<p>We can see that the two models share 4 out of 5 of the same most important features.   They differ on Eve. Charge and Int'l Plan.  </p>
<p>Before we start selecting featuers, we want to make sure that we are seeing the best model. </p>
<div class="highlight"><pre>def tree_accuracy(num_trees):    
    model = RandomForestClassifier(oob_score=True, n_estimators=num_trees)
    model.fit(x_train, y_train)
    accuracy = model.score(x_test, y_test)
    return accuracy

treevalues1 = range(1, 1000, 100)
values1 = []
for v in treevalues1:
    values1.append(tree_accuracy(v))

treevalues2 = range(1, 200, 10)
values2 = []
for v in treevalues2:
    values2.append(tree_accuracy(v))
plt.figure(figsize=(14,8))
plt.subplot(1,2,1)
plt.plot(treevalues1, values1,lw=2,color=&#39;seagreen&#39;)
plt.subplot(1,2,2)
plt.plot(treevalues2, values2,lw=2,color=&#39;steelblue&#39;)
plt.xlabel(&#39;Number of Trees&#39;)
plt.ylabel(&#39;accuracy&#39;)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_23_0.png" /></p>
<p>We can see after about 50 estimators the accuracy does not significantly improve.   We also see if there is a limit on the number of features to consider at each node.</p>
<div class="highlight"><pre>def tree_accuracy(num_features, num_trees=50):    
    model = RandomForestClassifier(oob_score=True, n_estimators=num_trees, max_features=num_features)
    model.fit(x_train, y_train)
    accuracy = model.score(x_test, y_test)
    return accuracy

treevalues1 = range(1, 17, 1)
values1 = []
for v in treevalues1:
    values1.append(tree_accuracy(v))

plt.figure(figsize=(14,8))
plt.plot(treevalues1, values1,lw=2,color=&#39;seagreen&#39;)

plt.xlabel(&#39;Number of Features&#39;)
plt.ylabel(&#39;accuracy&#39;)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_25_0.png" /></p>
<p>We can consider 4 features for each node and about 5 estimators while improving the results.</p>
<p>I am wondering how this compares to other models we have covered.</p>
<div class="highlight"><pre>plt.figure(figsize=(14,8))
for model in [LogisticRegression(),DecisionTreeClassifier(),KNeighborsClassifier(),RandomForestClassifier(n_estimators=50,max_features=5)]:
    model.fit(x_train,y_train)
    print &quot;MODEL: &quot;, model.__class__.__name__
    y_pred = model.predict(x_test)
    y_prob = model.predict_proba(x_test)[:,1]
    print &quot;Accuracy&quot;, accuracy_score(y_pred,y_test)
    print &quot;Recall&quot;, recall_score(y_pred,y_test)
    print &quot;Precision&quot;, precision_score(y_pred,y_test)
    print &quot;&quot;
    fpr,trp,thres = roc_curve(y_test,y_prob)
    plt.plot(fpr,trp,label=model.__class__.__name__)
plt.xlabel(&quot;False Positive Rate&quot;)
plt.ylabel(&quot;True Positive Rate&quot;)
plt.legend(loc=4)
plt.show()

MODEL:  LogisticRegression
Accuracy 0.872
Recall 0.647058823529
Precision 0.159420289855

MODEL:  DecisionTreeClassifier
Accuracy 0.907
Recall 0.659574468085
Precision 0.673913043478

MODEL:  KNeighborsClassifier
Accuracy 0.886
Recall 0.714285714286
Precision 0.289855072464

MODEL:  RandomForestClassifier
Accuracy 0.96
Recall 0.929824561404
Precision 0.768115942029
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_27_1.png" /></p>
<p>The random forest out performs the other models.   We have a relativley high true positve for a small false positive rate.   If we wanted to avoid false positive predictions of churn, the random forest can be turned to have a tre positive rate between 70 and 80%.   </p>
<p>Now lets try to find the most important features in the data set.  We are going through each tree in our model and getting the feature importance of that model.  We then will average over all the importance estimates.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">izip</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
    <span class="n">findex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
    <span class="n">ordered_features</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">findex</span><span class="p">]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">findex</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">name</span> <span class="ow">in</span> <span class="n">izip</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">ordered_features</span><span class="p">):</span>
        <span class="n">d</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">feature_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">feature_stds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">feature_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">feature_stds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>

<span class="n">feature_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_means</span><span class="p">)</span>
<span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">feature_means</span><span class="p">)</span>
<span class="n">feature_means</span> <span class="o">=</span> <span class="n">feature_means</span><span class="p">[</span><span class="n">indexes</span><span class="p">]</span>
<span class="n">feature_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_stds</span><span class="p">)[</span><span class="n">indexes</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">999</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[</span><span class="n">indexes</span><span class="p">]</span>

<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>  <span class="c"># the x locations for the groups</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.55</span>       <span class="c"># the width of the bars</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)),</span> <span class="n">feature_means</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">feature_stds</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Mean Importance&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&#39;Feature Importance&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span> <span class="n">features</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D2/output_29_0.png" /></p>
<p>So we see that the most important features, on average, are day charge, day mins, custserv calls, int'l plan, eve charge, and int'l calls. </p>
<p>Lets retrain the dataset on this subseted data.</p>
<div class="highlight"><pre>df2 = df[[u&#39;Int\&#39;l Plan&#39;, u&#39;Day Mins&#39;, u&#39;Day Charge&#39;, u&#39;Eve Mins&#39;, u&#39;Eve Charge&#39;, u&#39;CustServ Calls&#39;]]
x = df2.values
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Int'l Plan</th>
      <th>Day Mins</th>
      <th>Day Charge</th>
      <th>Eve Mins</th>
      <th>Eve Charge</th>
      <th>CustServ Calls</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>265.1</td>
      <td>45.07</td>
      <td>197.4</td>
      <td>16.78</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>161.6</td>
      <td>27.47</td>
      <td>195.5</td>
      <td>16.62</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>243.4</td>
      <td>41.38</td>
      <td>121.2</td>
      <td>10.30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>299.4</td>
      <td>50.90</td>
      <td>61.9</td>
      <td>5.26</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>166.7</td>
      <td>28.34</td>
      <td>148.3</td>
      <td>12.61</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)
model = RandomForestClassifier(n_estimators=100,max_features=5)
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
print &quot;Base Accuracy: &quot;, model.score(x_test,y_test)
print &quot;Guessing No Churn:&quot;, np.sum(y_test==0).astype(float)/len(y_test)
print &quot;Recall: &quot;, recall_score(y_test,y_predict)
print &quot;Precision: &quot;, precision_score(y_test,y_predict)
print confusion_matrix(y_test, model.predict(x_test))

Base Accuracy:  0.944
Guessing No Churn: 0.857
Recall:  0.671328671329
Precision:  0.914285714286
[[848   9]
 [ 47  96]]
</pre></div>


<p>The similar model gives similar results as the more complicated model, lending credence to the idea these are the most influencial factors.   If a cell phone company wants to reduce churn then they need to deal with the number of mins and charges for customer plans.   This is where they will reduce churn.</p>
    </div>
  </div>
  <hr class="separator">
  <div class="col-md-8 col-md-offset-2">
  <div id="disqus_thread">
    <script>
      var disqus_shortname = 'bryansmithphd';
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] ||
         document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript=bryansmithphd">
        comments powered by Disqus.
      </a>
    </noscript>
    <a href="https://disqus.com" class="dsq-brlink">
      blog comments powered by <span class="logo-disqus">Disqus</span>
    </a>
  </div>
  </div>
  </div>
<footer class="footer">
  <div class="container">
    <p class="text-center">
      Bryan Smith, <a href="" target="_blank"></a> unless otherwise noted.
    </p>
    <div class="text-center">
      Generated by <a href="http://getpelican.com" target="_blank">Pelican</a> with the <a href="http://github.com/nairobilug/pelican-alchemy">alchemy</a> theme.
    </div>
  </div>
</footer> <!-- /.footer -->
  <script src="http://www.bryantravissmith.com/theme/js/jquery.min.js"></script>
  <script src="http://www.bryantravissmith.com/theme/js/bootstrap.min.js"></script>
</body> <!-- 42 -->
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$$','$$'], ['\\(','\\)']]}
});
</script>
</html>