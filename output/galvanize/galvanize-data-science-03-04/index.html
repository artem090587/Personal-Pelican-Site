<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-04/index.html" />

    <title>  Bryan Travis Smith, Ph.D &mdash; Galvanize - Week 03 - Day 4
</title>




    <link rel="stylesheet" href="http://www.bryantravissmith.com/theme/css/style.css">

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-24340005-3', 'auto');
    ga('send', 'pageview');

  </script>

    <meta name="author" content="Bryan Smith">
    <meta name="description" content="Today we covered logistic regression and ROC curves.">
  <meta name="tags" contents="data-science, galvanize, Lasso Regresion, Ridge Regression, Regularization, ">
</head>

<body>
<header class="header">
  <div class="container">
      <div class="header-image pull-left">
        <a class="nodec" href="http://www.bryantravissmith.com"><img src=http://www.bryantravissmith.com/img/bryan.jpeg></a>
      </div>
    <div class="header-inner">
      <h1 class="header-name">
        <a class="nodec" href="http://www.bryantravissmith.com">Bryan Travis Smith, Ph.D</a>
      </h1>
      <h3 class="header-text">Physicist, Data Scientist, Martial Artist, & Life Enthusiast</h3>
      <ul class="header-menu list-inline">
              <li class="muted">|</li>
            <li><a class="nodec" href="http://www.bryantravissmith.com/about/">About</a></li>
              <li class="muted">|</li>
          <li><a class="nodec icon-mail-alt" href="mailto:bryantravissmith@gmail.com"></a></li>
          <li><a class="nodec icon-github" href="https://github.com/bryantravissmith"></a></li>
      </ul>
    </div>
  </div>
</header> <!-- /.header -->  <div class="container">
  <div class="post full-post">
    <h1 class="post-title">
      <a href="/galvanize/galvanize-data-science-03-04/" title="Permalink to Galvanize - Week 03 - Day 4">Galvanize - Week 03 - Day 4</a>
    </h1>
    <ul class="list-inline">
      <li class="post-date">
        <a class="text-muted" href="/galvanize/galvanize-data-science-03-04/" title="2015-06-18T10:20:00-07:00">Thu 18 June 2015</a>
      </li>
      <li class="muted">&middot;</li>
      <li class="post-category">
        <a href="http://www.bryantravissmith.com/category/galvanize.html">Galvanize</a>
      </li>
        <li class="muted">&middot;</li>
        <li>
          <address class="post-author">
            By <a href="http://www.bryantravissmith.com/author/bryan-smith.html">Bryan Smith</a>
          </address>
        </li>
    </ul>
    <div class="post-content">
      <h1>Galvanize Immersive Data Science</h1>
<h2>Week 3 - Day 4</h2>
<p>Today we had a 2 hour assessment on everything we covered.  There were programming style problem and 1 math style problems.  The topics were on everything we have covered up to now.  </p>
<h2>Afternoon</h2>
<p>The lectures today were on Logistic Regresion, Odds, and ROC Curves.   </p>
<h2>ROC Curve</h2>
<p>We were told that one of the best ways to evaluate how a classifier performs is an <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a>.   They display the change in the false and true positive rates as paramters in the model change.  In the case of logistic regression, its the threshold use to classify a data point based on the predicted probability. </p>
<p>Recall that the <em>true positive rate</em> is</p>
<div class="highlight"><pre> number of true positives     number correctly predicted positive
-------------------------- = -------------------------------------
 number of positive cases           number of positive cases
</pre></div>


<p>and the <em>false positive rate</em> is</p>
<div class="highlight"><pre> number of false positives     number incorrectly predicted positive
--------------------------- = ---------------------------------------
  number of negative cases           number of negative cases
</pre></div>


<div class="highlight"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="k">def</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">TPR</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">FPR</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">&gt;=</span><span class="n">thresh</span><span class="p">)</span><span class="o">.</span><span class="kp">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">TPR</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">sum</span><span class="p">((</span><span class="n">pred</span><span class="o">==</span><span class="n">y</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="kp">sum</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">FPR</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">sum</span><span class="p">((</span><span class="n">pred</span><span class="o">!=</span><span class="n">y</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="kp">sum</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">TPR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">TPR</span><span class="p">)</span>
    <span class="n">FPR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">FPR</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">TPR</span><span class="p">,</span><span class="n">FPR</span><span class="p">,</span><span class="n">thresholds</span>


<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">tpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;False Positive Rate (1 - Specificity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;True Positive Rate (Sensitivity, Recall)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;ROC plot of fake data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_1_0.png" /></p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span> <span class="k">as</span> <span class="n">roc</span>
<span class="n">fpr1</span><span class="p">,</span><span class="n">tpr1</span><span class="p">,</span><span class="n">threshs</span> <span class="o">=</span> <span class="n">roc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">probabilities</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">&#39;--&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;Bryans ROC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr1</span><span class="p">,</span> <span class="n">tpr1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;indianred&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;Sklearn ROC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;False Positive Rate (1 - Specificity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;True Positive Rate (Sensitivity, Recall)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;ROC plot of fake data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_2_0.png" /></p>
<p>The results between the two curves is negligable on a fake dataset.  We are now going to do this with FICO data.</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;data/loanf.csv&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Interest.Rate&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">&#39;FICO.Score&#39;</span><span class="p">,</span> <span class="s">&#39;Loan.Length&#39;</span><span class="p">,</span> <span class="s">&#39;Loan.Amount&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>


<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibratedClassifierCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span><span class="n">accuracy_score</span><span class="p">,</span><span class="n">recall_score</span><span class="p">,</span><span class="n">precision_score</span>

<span class="n">a_train</span><span class="p">,</span> <span class="n">a_test</span><span class="p">,</span> <span class="n">b_train</span><span class="p">,</span> <span class="n">b_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">cal</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;isotonic&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">cal</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">a_train</span><span class="p">,</span> <span class="n">b_train</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">cal</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">a_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">b_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;False Positive Rate (1 - Specificity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;True Positive Rate (Sensitivity, Recall)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;ROC plot of Loan Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">print</span> <span class="s">&quot;AUC: &quot;</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">cal</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Accuracy: &quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">cal</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Recall: &quot;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">cal</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Precision: &quot;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">cal</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_5_0.png" /></p>
<div class="highlight"><pre> AUC:  0.763176276353
Accuracy:  0.769333333333
Recall:  0.744094488189
Precision:  0.636363636364
</pre></div>


<p>The the model is not a great model, but it does start off identifying 3 out of 4 true positives and has 6 our of 10 of those predicted to be positive actually being positive.  </p>
<h2>Graduate School Admissions</h2>
<p>The data we will be using net is admission data on Grad school acceptances.</p>
<ul>
<li><code>admit</code>: whether or not the applicant was admitted to grad. school</li>
<li><code>gpa</code>: undergraduate GPA</li>
<li><code>GRE</code>: score of GRE test</li>
<li><code>rank</code>: prestige of undergraduate school (1 is highest prestige, ala Harvard)</li>
</ul>
<p>We will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.</p>
<p>Before we get to predictions, we should do some data exploration.</p>
<ol>
<li>
<p>Load in the dataset into pandas: <code>data/grad.csv</code>.</p>
<p>grad = pd.read_csv('data/grad.csv')
grad.head()</p>
</li>
</ol>
<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>admit</th>
      <th>gre</th>
      <th>gpa</th>
      <th>rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>380</td>
      <td>3.61</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>660</td>
      <td>3.67</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>800</td>
      <td>4.00</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>640</td>
      <td>3.19</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>520</td>
      <td>2.93</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>grad.info()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 400 entries, 0 to 399
Data columns (total 4 columns):
admit    400 non-null int64
gre      400 non-null int64
gpa      400 non-null float64
rank     400 non-null int64
dtypes: float64(1), int64(3)
memory usage: 15.6 KB



grad.describe()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>admit</th>
      <th>gre</th>
      <th>gpa</th>
      <th>rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>400.000000</td>
      <td>400.000000</td>
      <td>400.000000</td>
      <td>400.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.317500</td>
      <td>587.700000</td>
      <td>3.389900</td>
      <td>2.48500</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.466087</td>
      <td>115.516536</td>
      <td>0.380567</td>
      <td>0.94446</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>220.000000</td>
      <td>2.260000</td>
      <td>1.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>520.000000</td>
      <td>3.130000</td>
      <td>2.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>580.000000</td>
      <td>3.395000</td>
      <td>2.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>660.000000</td>
      <td>3.670000</td>
      <td>3.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>800.000000</td>
      <td>4.000000</td>
      <td>4.00000</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>temp = pd.crosstab(grad[&#39;admit&#39;],grad[&#39;rank&#39;])
temp
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>rank</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
    <tr>
      <th>admit</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>97</td>
      <td>93</td>
      <td>55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>33</td>
      <td>54</td>
      <td>28</td>
      <td>12</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>temp.transpose().plot(kind=&#39;bar&#39;)




&lt;matplotlib.axes._subplots.AxesSubplot at 0x10a7a8a90&gt;
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_11_1.png" /></p>
<p>We see that if a person is applying to grad school from a rank 1 school, they are more likely than not to be accepeted.   We also see that this ratio drops as the rank of the current school is lower.   We also see that most of the dat is from ranke 2 and rank 3 scores.</p>
<div class="highlight"><pre>(temp/temp.sum()).transpose().plot(kind=&#39;bar&#39;)




&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f591a50&gt;
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_13_1.png" /></p>
<p>Looking at the rations instead of hte counts highlight that change.   There is an increase in chances of getting accepted if the person is coming from a better school.   </p>
<p>Lets look at the GRE and GPA distributions.</p>
<div class="highlight"><pre>grad.gre.hist()




&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f854990&gt;
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_15_1.png" /></p>
<div class="highlight"><pre>grad.gpa.hist()




&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f8f2a10&gt;
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_16_1.png" /></p>
<p>Both of these are skewed left, but we do have a cut off on the GPA not being above 4.  We see a spike there, and this is commonly seen in any data with arbitary cutoffs.   If there was no max GPA, we would expenct that some people would have 5's,6's, and so on.  They might be rare, but they are there.  The cap compresses all these overacheivers to 4.0.  </p>
<h2>Fitting Grad School Admissions</h2>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.discrete.discrete_model</span> <span class="kn">import</span> <span class="n">Logit</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[[</span><span class="s">&#39;admit&#39;</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">grad</span><span class="p">[[</span><span class="s">&#39;gre&#39;</span><span class="p">,</span><span class="s">&#39;gpa&#39;</span><span class="p">,</span><span class="s">&#39;rank&#39;</span><span class="p">]])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Logit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">Optimization</span> <span class="n">terminated</span> <span class="n">successfully</span><span class="o">.</span>
         <span class="n">Current</span> <span class="n">function</span> <span class="n">value</span><span class="p">:</span> <span class="mf">0.574302</span>
         <span class="n">Iterations</span> <span class="mi">6</span>
</pre></div>


<table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>admit</td>      <th>  No. Observations:  </th>  <td>   400</td>  
</tr>
<tr>
  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   396</td>  
</tr>
<tr>
  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  
</tr>
<tr>
  <th>Date:</th>          <td>Fri, 19 Jun 2015</td> <th>  Pseudo R-squ.:     </th>  <td>0.08107</td> 
</tr>
<tr>
  <th>Time:</th>              <td>07:23:59</td>     <th>  Log-Likelihood:    </th> <td> -229.72</td> 
</tr>
<tr>
  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -249.99</td> 
</tr>
<tr>
  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>8.207e-09</td>
</tr>
</table>

<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> 
</tr>
<tr>
  <th>const</th> <td>   -3.4495</td> <td>    1.133</td> <td>   -3.045</td> <td> 0.002</td> <td>   -5.670    -1.229</td>
</tr>
<tr>
  <th>gre</th>   <td>    0.0023</td> <td>    0.001</td> <td>    2.101</td> <td> 0.036</td> <td>    0.000     0.004</td>
</tr>
<tr>
  <th>gpa</th>   <td>    0.7770</td> <td>    0.327</td> <td>    2.373</td> <td> 0.018</td> <td>    0.135     1.419</td>
</tr>
<tr>
  <th>rank</th>  <td>   -0.5600</td> <td>    0.127</td> <td>   -4.405</td> <td> 0.000</td> <td>   -0.809    -0.311</td>
</tr>
</table>

<p>We see that the model is not very good.  The Pseudo R-square, which as to do with the deviance or negative log likelihood, is very low.  The coefficients are interested in that they do predict more likely admision for higher gre, higher gpa, and better schools.   </p>
<p>I think I want to plot the over of the predictions to show how poor the model is.</p>
<div class="highlight"><pre>xp = X.values[(y.values==1)[:,0],:]
yp = model.fittedvalues.values[(y.values==1)[:,0]]
xn = X.values[(y.values==0)[:,0],:]
yn = model.fittedvalues.values[(y.values==0)[:,0]]
zp = xp.dot(model.params.values)
zn = xn.dot(model.params.values)
plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(zp,np.exp(yp)/(1+np.exp(yp)),&#39;go&#39;,alpha=0.5)
plt.xlabel(&quot;Logodds for Datapoint&quot;)
plt.ylabel(&quot;Prob for Student Admited&quot;)
plt.subplot(1,2,2)
plt.plot(zn,np.exp(yn)/(1+np.exp(yn)),&#39;ro&#39;,alpha=0.5)
plt.xlabel(&quot;Logodds for Datapoint&quot;)
plt.ylabel(&quot;Prob for Student Not Admited&quot;)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_20_0.png" /></p>
<p>This model does not do a great job of predicting admissions, but we can attemp to use Sklearn's machinery to get a better fit, and also measure the model metrics simply.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">roc_auc_score</span>


<span class="n">a_train</span><span class="p">,</span> <span class="n">a_test</span><span class="p">,</span> <span class="n">b_train</span><span class="p">,</span> <span class="n">b_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">grad</span><span class="p">[[</span><span class="s">&#39;gre&#39;</span><span class="p">,</span><span class="s">&#39;gpa&#39;</span><span class="p">,</span><span class="s">&#39;rank&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">admit</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">cal1</span><span class="o">=</span><span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">lin</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;isotonic&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">cal1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">a_train</span><span class="p">,</span><span class="n">b_train</span><span class="p">)</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">lin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">a_train</span><span class="p">,</span><span class="n">b_train</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">cal1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">a_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">b_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;False Positive Rate (1 - Specificity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;True Positive Rate (Sensitivity, Recall)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;ROC plot of College Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span> <span class="s">&quot;AUC: &quot;</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">cal1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Accuracy: &quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">cal1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Recall: &quot;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">cal1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Precision: &quot;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">cal1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_test</span><span class="p">),</span><span class="n">b_test</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_22_0.png" /></p>
<div class="highlight"><pre><span class="n">AUC</span><span class="o">:</span>  <span class="mf">0.613445378151</span>
<span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">0.666666666667</span>
<span class="n">Recall</span><span class="o">:</span>  <span class="mf">0.485714285714</span>
<span class="n">Precision</span><span class="o">:</span>  <span class="mf">0.435897435897</span>
</pre></div>


<p>The area under the ROC curve is not far from 0.5 (random guessing).  The model only predicts 50% of the students admitted to college of being admitted, and only 43% of those predicted to be admitted were actually admitted.   I do not think the College Board will be breaking down our doors for this model.</p>
<p>In one way we treated the Rank as a continuous variable, and could try treating it like a categorical variable instead.</p>
<div class="highlight"><pre>grad1 = pd.get_dummies(grad, columns=[&#39;rank&#39;])
a_train, a_test, b_train, b_test = train_test_split(grad1.drop([&#39;admit&#39;,&#39;rank_4&#39;],axis=1).values, grad1.admit.values, test_size=0.30, random_state=42)

lin = LogisticRegression()
cal2=CalibratedClassifierCV(lin, method=&#39;isotonic&#39;, cv=20)
cal2.fit(a_train,b_train)

probs = cal2.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&#39;seagreen&#39;,alpha=0.8,lw=3,label=&quot;Original&quot;)
plt.plot(fpr1, tpr1,color=&#39;indianred&#39;,alpha=0.8,lw=3,label=&quot;With Rank Categories&quot;)
plt.xlabel(&quot;False Positive Rate (1 - Specificity)&quot;)
plt.ylabel(&quot;True Positive Rate (Sensitivity, Recall)&quot;)
plt.title(&quot;ROC plot of College Data&quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &quot;AUC: &quot;, roc_auc_score(cal2.predict(a_test),b_test)
print &quot;Accuracy: &quot;, accuracy_score(cal2.predict(a_test),b_test)
print &quot;Recall: &quot;, recall_score(cal2.predict(a_test),b_test)
print &quot;Precision: &quot;, precision_score(cal2.predict(a_test),b_test)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_24_0.png" /></p>
<div class="highlight"><pre><span class="n">AUC</span><span class="o">:</span>  <span class="mf">0.640625</span>
<span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">0.691666666667</span>
<span class="n">Recall</span><span class="o">:</span>  <span class="mf">0.53125</span>
<span class="n">Precision</span><span class="o">:</span>  <span class="mf">0.435897435897</span>
</pre></div>


<p>We could have an initial pass where we want a TPR &gt; 60% and FPR &lt; 40.  We can find the thresholds for these values.</p>
<div class="highlight"><pre>thresholds[(tpr &gt; 0.6)&amp;(fpr &lt; 0.4)]




array([ 0.24242424,  0.25252525,  0.26262626])
</pre></div>


<h2>Beta coefficients as Odds Ratio</h2>
<p>One thing that is often lost when talking about logistic regression is the idea of the odds ratio, or rather the probabilistic interpretation of the model. For this next part we will get hands on with the odds ratio.</p>
<p>The <strong><em>odds ratio</em></strong> is defined as the product of the exponential of each coefficient.</p>
<p><img alt="" src="images/odds_ratio.png" /></p>
<p>This is the odds of being admitted over not being admitted.</p>
<p>It tells you how much a one unit increase of a feature corresponds to the odds of being admitted to grad school. And in doing so the coefficients of the logistic regression can be interpreted similarly to the coefficients of linear regression.</p>
<p>From our model we can look at the beta coefficients (Intercept,GRE, GPA, Rank)</p>
<div class="highlight"><pre>beta = cal1.calibrated_classifiers_[-1].__dict__[&#39;base_estimator&#39;].coef_[0,:]
print beta
beta = np.hstack((cal1.calibrated_classifiers_[-1].__dict__[&#39;base_estimator&#39;].intercept_[0],beta))
beta

[ 0.00084169  0.36208945 -0.54024057]





array([ -1.19927209e+00,   8.41691796e-04,   3.62089453e-01,
        -5.40240569e-01])
</pre></div>


<p>The odd ratios for each beta is given by:</p>
<p>$$\mbox{odds ratio} = e^{\beta_i}$$</p>
<div class="highlight"><pre>np.exp(beta)




array([ 0.30141353,  1.00084205,  1.43632742,  0.58260808])
</pre></div>


<p>This means that the base line odd ration of getting into grad school is 0.3.</p>
<p><strong>Increasing your gre by 1 point increases your odds by 1.0008.</strong></p>
<p><strong>Increasing your gpa by 1 point increases your ods by 1.435.</strong></p>
<p><strong>Decreasing your rank of college 1 unit decreases your odds by 0.582.</strong></p>
<p>We can also ask how much change would result in doubling the odds ratio:</p>
<p>$$x_i = \frac{ln(2)}{\beta_i}$$</p>
<div class="highlight"><pre>np.log(2)/beta




array([ -5.77973244e-01,   8.23516617e+02,   1.91429818e+00,
        -1.28303430e+00])
</pre></div>


<p>Increasing my GRE by 824 doubles my odds ration.  Increasing my GPA by 1.91 doubles the odds ratio, and increasing the ranking by 1.28 doubles the odds ratio.  </p>
<h2>Predicted Probabilities</h2>
<p>Now let's actually play with our data to verify what we calculated above with the Odds Ratio.  We can look, on average, how the rank changes the odds.</p>
<div class="highlight"><pre>g = grad.groupby(&#39;rank&#39;).mean().reset_index().drop(&#39;admit&#39;,axis=1)
g
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>gre</th>
      <th>gpa</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>611.803279</td>
      <td>3.453115</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>596.026490</td>
      <td>3.361656</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>574.876033</td>
      <td>3.432893</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>570.149254</td>
      <td>3.318358</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>p_g = cal1.predict_proba(g[[&#39;gre&#39;,&#39;gpa&#39;,&#39;rank&#39;]].values)[:,1]
p_g




array([ 0.68469485,  0.3519206 ,  0.17594864,  0.05175475])
</pre></div>


<p>We can use this to calculate the odds rations for each rank</p>
<div class="highlight"><pre>odds_g = p_g/(1-p_g)
final = np.vstack((np.arange(1,5),np.vstack((p_g,odds_g))))
final




array([[ 1.        ,  2.        ,  3.        ,  4.        ],
       [ 0.68469485,  0.3519206 ,  0.17594864,  0.05175475],
       [ 2.1715308 ,  0.54302082,  0.21351659,  0.0545795 ]])




predicted_odds = np.hstack((np.array(2.1715408),(odds_g*0.583)[:3]))
predicted_odds




array([ 2.1715408 ,  1.26600246,  0.31658114,  0.12448017])
</pre></div>


<p>The odds to drop, but they do not match the vlaues we have from the average predictions.  We can make a graph of the log odds and find the slope:</p>
<div class="highlight"><pre>pd.DataFrame({&quot;logodds&quot;:np.log(odds_g),&quot;rank&quot;:range(1,5)}).set_index(&#39;rank&#39;).plot(kind=&#39;bar&#39;)
print -1.7/3,beta[3]

-0.566666666667 -0.540240569018
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_41_1.png" /></p>
<p>In this case the slop of the logodds and the fitted coefficient match up very close.  Lets do this for the GRE and GPA</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#39;gre&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;admit&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p_g</span> <span class="o">=</span> <span class="n">cal1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">g</span><span class="p">[[</span><span class="s">&#39;gre&#39;</span><span class="p">,</span><span class="s">&#39;gpa&#39;</span><span class="p">,</span><span class="s">&#39;rank&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">odds_g</span> <span class="o">=</span> <span class="n">p_g</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p_g</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">odds_g</span><span class="p">,</span><span class="s">&#39;go&#39;</span><span class="p">)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">26</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">odds_g</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">26</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">print</span> <span class="n">linear</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="mf">0.00138822870627</span> <span class="mf">0.000841691795892</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_43_1.png" /></p>
<p>They are the same scale, but differ by a factor 1.5.   This makes sense because we have a few clear outliers in the data.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#39;gpa&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;admit&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p_g</span> <span class="o">=</span> <span class="n">cal1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">g</span><span class="p">[[</span><span class="s">&#39;gre&#39;</span><span class="p">,</span><span class="s">&#39;gpa&#39;</span><span class="p">,</span><span class="s">&#39;rank&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">odds_g</span> <span class="o">=</span> <span class="n">p_g</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p_g</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">odds_g</span><span class="p">,</span><span class="s">&#39;go&#39;</span><span class="p">)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c">#linear.fit(g.gpa.values,odds_g)</span>
<span class="n">linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">odds_g</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">print</span> <span class="n">linear</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="n">beta</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="mf">0.506303444619</span> <span class="mf">0.362089452847</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_45_1.png" /></p>
<p>This is also close, and the same order of magnitude.  But we have some clear outliers that need to be address in with this model.   The fact tha the coefficients of the model have a interpretation in terms of odds is great.  It adds a hook to reality in this abstraction. </p>
<h2>MOOCS</h2>
<p>This is the future!  No one goes to physical schools any more and MOOCs rule the world.</p>
<p>Harvard and MIT have <a href="http://newsoffice.mit.edu/2014/mit-and-harvard-release-de-identified-learning-data-open-online-courses">released</a> a great dataset around engagement statistics for their MOOC courses. One of the biggest issues with MOOCs is engagement. We will try to predict the probability of 'engagement' of a student given all the other columns.  We will define engagement here as either: <code>explored == 1 OR certified == 1</code>.</p>
<div class="highlight"><pre>mooc = pd.read_csv(&#39;data/mooc.csv&#39;)
mooc[&#39;engagement&#39;] = mooc[&#39;explored&#39;]+mooc[&#39;certified&#39;]
mooc[&#39;engagement&#39;] = np.where(mooc.engagement &gt; 0,1,0)
mooc.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>course_id</th>
      <th>userid_DI</th>
      <th>registered</th>
      <th>viewed</th>
      <th>explored</th>
      <th>certified</th>
      <th>final_cc_cname_DI</th>
      <th>LoE_DI</th>
      <th>YoB</th>
      <th>gender</th>
      <th>...</th>
      <th>start_time_DI</th>
      <th>last_event_DI</th>
      <th>nevents</th>
      <th>ndays_act</th>
      <th>nplay_video</th>
      <th>nchapters</th>
      <th>nforum_posts</th>
      <th>roles</th>
      <th>incomplete_flag</th>
      <th>engagement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>HarvardX/CB22x/2013_Spring</td>
      <td>MHxPC130442623</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>United States</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>2012-12-19</td>
      <td>2013-11-17</td>
      <td>NaN</td>
      <td>9</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>HarvardX/CS50x/2012</td>
      <td>MHxPC130442623</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>United States</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>2012-10-15</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>9</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>HarvardX/CB22x/2013_Spring</td>
      <td>MHxPC130275857</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>United States</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>2013-02-08</td>
      <td>2013-11-17</td>
      <td>NaN</td>
      <td>16</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>HarvardX/CS50x/2012</td>
      <td>MHxPC130275857</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>United States</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>2012-09-17</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>16</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>HarvardX/ER22x/2013_Spring</td>
      <td>MHxPC130275857</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>United States</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>2012-12-19</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>16</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows  21 columns</p>
</div>

<div class="highlight"><pre>mg = mooc.groupby(&#39;course_id&#39;)[&#39;course_id&#39;,&#39;viewed&#39;,&#39;explored&#39;,&#39;certified&#39;,&#39;engagement&#39;].mean()
mg = mg.sort([&#39;engagement&#39;])
mg
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>viewed</th>
      <th>explored</th>
      <th>certified</th>
      <th>engagement</th>
    </tr>
    <tr>
      <th>course_id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>HarvardX/CB22x/2013_Spring</th>
      <td>0.543764</td>
      <td>0.018232</td>
      <td>0.012799</td>
      <td>0.018299</td>
    </tr>
    <tr>
      <th>MITx/3.091x/2013_Spring</th>
      <td>0.960906</td>
      <td>0.023457</td>
      <td>0.022479</td>
      <td>0.028506</td>
    </tr>
    <tr>
      <th>HarvardX/PH278x/2013_Spring</th>
      <td>0.379173</td>
      <td>0.030049</td>
      <td>0.017954</td>
      <td>0.031867</td>
    </tr>
    <tr>
      <th>MITx/6.002x/2013_Spring</th>
      <td>0.480549</td>
      <td>0.040612</td>
      <td>0.026670</td>
      <td>0.041511</td>
    </tr>
    <tr>
      <th>MITx/8.MReV/2013_Summer</th>
      <td>0.708663</td>
      <td>0.039675</td>
      <td>0.031339</td>
      <td>0.042841</td>
    </tr>
    <tr>
      <th>MITx/6.00x/2013_Spring</th>
      <td>0.944850</td>
      <td>0.046678</td>
      <td>0.021710</td>
      <td>0.046695</td>
    </tr>
    <tr>
      <th>MITx/8.02x/2013_Spring</th>
      <td>0.669447</td>
      <td>0.058039</td>
      <td>0.026475</td>
      <td>0.058071</td>
    </tr>
    <tr>
      <th>MITx/3.091x/2012_Fall</th>
      <td>0.493352</td>
      <td>0.063032</td>
      <td>0.044460</td>
      <td>0.063032</td>
    </tr>
    <tr>
      <th>MITx/6.00x/2012_Fall</th>
      <td>0.620716</td>
      <td>0.062685</td>
      <td>0.037119</td>
      <td>0.063074</td>
    </tr>
    <tr>
      <th>HarvardX/CS50x/2012</th>
      <td>0.625430</td>
      <td>0.064986</td>
      <td>0.007588</td>
      <td>0.065016</td>
    </tr>
    <tr>
      <th>HarvardX/ER22x/2013_Spring</th>
      <td>0.560238</td>
      <td>0.061527</td>
      <td>0.040867</td>
      <td>0.069435</td>
    </tr>
    <tr>
      <th>MITx/6.002x/2012_Fall</th>
      <td>0.637549</td>
      <td>0.073951</td>
      <td>0.042881</td>
      <td>0.074343</td>
    </tr>
    <tr>
      <th>MITx/7.00x/2013_Spring</th>
      <td>0.622686</td>
      <td>0.073826</td>
      <td>0.039174</td>
      <td>0.074825</td>
    </tr>
    <tr>
      <th>MITx/2.01x/2013_Spring</th>
      <td>0.682436</td>
      <td>0.098853</td>
      <td>0.043601</td>
      <td>0.099029</td>
    </tr>
    <tr>
      <th>HarvardX/PH207x/2012_Fall</th>
      <td>0.583742</td>
      <td>0.104155</td>
      <td>0.044287</td>
      <td>0.104179</td>
    </tr>
    <tr>
      <th>MITx/14.73x/2013_Spring</th>
      <td>0.588016</td>
      <td>0.105310</td>
      <td>0.074812</td>
      <td>0.105633</td>
    </tr>
  </tbody>
</table>
</div>

<p>The goal is to attempt to predict engagement for a user based on data they do not have before they start the course.   This is only self-reported information like the Highest Level of Education, Year of Birth, Gender, and when the course started.   We can also attemp to predict it including the course id, but this will not generalize to other courses.  </p>
<div class="highlight"><pre><span class="n">m1</span> <span class="o">=</span> <span class="n">mooc</span><span class="p">[[</span><span class="s">u&#39;LoE_DI&#39;</span><span class="p">,</span> <span class="s">u&#39;YoB&#39;</span><span class="p">,</span> <span class="s">u&#39;gender&#39;</span><span class="p">,</span> <span class="s">u&#39;start_time_DI&#39;</span><span class="p">,</span><span class="s">u&#39;engagement&#39;</span><span class="p">]]</span>


<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m1</span><span class="o">.</span><span class="n">LoE_DI</span><span class="o">.</span><span class="n">unique</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">m1</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">m1</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">m1</span><span class="o">.</span><span class="n">LoE_DI</span><span class="o">==</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">m1</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="o">/</span><span class="n">Library</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">2.7</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">IPython</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">__main__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span> <span class="n">SettingWithCopyWarning</span><span class="p">:</span> 
<span class="n">A</span> <span class="n">value</span> <span class="ow">is</span> <span class="n">trying</span> <span class="n">to</span> <span class="n">be</span> <span class="nb">set</span> <span class="n">on</span> <span class="n">a</span> <span class="n">copy</span> <span class="n">of</span> <span class="n">a</span> <span class="nb">slice</span> <span class="kn">from</span> <span class="nn">a</span> <span class="nn">DataFrame.</span>
<span class="n">Try</span> <span class="n">using</span> <span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row_indexer</span><span class="p">,</span><span class="n">col_indexer</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span> <span class="n">instead</span>

<span class="n">See</span> <span class="n">the</span> <span class="n">the</span> <span class="n">caveats</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">documentation</span><span class="p">:</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">pandas</span><span class="o">.</span><span class="n">pydata</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pandas</span><span class="o">-</span><span class="n">docs</span><span class="o">/</span><span class="n">stable</span><span class="o">/</span><span class="n">indexing</span><span class="o">.</span><span class="n">html</span><span class="c">#indexing-view-versus-copy</span>
  <span class="kn">from</span> <span class="nn">IPython.kernel.zmq</span> <span class="kn">import</span> <span class="n">kernelapp</span> <span class="k">as</span> <span class="n">app</span>
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LoE_DI</th>
      <th>YoB</th>
      <th>gender</th>
      <th>start_time_DI</th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-10-15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2013-02-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-09-17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m1</span><span class="o">.</span><span class="n">gender</span><span class="o">.</span><span class="n">unique</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">m1</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">m1</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">m1</span><span class="o">.</span><span class="n">gender</span><span class="o">==</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">m1</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>


<span class="o">/</span><span class="n">Library</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">2.7</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">IPython</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">__main__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span> <span class="n">SettingWithCopyWarning</span><span class="p">:</span> 
<span class="n">A</span> <span class="n">value</span> <span class="ow">is</span> <span class="n">trying</span> <span class="n">to</span> <span class="n">be</span> <span class="nb">set</span> <span class="n">on</span> <span class="n">a</span> <span class="n">copy</span> <span class="n">of</span> <span class="n">a</span> <span class="nb">slice</span> <span class="kn">from</span> <span class="nn">a</span> <span class="nn">DataFrame.</span>
<span class="n">Try</span> <span class="n">using</span> <span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row_indexer</span><span class="p">,</span><span class="n">col_indexer</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span> <span class="n">instead</span>

<span class="n">See</span> <span class="n">the</span> <span class="n">the</span> <span class="n">caveats</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">documentation</span><span class="p">:</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">pandas</span><span class="o">.</span><span class="n">pydata</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pandas</span><span class="o">-</span><span class="n">docs</span><span class="o">/</span><span class="n">stable</span><span class="o">/</span><span class="n">indexing</span><span class="o">.</span><span class="n">html</span><span class="c">#indexing-view-versus-copy</span>
  <span class="kn">from</span> <span class="nn">IPython.kernel.zmq</span> <span class="kn">import</span> <span class="n">kernelapp</span> <span class="k">as</span> <span class="n">app</span>
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LoE_DI</th>
      <th>YoB</th>
      <th>gender</th>
      <th>start_time_DI</th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
      <th>m</th>
      <th>f</th>
      <th>o</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-10-15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2013-02-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-09-17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>m1 = m1.drop([&#39;LoE_DI&#39;,&#39;gender&#39;],axis=1)
m1.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>YoB</th>
      <th>start_time_DI</th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
      <th>m</th>
      <th>f</th>
      <th>o</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>2012-10-15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>2013-02-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>2012-09-17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>m1.YoB = pd.cut(m1.YoB,[0,1960,1970,1980,1990,2000,2010,2020])
for x in m1.YoB.unique()[1:]:
    m1[x] = np.nan
    m1.loc[:,x] = np.where(m1.YoB==x,1,0)
m1.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>YoB</th>
      <th>start_time_DI</th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
      <th>m</th>
      <th>f</th>
      <th>o</th>
      <th>(2010, 2020]</th>
      <th>(1980, 1990]</th>
      <th>(1960, 1970]</th>
      <th>(1970, 1980]</th>
      <th>(1990, 2000]</th>
      <th>(0, 1960]</th>
      <th>(2000, 2010]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>2012-10-15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>2013-02-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>2012-09-17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>m1 = m1.drop(&#39;YoB&#39;,axis=1)
m1.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>start_time_DI</th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
      <th>m</th>
      <th>f</th>
      <th>o</th>
      <th>(2010, 2020]</th>
      <th>(1980, 1990]</th>
      <th>(1960, 1970]</th>
      <th>(1970, 1980]</th>
      <th>(1990, 2000]</th>
      <th>(0, 1960]</th>
      <th>(2000, 2010]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2012-10-15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2013-02-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012-09-17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2012-12-19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>m1 = m1.drop([&#39;start_time_DI&#39;],axis=1)
m1.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>engagement</th>
      <th>Secondary</th>
      <th>Bachelor's</th>
      <th>Master's</th>
      <th>Doctorate</th>
      <th>Less than Secondary</th>
      <th>m</th>
      <th>f</th>
      <th>o</th>
      <th>(2010, 2020]</th>
      <th>(1980, 1990]</th>
      <th>(1960, 1970]</th>
      <th>(1970, 1980]</th>
      <th>(1990, 2000]</th>
      <th>(0, 1960]</th>
      <th>(2000, 2010]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>m1.info()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 641138 entries, 0 to 641137
Data columns (total 16 columns):
engagement             641138 non-null int64
Secondary              641138 non-null int64
Bachelor&#39;s             641138 non-null int64
Master&#39;s               641138 non-null int64
Doctorate              641138 non-null int64
Less than Secondary    641138 non-null int64
m                      641138 non-null int64
f                      641138 non-null int64
o                      641138 non-null int64
(2010, 2020]           641138 non-null int64
(1980, 1990]           641138 non-null int64
(1960, 1970]           641138 non-null int64
(1970, 1980]           641138 non-null int64
(1990, 2000]           641138 non-null int64
(0, 1960]              641138 non-null int64
(2000, 2010]           641138 non-null int64
dtypes: int64(16)
memory usage: 83.2 MB



y = m1.engagement.values
x = m1.drop(&#39;engagement&#39;,axis=1).values
print y.shape,x.shape

(641138,) (641138, 15)



from sklearn.metrics import confusion_matrix

a_train, a_test, b_train, b_test = train_test_split(x,y,test_size=0.50)

mlog = LogisticRegressionCV(cv=20)
mlog.fit(a_train,b_train)

probs = mlog.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&#39;seagreen&#39;,alpha=0.8,lw=3)
plt.xlabel(&quot;False Positive Rate (1 - Specificity)&quot;)
plt.ylabel(&quot;True Positive Rate (Sensitivity, Recall)&quot;)
plt.title(&quot;MOOC&quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &quot;Recall: &quot;, precision_score(mlog.predict(a_test),b_test)
print &quot;Precision: &quot;, recall_score(mlog.predict(a_test),b_test)
print &quot;Accuracy: &quot;, accuracy_score(mlog.predict(a_test),b_test)
confusion_matrix(mlog.predict(a_test),b_test)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_60_0.png" /></p>
<div class="highlight"><pre><span class="n">Recall</span><span class="o">:</span>  <span class="mf">0.0</span>
<span class="n">Precision</span><span class="o">:</span>  <span class="mf">0.0</span>
<span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">0.937143017572</span>





<span class="n">array</span><span class="o">([[</span><span class="mi">300419</span><span class="o">,</span>  <span class="mi">20150</span><span class="o">],</span>
       <span class="o">[</span>     <span class="mi">0</span><span class="o">,</span>      <span class="mi">0</span><span class="o">]])</span>
</pre></div>


<p>This model, unsurpisingly, predicts no engagement because engagement is so rare.  This is a know problem with logistic regresion.   To avoid this we can drop duplicates and refit the data.</p>
<div class="highlight"><pre>m2 = m1.drop_duplicates()
y = m2.engagement.values
x = m2.drop(&#39;engagement&#39;,axis=1).values
print y.shape,x.shape

(163,) (163, 15)



a_train, a_test, b_train, b_test = train_test_split(x,y,test_size=0.50)

mlog = LogisticRegressionCV(cv=20)
mlog.fit(a_train,b_train)

probs = mlog.predict_proba(a_test)[:,1]
tpr1, fpr1, thresholds1 = roc_curve(probs, b_test)

plt.plot(fpr, tpr,color=&#39;seagreen&#39;,alpha=0.8,lw=3)
plt.xlabel(&quot;False Positive Rate (1 - Specificity)&quot;)
plt.ylabel(&quot;True Positive Rate (Sensitivity, Recall)&quot;)
plt.title(&quot;MOOC&quot;)
plt.xlim([0,1])
plt.ylim([0,1])
plt.legend(loc=4)
plt.show()
print &quot;Recall: &quot;, precision_score(mlog.predict(a_test),b_test)
print &quot;Precision: &quot;, recall_score(mlog.predict(a_test),b_test)
print &quot;Accuracy: &quot;, accuracy_score(mlog.predict(a_test),b_test)
confusion_matrix(mlog.predict(a_test),b_test)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_63_0.png" /></p>
<div class="highlight"><pre><span class="n">Recall</span><span class="o">:</span>  <span class="mf">0.121951219512</span>
<span class="n">Precision</span><span class="o">:</span>  <span class="mf">0.277777777778</span>
<span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">0.40243902439</span>





<span class="n">array</span><span class="o">([[</span><span class="mi">28</span><span class="o">,</span> <span class="mi">36</span><span class="o">],</span>
       <span class="o">[</span><span class="mi">13</span><span class="o">,</span>  <span class="mi">5</span><span class="o">]])</span>




<span class="n">probs</span> <span class="o">=</span> <span class="n">mlog</span><span class="o">.</span><span class="na">predict_proba</span><span class="o">(</span><span class="n">m1</span><span class="o">.</span><span class="na">drop</span><span class="o">(</span><span class="s1">&#39;engagement&#39;</span><span class="o">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="o">).</span><span class="n">values</span><span class="o">)[:,</span><span class="mi">1</span><span class="o">]</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">mlog</span><span class="o">.</span><span class="na">predict</span><span class="o">(</span><span class="n">m1</span><span class="o">.</span><span class="na">drop</span><span class="o">(</span><span class="s1">&#39;engagement&#39;</span><span class="o">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="o">).</span><span class="n">values</span><span class="o">)</span>
<span class="n">tpr1</span><span class="o">,</span> <span class="n">fpr1</span><span class="o">,</span> <span class="n">thresholds1</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="o">(</span><span class="n">probs</span><span class="o">,</span> <span class="n">m1</span><span class="o">.</span><span class="na">engagement</span><span class="o">.</span><span class="na">values</span><span class="o">)</span>

<span class="n">plt</span><span class="o">.</span><span class="na">plot</span><span class="o">(</span><span class="n">fpr</span><span class="o">,</span> <span class="n">tpr</span><span class="o">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;seagreen&#39;</span><span class="o">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="o">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="o">)</span>
<span class="n">plt</span><span class="o">.</span><span class="na">xlabel</span><span class="o">(</span><span class="s2">&quot;False Positive Rate (1 - Specificity)&quot;</span><span class="o">)</span>
<span class="n">plt</span><span class="o">.</span><span class="na">ylabel</span><span class="o">(</span><span class="s2">&quot;True Positive Rate (Sensitivity, Recall)&quot;</span><span class="o">)</span>
<span class="n">plt</span><span class="o">.</span><span class="na">title</span><span class="o">(</span><span class="s2">&quot;MOOC&quot;</span><span class="o">)</span>
<span class="n">plt</span><span class="o">.</span><span class="na">xlim</span><span class="o">([</span><span class="mi">0</span><span class="o">,</span><span class="mi">1</span><span class="o">])</span>
<span class="n">plt</span><span class="o">.</span><span class="na">ylim</span><span class="o">([</span><span class="mi">0</span><span class="o">,</span><span class="mi">1</span><span class="o">])</span>
<span class="n">plt</span><span class="o">.</span><span class="na">legend</span><span class="o">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="o">)</span>
<span class="n">plt</span><span class="o">.</span><span class="na">show</span><span class="o">()</span>
<span class="n">print</span> <span class="s2">&quot;Recall: &quot;</span><span class="o">,</span> <span class="n">precision_score</span><span class="o">(</span><span class="n">predict</span><span class="o">,</span><span class="n">m1</span><span class="o">.</span><span class="na">engagement</span><span class="o">.</span><span class="na">values</span><span class="o">)</span>
<span class="n">print</span> <span class="s2">&quot;Precision: &quot;</span><span class="o">,</span> <span class="n">recall_score</span><span class="o">(</span><span class="n">predict</span><span class="o">,</span><span class="n">m1</span><span class="o">.</span><span class="na">engagement</span><span class="o">.</span><span class="na">values</span><span class="o">)</span>
<span class="n">print</span> <span class="s2">&quot;Accuracy: &quot;</span><span class="o">,</span> <span class="n">accuracy_score</span><span class="o">(</span><span class="n">predict</span><span class="o">,</span><span class="n">m1</span><span class="o">.</span><span class="na">engagement</span><span class="o">.</span><span class="na">values</span><span class="o">)</span>
<span class="n">confusion_matrix</span><span class="o">(</span><span class="n">predict</span><span class="o">,</span><span class="n">m1</span><span class="o">.</span><span class="na">engagement</span><span class="o">.</span><span class="na">values</span><span class="o">)</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW03D4/output_64_0.png" /></p>
<div class="highlight"><pre><span class="n">Recall</span><span class="o">:</span>  <span class="mf">0.136417673866</span>
<span class="n">Precision</span><span class="o">:</span>  <span class="mf">0.0649505324104</span>
<span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">0.821936930895</span>





<span class="n">array</span><span class="o">([[</span><span class="mi">521467</span><span class="o">,</span>  <span class="mi">34868</span><span class="o">],</span>
       <span class="o">[</span> <span class="mi">79295</span><span class="o">,</span>   <span class="mi">5508</span><span class="o">]])</span>
</pre></div>


<p>I do not think the model is any better.   Ad at this point I think based on this data we are not able to predict engagement by self-reported features</p>
    </div>
  </div>
  <hr class="separator">
  <div class="col-md-8 col-md-offset-2">
  <div id="disqus_thread">
    <script>
      var disqus_shortname = 'bryansmithphd';
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] ||
         document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript=bryansmithphd">
        comments powered by Disqus.
      </a>
    </noscript>
    <a href="https://disqus.com" class="dsq-brlink">
      blog comments powered by <span class="logo-disqus">Disqus</span>
    </a>
  </div>
  </div>
  </div>
<footer class="footer">
  <div class="container">
    <p class="text-center">
      Bryan Smith, <a href="" target="_blank"></a> unless otherwise noted.
    </p>
    <div class="text-center">
      Generated by <a href="http://getpelican.com" target="_blank">Pelican</a> with the <a href="http://github.com/nairobilug/pelican-alchemy">alchemy</a> theme.
    </div>
  </div>
</footer> <!-- /.footer -->
  <script src="http://www.bryantravissmith.com/theme/js/jquery.min.js"></script>
  <script src="http://www.bryantravissmith.com/theme/js/bootstrap.min.js"></script>
</body> <!-- 42 -->
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$$','$$'], ['\\(','\\)']]}
});
</script>
</html>