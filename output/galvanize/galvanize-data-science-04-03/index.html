<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-04-03/index.html" />

    <title>  Bryan Travis Smith, Ph.D &mdash; Galvanize - Week 04 - Day 3
</title>




    <link rel="stylesheet" href="http://www.bryantravissmith.com/theme/css/style.css">

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-24340005-3', 'auto');
    ga('send', 'pageview');

  </script>

    <meta name="author" content="Bryan Smith">
    <meta name="description" content="Today we covered SVMs.">
  <meta name="tags" contents="data-science, galvanize, SVM, support vector machines, ">
</head>

<body>
<header class="header">
  <div class="container">
      <div class="header-image pull-left">
        <a class="nodec" href="http://www.bryantravissmith.com"><img src=http://www.bryantravissmith.com/img/bryan.jpeg></a>
      </div>
    <div class="header-inner">
      <h1 class="header-name">
        <a class="nodec" href="http://www.bryantravissmith.com">Bryan Travis Smith, Ph.D</a>
      </h1>
      <h3 class="header-text">Physicist, Data Scientist, Martial Artist, & Life Enthusiast</h3>
      <ul class="header-menu list-inline">
              <li class="muted">|</li>
            <li><a class="nodec" href="http://www.bryantravissmith.com/about/">About</a></li>
              <li class="muted">|</li>
          <li><a class="nodec icon-mail-alt" href="mailto:bryantravissmith@gmail.com"></a></li>
          <li><a class="nodec icon-github" href="https://github.com/bryantravissmith"></a></li>
      </ul>
    </div>
  </div>
</header> <!-- /.header -->  <div class="container">
  <div class="post full-post">
    <h1 class="post-title">
      <a href="/galvanize/galvanize-data-science-04-03/" title="Permalink to Galvanize - Week 04 - Day 3">Galvanize - Week 04 - Day 3</a>
    </h1>
    <ul class="list-inline">
      <li class="post-date">
        <a class="text-muted" href="/galvanize/galvanize-data-science-04-03/" title="2015-06-24T10:20:00-07:00">Wed 24 June 2015</a>
      </li>
      <li class="muted">&middot;</li>
      <li class="post-category">
        <a href="http://www.bryantravissmith.com/category/galvanize.html">Galvanize</a>
      </li>
        <li class="muted">&middot;</li>
        <li>
          <address class="post-author">
            By <a href="http://www.bryantravissmith.com/author/bryan-smith.html">Bryan Smith</a>
          </address>
        </li>
    </ul>
    <div class="post-content">
      <h1>Galvanize Immersive Data Science</h1>
<h2>Week 4 - Day 3</h2>
<p>Our quiz toda was about making change.  Given a sufficient amount US coins what is the minimum number of coins needed to give change for a specificied amount?   </p>
<p>We were suppose to build a function to do this.  My solution was to sort the list of coins from largest to smallest.   We then make the maximum amount of change using the largest denomination, then continue this until we have given change back.</p>
<div class="highlight"><pre>def find_change(coins,value):
    coins.sort(reverse=True)
    n = 0
    v = value
    for x in coins:
        m =  v / x
        n += m
        v = v - m*x       
    return n

coins = [1,5,10,25]
print &quot;Correct Answer 4, Your Answer: &quot;, find_change(coins,100)
print &quot;Correct Answer 8, Your Answer: &quot;,find_change(coins,74)

Correct Answer 4, Your Answer:  4
Correct Answer 8, Your Answer:  8
</pre></div>


<p>After the quiz we had a lecture on Support Vector Machines.  The afternoon lecture was on kernel tricks for SVMs.   </p>
<h2>Morning: Maximal Margin Classifier</h2>
<p>We learned that a support vectore machine is a maximum margin classifier, trying to construct a hyper plane that maximize the margin of linearly seperable data.   </p>
<p>To help get a feel between SVMs and other classifiers we looked up some made up data about the number of hours emailing and number of hours spent at the gym for a people labeled as a data scientist or not a data scientist.</p>
<p>This dataset is special in that it is linearly seperable and easily displayed in two dimensions</p>
<div class="highlight"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;data/data_scientist.csv&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">&#39;gym_hours&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;steelblue&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;Not Data Scientist&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">&#39;gym_hours&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;Data Scientist&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Hours Emailing&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Hours At Gym&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/(output_3_0.png" /></p>
<h2>Margin of Logistic Regression Boundary</h2>
<p>We learned previously that logistic regression minimizes the log-loss function:</p>
<p>$$ - \ \sum_{i=1}^m [ \ y_i \ log(h_\theta (x_i)) \ + \ (1-y_i) \ log(1-h_\theta (x_i)) \ ]$$</p>
<p>Where $$h_\theta(x) = \frac{1}{1+e^{x\theta}}$$.  </p>
<p>This is different that explicitly maximizing the margin of a decision boundary.  </p>
<p>We can fit a logistic regression model on our data that has 100% accuracy.  I have plotted the theta dotted the data on the x-axis, and the data scientist status on the y-axis.  </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">lin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">+</span><span class="n">lin</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">df</span><span class="p">[</span><span class="s">&#39;z&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">z</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;z&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">&#39;data_scientist&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;steelblue&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;Not Data Scientist&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;z&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">&#39;data_scientist&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;Data Scientist&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">zp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yp</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">zp</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zp</span><span class="p">,</span><span class="n">yp</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;indianred&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;Logistic Fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">print</span> <span class="s">&quot;Accuracy: &quot;</span><span class="p">,</span> <span class="n">lin</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_5_0.png" /></p>
<div class="highlight"><pre><span class="n">Accuracy</span><span class="o">:</span>  <span class="mf">1.0</span>
</pre></div>


<h3>Fun Fact!</h3>
<p>I origianlly fit this without regulariaiton, but the algorithm was converging before finding the optimal solution.  I added a regularization term to make a better (100%) fit to the classification.  </p>
<ol>
<li>
<p>Write a function to compute and plot the decision boundary. Remember <code>y</code> is <code>0</code> at the decision boundary when
   the probability of a positive class is <code>0.5</code>. You should also define a range over one of your features (<code>gym_hours</code>
   for example) and compute the <code>email_hours</code> at the decision boundary.</p>
<p>x = np.linspace(0,50,100)
y = np.linspace(0,50,100)
xx, yy = np.meshgrid(x,y)
Z = lin.predict(np.c_[xx.ravel(), yy.ravel()])
zz = Z.reshape(xx.shape)</p>
<h1>zz = 1/(1+np.exp(xx<em>lin.coef_[0,0]+yy</em>lin.coef_[0,1]+lin.intercept_))</h1>
<p>extent = [0,50,0,50]
plt.figure(figsize=(14,8))
plt.pcolormesh(xx, yy, zz, cmap='winter',alpha=0.1, ) #plt.cm.Paired)</p>
<p>ax = plt.subplot(1,1,1)</p>
<h1>b = (0.5 - lin.intercept_)/lin.coef_[0,1]</h1>
<h1>m = -lin.coef_[0,0]/lin.coef_[0,1]</h1>
<h1>plt.plot(x,m*x+b,linestyle='--',color='black')</h1>
<p>df[df.data_scientist==0].plot(kind='scatter',x='email_hours',y='gym_hours',color='steelblue',s=100,label='Not Data Scientist',alpha=0.8,ax=ax)
df[df.data_scientist==1].plot(kind='scatter',x='email_hours',y='gym_hours',color='seagreen',s=100,label='Data Scientist',alpha=0.8, ax=ax)
plt.xlim([0,50])
plt.ylim([0,50])
plt.xlabel('Hours Emailing')
plt.ylabel('Hours At Gym')
plt.legend(loc=2)
plt.show()</p>
</li>
</ol>
<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_8_0.png" /></p>
<p>The distance each point is from the margin is given by the following equations </p>
<p>$$\mbox{distance} = \frac{\beta_0+\beta x^T}{||\beta||}$$</p>
<p>A line perpendicular to a line will always have a slope that is $\frac{1}{\mbox{slope}}.  We can use this equation and make our plot to illustrate the distance from the margin using the size property.  </p>
<div class="highlight"><pre>def distance(x,slopes,intercept):
    return (intercept+x.dot(slopes))/np.sqrt(intercept**2+slopes.T.dot(slopes))

df[&#39;s&#39;] = np.abs(250*distance(df[[&#39;email_hours&#39;,&#39;gym_hours&#39;]].values,lin.coef_.T,lin.intercept_))
x = np.linspace(0,50,100)
y = np.linspace(0,50,100)
xx, yy = np.meshgrid(x,y)
Z = lin.predict(np.c_[xx.ravel(), yy.ravel()])
zz = Z.reshape(xx.shape)
#zz = 1/(1+np.exp(xx*lin.coef_[0,0]+yy*lin.coef_[0,1]+lin.intercept_)) 
extent = [0,50,0,50]
plt.figure(figsize=(14,8))
plt.pcolormesh(xx, yy, zz, cmap=&#39;winter&#39;,alpha=0.1, ) #plt.cm.Paired)

ax = plt.subplot(1,1,1)
#b = (0.5 - lin.intercept_)/lin.coef_[0,1]
#m = -lin.coef_[0,0]/lin.coef_[0,1]
#plt.plot(x,m*x+b,linestyle=&#39;--&#39;,color=&#39;black&#39;)
df[df.data_scientist==0].plot(kind=&#39;scatter&#39;,x=&#39;email_hours&#39;,
                              y=&#39;gym_hours&#39;,color=&#39;steelblue&#39;,
                              s=df[df.data_scientist==0].s,
                              label=&#39;Not Data Scientist&#39;,
                              alpha=0.8,ax=ax)
df[df.data_scientist==1].plot(kind=&#39;scatter&#39;,x=&#39;email_hours&#39;,
                              y=&#39;gym_hours&#39;,color=&#39;seagreen&#39;,
                              s=df[df.data_scientist==1].s,
                              label=&#39;Data Scientist&#39;,alpha=0.8, 
                              ax=ax)
plt.xlim([0,50])
plt.ylim([0,50])
plt.xlabel(&#39;Hours Emailing&#39;)
plt.ylabel(&#39;Hours At Gym&#39;)
plt.legend(loc=2)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_11_0.png" /></p>
<h2>Margin of Support Vector Machines</h2>
<p>We learned that the SVM is a maximal margin classifier which in theory would have a larger margin than Logistic Regression. We will go through the same process that we just did for logistic regression.   </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>




<span class="mf">1.0</span>




<span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">zz</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s">&#39;ss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">distance</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span><span class="s">&#39;gym_hours&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">&#39;winter&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="p">)</span> <span class="c">#plt.cm.Paired)</span>


<span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span><span class="o">/</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">m</span> <span class="o">=</span> <span class="o">-</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">&#39;--&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">&#39;-&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="o">-</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">&#39;-&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span>
                              <span class="n">y</span><span class="o">=</span><span class="s">&#39;gym_hours&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;steelblue&#39;</span><span class="p">,</span>
                              <span class="n">s</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ss</span><span class="p">,</span>
                              <span class="n">label</span><span class="o">=</span><span class="s">&#39;Not Data Scientist&#39;</span><span class="p">,</span>
                              <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">&#39;scatter&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">&#39;email_hours&#39;</span><span class="p">,</span>
                              <span class="n">y</span><span class="o">=</span><span class="s">&#39;gym_hours&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span>
                              <span class="n">s</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">data_scientist</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ss</span><span class="p">,</span>
                              <span class="n">label</span><span class="o">=</span><span class="s">&#39;Data Scientist&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                              <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Hours Emailing&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Hours At Gym&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_14_0.png" /></p>
<p>The SVM classifier seems to match my intuition for what the optimal boundary should be, and the logistic regression did not capture that.  If we had a new data point at 10 hours of emailing and 15 hours at the gym, I would expect that person to be a data scientist because it is closer to the cluster of data scientists.  The logistic regression fit we did previously would have classifed it as a non-data scientist.  This is despit it is so far from the cluster.</p>
<p>Just because this is my intuition does not mean that its correct.   There are problems that this intuition is incorrect.   That is why you might have some insight into the problem because picking a classifier.  That is also why you cross validate and test on a unseen test set.   </p>
<h2>Scaling</h2>
<p>We just worked a problem where the scaling of the two variables are the same, but if they are not this will mess with the results the svm will produce.   The distance measurements change with units.  It is standard practice to scale variables before fitting an SVM on a data set.   If we do not, we can get different results.  </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;data/non_sep.csv&#39;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                    <span class="p">(</span><span class="s">&#39;svc&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">))])</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s">&#39;svc&#39;</span><span class="p">]</span>

<span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yvals</span> <span class="o">=</span> <span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="o">+</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">xvals</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">y</span><span class="o">==</span><span class="mi">1</span>

<span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span><span class="p">,</span><span class="n">zz</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">&#39;BrBG&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;seagreen&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;1</span><span class="se">\&#39;</span><span class="s">s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s">&#39;burlywood&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">&#39;0</span><span class="se">\&#39;</span><span class="s">s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_16_0.png" /></p>
<p>In this case the data is not linearly seperable.  You will have noticed I change the colors from the previous spot to values that contrast better.  That way you can see when values are misclassified more easily.  The overlap is not very much in this instance, so we can still git fair accuracy.   A 5-Fold cross validation shows above 90% accuracy.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>




<span class="mf">0.93000000000000005</span>
</pre></div>


<h2>Tuning SVMs</h2>
<p>The SVM has a C parameter that acts like $\frac{1}{\lambda}$ for regularization in Lasso and Ridge Regression.  Changing this values allows allowing error.  We get get a feel for how it affect the accuracy of a prediction by scanning through a number of values of this tuning paramter.</p>
<div class="highlight"><pre>cv_score = []
Cs = np.logspace(-3, 1, 100)
for c in Cs:
    pipeline = Pipeline([(&#39;scaler&#39;, StandardScaler()),
                    (&#39;svc&#39;, SVC(kernel=&#39;linear&#39;, C=c))])
    cv_score.append(cross_val_score(pipeline, x, y, scoring=&#39;accuracy&#39;, cv=10).mean())

plt.figure(figsize=(14,8))
plt.plot(Cs,cv_score,lw=2,color=&#39;seagreen&#39;)
plt.xlabel(&quot;Tuning Paramter&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.xscale(&quot;log&quot;)
plt.ylim([0.9,.95])
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_20_0.png" /></p>
<p>We can see that by changing the tuning parameter that we change the accuracy of the SVM, but there are similar accuracies for different C values.  To show what the algorithm is doing we can plot the decision boundaries for two very different C's on the non-seperable dataset we have started investigating.  </p>
<div class="highlight"><pre>pipeline1 = Pipeline([(&#39;scaler&#39;, StandardScaler()),
                    (&#39;svc&#39;, SVC(kernel=&#39;linear&#39;,C=.01))])
pipeline1.fit(x, y)
pipeline2= Pipeline([(&#39;scaler&#39;, StandardScaler()),
                    (&#39;svc&#39;, SVC(kernel=&#39;linear&#39;,C=10))])
pipeline2.fit(x, y)

mask = y==1
xx,yy = np.meshgrid(np.linspace(-5,5,100),np.linspace(-5,5,100))
Z1 = pipeline1.predict(np.c_[xx.ravel(),yy.ravel()])
zz1 = Z1.reshape(xx.shape)

Z2 = pipeline2.predict(np.c_[xx.ravel(),yy.ravel()])
zz2 = Z2.reshape(xx.shape)

plt.figure(figsize=(14,8))
plt.pcolormesh(xx,yy,zz1,cmap=&#39;BrBG&#39;,alpha=0.2)
plt.pcolormesh(xx,yy,zz2,cmap=&#39;binary&#39;,alpha=0.2)
plt.plot(x[mask,0],x[mask,1],color=&#39;seagreen&#39;,marker=&#39;o&#39;, markersize=10,lw=0,label=&#39;1\&#39;s&#39;)
plt.plot(x[~mask,0],x[~mask,1],color=&#39;burlywood&#39;,marker=&#39;o&#39;,markersize=10,lw=0,label=&#39;0\&#39;s&#39;)
plt.legend()
plt.xlim([-5,5])
plt.ylim([-5,5])
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_22_0.png" /></p>
<p>We see that the two regions have different slopes.  I mapped a binary (black/white) colormap over the large C fit.   This makes the background darker.  The smaller C has the hyperplane seperator with a more negative slope than the larger C.   By changing the hyperparameter, we are ultimately move the hyperplane that is attempting to fit the data.</p>
<h2>Kernels</h2>
<p>SVM's can accept a kernal argument that effectively maps the data into a higher dimension, potentially making the data linearably seperable when it otherwise might not be.  We can illustrate this with a simple example using random data.  We will have some data that is seperable, but not linearably seperable.   </p>
<div class="highlight"><pre>x1r = 2*np.random.random((200,1))-1
x2r = 2*np.random.random((200,1))-1

x3r = x1r*x1r+x2r*x2r

yr = np.sqrt(x1r*x1r+x2r*x2r) &lt; .5

mask = yr==0
plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
plt.plot(x1r[mask],x2r[mask],&#39;ro&#39;)
plt.plot(x1r[~mask],x2r[~mask],&#39;bo&#39;)
plt.subplot(1,2,2)
plt.plot(x1r[mask],x3r[mask],&#39;ro&#39;)
plt.plot(x1r[~mask],x3r[~mask],&#39;bo&#39;)
plt.axhline(y=0.25)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_24_0.png" /></p>
<p>By transforming the data into a third dimention we take a seperable data and make in seperable by a hyperplane.  How we can train an SVM on this data, and find the decision boundary.  Two common kernals to fit data two is a gaussian kernal and a polynomial kernal.  This allow for making non-linear decision surfaces.  Lets first look at the RBF kernel.</p>
<h2>RBF Kernel</h2>
<div class="highlight"><pre>def plot_surface(kernel,C=1,degree=3,gamma=1,*args):
    pipeline = Pipeline([(&#39;scaler&#39;, StandardScaler()),
                    (&#39;svc&#39;, SVC(kernel=kernel,C=C,degree=degree,gamma=1,*args))])
    pipeline.fit(x,y)
    svc_rbf = pipeline.named_steps[&#39;svc&#39;]


    mask = y==1
    xx,yy = np.meshgrid(np.linspace(-5,5,100),np.linspace(-5,5,100))
    Z = pipeline.predict(np.c_[xx.ravel(),yy.ravel()])
    zz = Z.reshape(xx.shape)


    plt.figure(figsize=(14,8))
    plt.pcolormesh(xx,yy,zz,cmap=&#39;BrBG&#39;,alpha=0.3)
    plt.plot(x[mask,0],x[mask,1],color=&#39;seagreen&#39;,marker=&#39;o&#39;, markersize=10,lw=0,label=&#39;1\&#39;s&#39;)
    plt.plot(x[~mask,0],x[~mask,1],color=&#39;burlywood&#39;,marker=&#39;o&#39;,markersize=10,lw=0,label=&#39;0\&#39;s&#39;)
    plt.legend()
    plt.xlim([-5,5])
    plt.ylim([-5,5])
    plt.title(&quot;SVM with &quot; + kernel + &quot; Kernel, C = &quot;+str(C))
    plt.show()

plot_surface(&#39;rbf&#39;)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_26_0.png" /></p>
<p>The decision surface of this SVM is clearly not linear in the data, but it is linear in a hyperspace the data is projected into.   We can look at how the accuracy changes as we turn the model.  As well as how the deciion surfaces changes.</p>
<div class="highlight"><pre>cv_score = []
Cs = np.logspace(-3, 1, 100)
for c in Cs:
    pipeline = Pipeline([(&#39;scaler&#39;, StandardScaler()),
                    (&#39;svc&#39;, SVC(kernel=&#39;rbf&#39;, C=c))])
    cv_score.append(cross_val_score(pipeline, x, y, scoring=&#39;accuracy&#39;, cv=10).mean())

plt.figure(figsize=(14,8))
plt.plot(Cs,cv_score,lw=2,color=&#39;seagreen&#39;)
plt.xlabel(&quot;Tuning Paramter&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.xscale(&quot;log&quot;)
plt.ylim([0.89,.95])
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_28_0.png" /></p>
<div class="highlight"><pre>plot_surface(&#39;rbf&#39;,C=100)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_29_0.png" /></p>
<div class="highlight"><pre>plot_surface(&#39;rbf&#39;,C=1000)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_30_0.png" /></p>
<p>We can see that as we increase the turning parameter, which is lower the regulization, we get very curvy decision surfaces.</p>
<h2>Polynomial</h2>
<p>The polynomial kernal allows for some curvature to the decision surface, but usally less than that fit by the RBF kernel.   We can see a degree polynomial curve below.</p>
<div class="highlight"><pre>plot_surface(&#39;poly&#39;)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_32_0.png" /></p>
<p>We can look at the surface as we change the turning parameter and the degree of the kernel.</p>
<div class="highlight"><pre>plot_surface(&#39;rbf&#39;,C=100,degree=3)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_34_0.png" /></p>
<div class="highlight"><pre>plot_surface(&#39;rbf&#39;,C=1,degree=5)
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_35_0.png" /></p>
<p>We see that the hypersurface changes its form adn has more curvature as we increase the size of these parameters.  </p>
<h2>Grid Search</h2>
<p>Ideally we will want to find the best model to fit the data we are given.  This is difficult to do by hand, so we can use a grid search strategy to find the best model on our training data.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;svc__degree&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="s">&#39;svc__C&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                    <span class="p">(</span><span class="s">&#39;svc&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;poly&#39;</span><span class="p">))])</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Best Accuracy:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span>
<span class="k">print</span> <span class="s">&quot;Best Parameters:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span>

<span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.94</span>
<span class="n">Best</span> <span class="n">Parameters</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;svc__degree&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">&#39;svc__C&#39;</span><span class="p">:</span> <span class="mf">0.058570208180566671</span><span class="p">}</span>



<span class="n">plot_surface</span><span class="p">(</span><span class="s">&#39;poly&#39;</span><span class="p">,</span><span class="n">C</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s">&#39;svc__C&#39;</span><span class="p">],</span><span class="n">degree</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s">&#39;svc__degree&#39;</span><span class="p">])</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_38_0.png" /></p>
<h2>Multi-Classification</h2>
<p>We can also use SVM's, and other classifiers, with datesets that have multiple classifications.   An example is the digits dataset that has hand written digits as images and we are attempting to classify them.</p>
<p>The two methods is 1 vs all, which makes a classifier for each classification.  Then each data point is scored by each classifier.  The highest scored predictor is then classified as a positive example in that classifier.
In 1 vs 1, a classifier is made for each pair of data, and then there is a vote.  The classification with the most votes win.   We will be using both of these with a SVM on the digit data.</p>
<p>Because we are going to be using linear kernals, there is a optimize algorithm in sklearn that we will be using.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span><span class="p">,</span><span class="n">OneVsOneClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">precision_score</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">n_class</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1797</span><span class="p">,</span><span class="mi">64</span><span class="p">)</span>

<span class="n">LinVsAll</span> <span class="o">=</span> <span class="n">OneVsRestClassifier</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">())</span>
<span class="n">LinVsOne</span> <span class="o">=</span> <span class="n">OneVsOneClassifier</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="s">&#39;Greys&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/GW04D3/output_40_0.png" /></p>
<p>The data are 8x8 pixle images of numbers that are hand written.  There are 10 classifications available in tis dataset.  We will train both multiclassification methods on the dataset and check the test accuracy.</p>
<div class="highlight"><pre>x_train, x_test, y_train, y_test = train_test_split(images,data.target)

LinVsAll.fit(x_train,y_train)
y_pred = LinVsAll.predict(x_test)

print &quot;One Vs All&quot;
print &quot;Accuracy: &quot;, accuracy_score(y_test,y_pred)
print &quot;Recall: &quot;, recall_score(y_test,y_pred,average=&#39;weighted&#39;)
print &quot;Precision: &quot;, precision_score(y_test,y_pred,average=&#39;weighted&#39;)


print &quot;&quot;
print &quot;One Vs One&quot;
LinVsOne.fit(x_train,y_train)
y_pred = LinVsOne.predict(x_test)

print &quot;Accuracy: &quot;, accuracy_score(y_test,y_pred)
print &quot;Recall: &quot;, recall_score(y_test,y_pred,average=&#39;weighted&#39;)
print &quot;Precision: &quot;, precision_score(y_test,y_pred,average=&#39;weighted&#39;)

One Vs All
Accuracy:  0.942222222222
Recall:  0.942222222222
Precision:  0.942982349293

One Vs One
Accuracy:  0.975555555556
Recall:  0.975555555556
Precision:  0.976581128748
</pre></div>


<p>In this case we have the One vs One method being more accurate.  I suspect that we find some values of 7,9, and 4 that are pretty similar, and the voting helps seperate them while the best score does not. </p>
<h2>Real World Data</h2>
<p>We have some biological data that we are asked to predict if a given sample comes from stool or tissue.  This problem is interesting to me because the data is not structured in a way for us to answer it.  We have to restructure the data set!</p>
<div class="highlight"><pre>df_b = pd.read_csv(&#39;data/bio.csv&#39;).drop(&#39;Group&#39;,axis=1)
df_b.head()
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Taxon</th>
      <th>Patient</th>
      <th>Tissue</th>
      <th>Stool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Firmicutes</td>
      <td>1</td>
      <td>136</td>
      <td>4182</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Firmicutes</td>
      <td>2</td>
      <td>1174</td>
      <td>703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Firmicutes</td>
      <td>3</td>
      <td>408</td>
      <td>3946</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Firmicutes</td>
      <td>4</td>
      <td>831</td>
      <td>8605</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Firmicutes</td>
      <td>5</td>
      <td>693</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df_b = df_b.pivot(&#39;Patient&#39;,&#39;Taxon&#39;)
df_b
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="5" halign="left">Tissue</th>
      <th colspan="5" halign="left">Stool</th>
    </tr>
    <tr>
      <th>Taxon</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
    </tr>
    <tr>
      <th>Patient</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>1590</td>
      <td>67</td>
      <td>136</td>
      <td>195</td>
      <td>2469</td>
      <td>4</td>
      <td>0</td>
      <td>4182</td>
      <td>18</td>
      <td>1821</td>
    </tr>
    <tr>
      <th>2</th>
      <td>25</td>
      <td>0</td>
      <td>1174</td>
      <td>42</td>
      <td>839</td>
      <td>2</td>
      <td>0</td>
      <td>703</td>
      <td>2</td>
      <td>661</td>
    </tr>
    <tr>
      <th>3</th>
      <td>259</td>
      <td>85</td>
      <td>408</td>
      <td>316</td>
      <td>4414</td>
      <td>300</td>
      <td>5</td>
      <td>3946</td>
      <td>43</td>
      <td>18</td>
    </tr>
    <tr>
      <th>4</th>
      <td>568</td>
      <td>143</td>
      <td>831</td>
      <td>202</td>
      <td>12044</td>
      <td>7</td>
      <td>7</td>
      <td>8605</td>
      <td>40</td>
      <td>83</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1102</td>
      <td>678</td>
      <td>693</td>
      <td>116</td>
      <td>2310</td>
      <td>9</td>
      <td>2</td>
      <td>50</td>
      <td>0</td>
      <td>12</td>
    </tr>
    <tr>
      <th>6</th>
      <td>678</td>
      <td>4829</td>
      <td>718</td>
      <td>527</td>
      <td>3053</td>
      <td>377</td>
      <td>209</td>
      <td>717</td>
      <td>12</td>
      <td>547</td>
    </tr>
    <tr>
      <th>7</th>
      <td>260</td>
      <td>74</td>
      <td>173</td>
      <td>357</td>
      <td>395</td>
      <td>58</td>
      <td>651</td>
      <td>33</td>
      <td>11</td>
      <td>2174</td>
    </tr>
    <tr>
      <th>8</th>
      <td>424</td>
      <td>169</td>
      <td>228</td>
      <td>106</td>
      <td>2651</td>
      <td>233</td>
      <td>254</td>
      <td>80</td>
      <td>11</td>
      <td>767</td>
    </tr>
    <tr>
      <th>9</th>
      <td>548</td>
      <td>106</td>
      <td>162</td>
      <td>67</td>
      <td>1195</td>
      <td>21</td>
      <td>10</td>
      <td>3196</td>
      <td>14</td>
      <td>76</td>
    </tr>
    <tr>
      <th>10</th>
      <td>201</td>
      <td>73</td>
      <td>372</td>
      <td>203</td>
      <td>6857</td>
      <td>83</td>
      <td>381</td>
      <td>32</td>
      <td>6</td>
      <td>795</td>
    </tr>
    <tr>
      <th>11</th>
      <td>42</td>
      <td>30</td>
      <td>4255</td>
      <td>392</td>
      <td>483</td>
      <td>75</td>
      <td>359</td>
      <td>4361</td>
      <td>6</td>
      <td>666</td>
    </tr>
    <tr>
      <th>12</th>
      <td>109</td>
      <td>51</td>
      <td>107</td>
      <td>28</td>
      <td>2950</td>
      <td>59</td>
      <td>51</td>
      <td>1667</td>
      <td>25</td>
      <td>3994</td>
    </tr>
    <tr>
      <th>13</th>
      <td>51</td>
      <td>2473</td>
      <td>96</td>
      <td>12</td>
      <td>1541</td>
      <td>183</td>
      <td>2314</td>
      <td>223</td>
      <td>22</td>
      <td>816</td>
    </tr>
    <tr>
      <th>14</th>
      <td>310</td>
      <td>102</td>
      <td>281</td>
      <td>305</td>
      <td>1307</td>
      <td>204</td>
      <td>33</td>
      <td>2377</td>
      <td>32</td>
      <td>53</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df_b = df_b.stack(level=0)
df_b
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Taxon</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
    </tr>
    <tr>
      <th>Patient</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>Tissue</th>
      <td>1590</td>
      <td>67</td>
      <td>136</td>
      <td>195</td>
      <td>2469</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>4</td>
      <td>0</td>
      <td>4182</td>
      <td>18</td>
      <td>1821</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">2</th>
      <th>Tissue</th>
      <td>25</td>
      <td>0</td>
      <td>1174</td>
      <td>42</td>
      <td>839</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>2</td>
      <td>0</td>
      <td>703</td>
      <td>2</td>
      <td>661</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">3</th>
      <th>Tissue</th>
      <td>259</td>
      <td>85</td>
      <td>408</td>
      <td>316</td>
      <td>4414</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>300</td>
      <td>5</td>
      <td>3946</td>
      <td>43</td>
      <td>18</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">4</th>
      <th>Tissue</th>
      <td>568</td>
      <td>143</td>
      <td>831</td>
      <td>202</td>
      <td>12044</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>7</td>
      <td>7</td>
      <td>8605</td>
      <td>40</td>
      <td>83</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">5</th>
      <th>Tissue</th>
      <td>1102</td>
      <td>678</td>
      <td>693</td>
      <td>116</td>
      <td>2310</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>9</td>
      <td>2</td>
      <td>50</td>
      <td>0</td>
      <td>12</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">6</th>
      <th>Tissue</th>
      <td>678</td>
      <td>4829</td>
      <td>718</td>
      <td>527</td>
      <td>3053</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>377</td>
      <td>209</td>
      <td>717</td>
      <td>12</td>
      <td>547</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">7</th>
      <th>Tissue</th>
      <td>260</td>
      <td>74</td>
      <td>173</td>
      <td>357</td>
      <td>395</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>58</td>
      <td>651</td>
      <td>33</td>
      <td>11</td>
      <td>2174</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">8</th>
      <th>Tissue</th>
      <td>424</td>
      <td>169</td>
      <td>228</td>
      <td>106</td>
      <td>2651</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>233</td>
      <td>254</td>
      <td>80</td>
      <td>11</td>
      <td>767</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">9</th>
      <th>Tissue</th>
      <td>548</td>
      <td>106</td>
      <td>162</td>
      <td>67</td>
      <td>1195</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>21</td>
      <td>10</td>
      <td>3196</td>
      <td>14</td>
      <td>76</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">10</th>
      <th>Tissue</th>
      <td>201</td>
      <td>73</td>
      <td>372</td>
      <td>203</td>
      <td>6857</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>83</td>
      <td>381</td>
      <td>32</td>
      <td>6</td>
      <td>795</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">11</th>
      <th>Tissue</th>
      <td>42</td>
      <td>30</td>
      <td>4255</td>
      <td>392</td>
      <td>483</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>75</td>
      <td>359</td>
      <td>4361</td>
      <td>6</td>
      <td>666</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">12</th>
      <th>Tissue</th>
      <td>109</td>
      <td>51</td>
      <td>107</td>
      <td>28</td>
      <td>2950</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>59</td>
      <td>51</td>
      <td>1667</td>
      <td>25</td>
      <td>3994</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">13</th>
      <th>Tissue</th>
      <td>51</td>
      <td>2473</td>
      <td>96</td>
      <td>12</td>
      <td>1541</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>183</td>
      <td>2314</td>
      <td>223</td>
      <td>22</td>
      <td>816</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">14</th>
      <th>Tissue</th>
      <td>310</td>
      <td>102</td>
      <td>281</td>
      <td>305</td>
      <td>1307</td>
    </tr>
    <tr>
      <th>Stool</th>
      <td>204</td>
      <td>33</td>
      <td>2377</td>
      <td>32</td>
      <td>53</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df_b = df_b.reset_index()
df_b
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Taxon</th>
      <th>Patient</th>
      <th>level_1</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Tissue</td>
      <td>1590</td>
      <td>67</td>
      <td>136</td>
      <td>195</td>
      <td>2469</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Stool</td>
      <td>4</td>
      <td>0</td>
      <td>4182</td>
      <td>18</td>
      <td>1821</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Tissue</td>
      <td>25</td>
      <td>0</td>
      <td>1174</td>
      <td>42</td>
      <td>839</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>Stool</td>
      <td>2</td>
      <td>0</td>
      <td>703</td>
      <td>2</td>
      <td>661</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Tissue</td>
      <td>259</td>
      <td>85</td>
      <td>408</td>
      <td>316</td>
      <td>4414</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>Stool</td>
      <td>300</td>
      <td>5</td>
      <td>3946</td>
      <td>43</td>
      <td>18</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4</td>
      <td>Tissue</td>
      <td>568</td>
      <td>143</td>
      <td>831</td>
      <td>202</td>
      <td>12044</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4</td>
      <td>Stool</td>
      <td>7</td>
      <td>7</td>
      <td>8605</td>
      <td>40</td>
      <td>83</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5</td>
      <td>Tissue</td>
      <td>1102</td>
      <td>678</td>
      <td>693</td>
      <td>116</td>
      <td>2310</td>
    </tr>
    <tr>
      <th>9</th>
      <td>5</td>
      <td>Stool</td>
      <td>9</td>
      <td>2</td>
      <td>50</td>
      <td>0</td>
      <td>12</td>
    </tr>
    <tr>
      <th>10</th>
      <td>6</td>
      <td>Tissue</td>
      <td>678</td>
      <td>4829</td>
      <td>718</td>
      <td>527</td>
      <td>3053</td>
    </tr>
    <tr>
      <th>11</th>
      <td>6</td>
      <td>Stool</td>
      <td>377</td>
      <td>209</td>
      <td>717</td>
      <td>12</td>
      <td>547</td>
    </tr>
    <tr>
      <th>12</th>
      <td>7</td>
      <td>Tissue</td>
      <td>260</td>
      <td>74</td>
      <td>173</td>
      <td>357</td>
      <td>395</td>
    </tr>
    <tr>
      <th>13</th>
      <td>7</td>
      <td>Stool</td>
      <td>58</td>
      <td>651</td>
      <td>33</td>
      <td>11</td>
      <td>2174</td>
    </tr>
    <tr>
      <th>14</th>
      <td>8</td>
      <td>Tissue</td>
      <td>424</td>
      <td>169</td>
      <td>228</td>
      <td>106</td>
      <td>2651</td>
    </tr>
    <tr>
      <th>15</th>
      <td>8</td>
      <td>Stool</td>
      <td>233</td>
      <td>254</td>
      <td>80</td>
      <td>11</td>
      <td>767</td>
    </tr>
    <tr>
      <th>16</th>
      <td>9</td>
      <td>Tissue</td>
      <td>548</td>
      <td>106</td>
      <td>162</td>
      <td>67</td>
      <td>1195</td>
    </tr>
    <tr>
      <th>17</th>
      <td>9</td>
      <td>Stool</td>
      <td>21</td>
      <td>10</td>
      <td>3196</td>
      <td>14</td>
      <td>76</td>
    </tr>
    <tr>
      <th>18</th>
      <td>10</td>
      <td>Tissue</td>
      <td>201</td>
      <td>73</td>
      <td>372</td>
      <td>203</td>
      <td>6857</td>
    </tr>
    <tr>
      <th>19</th>
      <td>10</td>
      <td>Stool</td>
      <td>83</td>
      <td>381</td>
      <td>32</td>
      <td>6</td>
      <td>795</td>
    </tr>
    <tr>
      <th>20</th>
      <td>11</td>
      <td>Tissue</td>
      <td>42</td>
      <td>30</td>
      <td>4255</td>
      <td>392</td>
      <td>483</td>
    </tr>
    <tr>
      <th>21</th>
      <td>11</td>
      <td>Stool</td>
      <td>75</td>
      <td>359</td>
      <td>4361</td>
      <td>6</td>
      <td>666</td>
    </tr>
    <tr>
      <th>22</th>
      <td>12</td>
      <td>Tissue</td>
      <td>109</td>
      <td>51</td>
      <td>107</td>
      <td>28</td>
      <td>2950</td>
    </tr>
    <tr>
      <th>23</th>
      <td>12</td>
      <td>Stool</td>
      <td>59</td>
      <td>51</td>
      <td>1667</td>
      <td>25</td>
      <td>3994</td>
    </tr>
    <tr>
      <th>24</th>
      <td>13</td>
      <td>Tissue</td>
      <td>51</td>
      <td>2473</td>
      <td>96</td>
      <td>12</td>
      <td>1541</td>
    </tr>
    <tr>
      <th>25</th>
      <td>13</td>
      <td>Stool</td>
      <td>183</td>
      <td>2314</td>
      <td>223</td>
      <td>22</td>
      <td>816</td>
    </tr>
    <tr>
      <th>26</th>
      <td>14</td>
      <td>Tissue</td>
      <td>310</td>
      <td>102</td>
      <td>281</td>
      <td>305</td>
      <td>1307</td>
    </tr>
    <tr>
      <th>27</th>
      <td>14</td>
      <td>Stool</td>
      <td>204</td>
      <td>33</td>
      <td>2377</td>
      <td>32</td>
      <td>53</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df_b[&#39;Location&#39;] = np.where(df_b.level_1 == &#39;Tissue&#39;,1,0)
df_b
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Taxon</th>
      <th>Patient</th>
      <th>level_1</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
      <th>Location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Tissue</td>
      <td>1590</td>
      <td>67</td>
      <td>136</td>
      <td>195</td>
      <td>2469</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Stool</td>
      <td>4</td>
      <td>0</td>
      <td>4182</td>
      <td>18</td>
      <td>1821</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Tissue</td>
      <td>25</td>
      <td>0</td>
      <td>1174</td>
      <td>42</td>
      <td>839</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>Stool</td>
      <td>2</td>
      <td>0</td>
      <td>703</td>
      <td>2</td>
      <td>661</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Tissue</td>
      <td>259</td>
      <td>85</td>
      <td>408</td>
      <td>316</td>
      <td>4414</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>Stool</td>
      <td>300</td>
      <td>5</td>
      <td>3946</td>
      <td>43</td>
      <td>18</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4</td>
      <td>Tissue</td>
      <td>568</td>
      <td>143</td>
      <td>831</td>
      <td>202</td>
      <td>12044</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4</td>
      <td>Stool</td>
      <td>7</td>
      <td>7</td>
      <td>8605</td>
      <td>40</td>
      <td>83</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5</td>
      <td>Tissue</td>
      <td>1102</td>
      <td>678</td>
      <td>693</td>
      <td>116</td>
      <td>2310</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>5</td>
      <td>Stool</td>
      <td>9</td>
      <td>2</td>
      <td>50</td>
      <td>0</td>
      <td>12</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>6</td>
      <td>Tissue</td>
      <td>678</td>
      <td>4829</td>
      <td>718</td>
      <td>527</td>
      <td>3053</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>6</td>
      <td>Stool</td>
      <td>377</td>
      <td>209</td>
      <td>717</td>
      <td>12</td>
      <td>547</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>7</td>
      <td>Tissue</td>
      <td>260</td>
      <td>74</td>
      <td>173</td>
      <td>357</td>
      <td>395</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>7</td>
      <td>Stool</td>
      <td>58</td>
      <td>651</td>
      <td>33</td>
      <td>11</td>
      <td>2174</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>8</td>
      <td>Tissue</td>
      <td>424</td>
      <td>169</td>
      <td>228</td>
      <td>106</td>
      <td>2651</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>8</td>
      <td>Stool</td>
      <td>233</td>
      <td>254</td>
      <td>80</td>
      <td>11</td>
      <td>767</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>9</td>
      <td>Tissue</td>
      <td>548</td>
      <td>106</td>
      <td>162</td>
      <td>67</td>
      <td>1195</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>9</td>
      <td>Stool</td>
      <td>21</td>
      <td>10</td>
      <td>3196</td>
      <td>14</td>
      <td>76</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>10</td>
      <td>Tissue</td>
      <td>201</td>
      <td>73</td>
      <td>372</td>
      <td>203</td>
      <td>6857</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19</th>
      <td>10</td>
      <td>Stool</td>
      <td>83</td>
      <td>381</td>
      <td>32</td>
      <td>6</td>
      <td>795</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>11</td>
      <td>Tissue</td>
      <td>42</td>
      <td>30</td>
      <td>4255</td>
      <td>392</td>
      <td>483</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>11</td>
      <td>Stool</td>
      <td>75</td>
      <td>359</td>
      <td>4361</td>
      <td>6</td>
      <td>666</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>12</td>
      <td>Tissue</td>
      <td>109</td>
      <td>51</td>
      <td>107</td>
      <td>28</td>
      <td>2950</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>12</td>
      <td>Stool</td>
      <td>59</td>
      <td>51</td>
      <td>1667</td>
      <td>25</td>
      <td>3994</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>13</td>
      <td>Tissue</td>
      <td>51</td>
      <td>2473</td>
      <td>96</td>
      <td>12</td>
      <td>1541</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>13</td>
      <td>Stool</td>
      <td>183</td>
      <td>2314</td>
      <td>223</td>
      <td>22</td>
      <td>816</td>
      <td>0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>14</td>
      <td>Tissue</td>
      <td>310</td>
      <td>102</td>
      <td>281</td>
      <td>305</td>
      <td>1307</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>14</td>
      <td>Stool</td>
      <td>204</td>
      <td>33</td>
      <td>2377</td>
      <td>32</td>
      <td>53</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre>df_b = df_b.drop([&#39;level_1&#39;, &#39;Patient&#39;], axis=1)
df_b
</pre></div>


<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Taxon</th>
      <th>Actinobacteria</th>
      <th>Bacteroidetes</th>
      <th>Firmicutes</th>
      <th>Other</th>
      <th>Proteobacteria</th>
      <th>Location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1590</td>
      <td>67</td>
      <td>136</td>
      <td>195</td>
      <td>2469</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>0</td>
      <td>4182</td>
      <td>18</td>
      <td>1821</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>25</td>
      <td>0</td>
      <td>1174</td>
      <td>42</td>
      <td>839</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>703</td>
      <td>2</td>
      <td>661</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>259</td>
      <td>85</td>
      <td>408</td>
      <td>316</td>
      <td>4414</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>300</td>
      <td>5</td>
      <td>3946</td>
      <td>43</td>
      <td>18</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>568</td>
      <td>143</td>
      <td>831</td>
      <td>202</td>
      <td>12044</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>7</td>
      <td>8605</td>
      <td>40</td>
      <td>83</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1102</td>
      <td>678</td>
      <td>693</td>
      <td>116</td>
      <td>2310</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9</td>
      <td>2</td>
      <td>50</td>
      <td>0</td>
      <td>12</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>678</td>
      <td>4829</td>
      <td>718</td>
      <td>527</td>
      <td>3053</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>377</td>
      <td>209</td>
      <td>717</td>
      <td>12</td>
      <td>547</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>260</td>
      <td>74</td>
      <td>173</td>
      <td>357</td>
      <td>395</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>58</td>
      <td>651</td>
      <td>33</td>
      <td>11</td>
      <td>2174</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>424</td>
      <td>169</td>
      <td>228</td>
      <td>106</td>
      <td>2651</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>233</td>
      <td>254</td>
      <td>80</td>
      <td>11</td>
      <td>767</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>548</td>
      <td>106</td>
      <td>162</td>
      <td>67</td>
      <td>1195</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>21</td>
      <td>10</td>
      <td>3196</td>
      <td>14</td>
      <td>76</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>201</td>
      <td>73</td>
      <td>372</td>
      <td>203</td>
      <td>6857</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19</th>
      <td>83</td>
      <td>381</td>
      <td>32</td>
      <td>6</td>
      <td>795</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>42</td>
      <td>30</td>
      <td>4255</td>
      <td>392</td>
      <td>483</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>75</td>
      <td>359</td>
      <td>4361</td>
      <td>6</td>
      <td>666</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>109</td>
      <td>51</td>
      <td>107</td>
      <td>28</td>
      <td>2950</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>59</td>
      <td>51</td>
      <td>1667</td>
      <td>25</td>
      <td>3994</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>51</td>
      <td>2473</td>
      <td>96</td>
      <td>12</td>
      <td>1541</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>183</td>
      <td>2314</td>
      <td>223</td>
      <td>22</td>
      <td>816</td>
      <td>0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>310</td>
      <td>102</td>
      <td>281</td>
      <td>305</td>
      <td>1307</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>204</td>
      <td>33</td>
      <td>2377</td>
      <td>32</td>
      <td>53</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>And now we have our data!!!!  We can split into a train test set and see how it performs.</p>
<div class="highlight"><pre>y = df_b[&#39;Location&#39;].values
x = df_b.drop(&#39;Location&#39;, axis=1).values
x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=.2)
svc_lin = LinearSVC()
parameters = {&#39;C&#39;:np.logspace(-3,3,1000)}
clf = GridSearchCV(svc_lin, parameters, scoring=&#39;accuracy&#39;, cv=10)
clf.fit(x_train,y_train)
print &quot;Best Accuracy:&quot;, clf.best_score_
print &quot;Test Accuracy:&quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.863636363636
Test Accuracy: 0.166666666667
</pre></div>


<p>Our linear SVC did not generalize to the test set at all.  Lets try a different model!</p>
<div class="highlight"><pre>svc_lin = SVC(kernel=&#39;rbf&#39;)
parameters = {&#39;C&#39;:np.logspace(-3,3,100),&#39;gamma&#39;:np.logspace(-8,2,11)}
clf = GridSearchCV(svc_lin, parameters, scoring=&#39;accuracy&#39;, cv=10)
clf.fit(x_train,y_train)
print &quot;Best Accuracy:&quot;, clf.best_score_
print &quot;Test Accuracy:&quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.818181818182
Test Accuracy: 0.666666666667
</pre></div>


<p>The RBF kernal generalized much better to the test set.  We are doing better then guessing, but I am wondering if a logistic regressor will do better.</p>
<div class="highlight"><pre>lin = LogisticRegression()
parameters = {&#39;C&#39;:np.logspace(-3,3,1000)}
clf = GridSearchCV(lin, parameters, scoring=&#39;accuracy&#39;, cv=10)
clf.fit(x_train,y_train)
print &quot;Best Accuracy:&quot;, clf.best_score_
print &quot;Test Accuracy:&quot;, accuracy_score(y_test,clf.best_estimator_.predict(x_test))

Best Accuracy: 0.818181818182
Test Accuracy: 0.333333333333
</pre></div>


<p>This is not the best model by far.   Before we come to a close, we will see if the SVC does better at predict than a random forest.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;n_estimators&#39;</span><span class="p">:[</span><span class="mi">20</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">]}</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Best Accuracy:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span>
<span class="k">print</span> <span class="s">&quot;Test Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>

<span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.818181818182</span>
<span class="n">Test</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.833333333333</span>
</pre></div>


<p>And as expected, the random forest goes for the best generalization.   Of course, that what ensemble methods are suppose to do.  We're suppose to learn about them in depth tomorrow.  Until then....</p>
    </div>
  </div>
  <hr class="separator">
  <div class="col-md-8 col-md-offset-2">
  <div id="disqus_thread">
    <script>
      var disqus_shortname = 'bryansmithphd';
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] ||
         document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript=bryansmithphd">
        comments powered by Disqus.
      </a>
    </noscript>
    <a href="https://disqus.com" class="dsq-brlink">
      blog comments powered by <span class="logo-disqus">Disqus</span>
    </a>
  </div>
  </div>
  </div>
<footer class="footer">
  <div class="container">
    <p class="text-center">
      Bryan Smith, <a href="" target="_blank"></a> unless otherwise noted.
    </p>
    <div class="text-center">
      Generated by <a href="http://getpelican.com" target="_blank">Pelican</a> with the <a href="http://github.com/nairobilug/pelican-alchemy">alchemy</a> theme.
    </div>
  </div>
</footer> <!-- /.footer -->
  <script src="http://www.bryantravissmith.com/theme/js/jquery.min.js"></script>
  <script src="http://www.bryantravissmith.com/theme/js/bootstrap.min.js"></script>
</body> <!-- 42 -->
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$$','$$'], ['\\(','\\)']]}
});
</script>
</html>