<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="http://www.bryantravissmith.com/galvanize/galvanize-data-science-03-03/index.html" />

    <title>  Bryan Travis Smith, Ph.D &mdash; Galvanize - Week 03 - Day 3
</title>




    <link rel="stylesheet" href="http://www.bryantravissmith.com/theme/css/style.css">

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-24340005-3', 'auto');
    ga('send', 'pageview');

  </script>

    <meta name="author" content="Bryan Smith">
    <meta name="description" content="The thirteenth day of Galvanize's Immersive Data Science program in San Francisco, CA where we covered lasso and ridge regress.">
  <meta name="tags" contents="data-science, galvanize, Lasso Regresion, Ridge Regression, Regularization, ">
</head>

<body>
<header class="header">
  <div class="container">
      <div class="header-image pull-left">
        <a class="nodec" href="http://www.bryantravissmith.com"><img src=http://www.bryantravissmith.com/img/bryan.jpeg></a>
      </div>
    <div class="header-inner">
      <h1 class="header-name">
        <a class="nodec" href="http://www.bryantravissmith.com">Bryan Travis Smith, Ph.D</a>
      </h1>
      <h3 class="header-text">Physicist, Data Scientist, Martial Artist, & Life Enthusiast</h3>
      <ul class="header-menu list-inline">
              <li class="muted">|</li>
            <li><a class="nodec" href="http://www.bryantravissmith.com/about/">About</a></li>
              <li class="muted">|</li>
          <li><a class="nodec icon-mail-alt" href="mailto:bryantravissmith@gmail.com"></a></li>
          <li><a class="nodec icon-github" href="https://github.com/bryantravissmith"></a></li>
      </ul>
    </div>
  </div>
</header> <!-- /.header -->  <div class="container">
  <div class="post full-post">
    <h1 class="post-title">
      <a href="/galvanize/galvanize-data-science-03-03/" title="Permalink to Galvanize - Week 03 - Day 3">Galvanize - Week 03 - Day 3</a>
    </h1>
    <ul class="list-inline">
      <li class="post-date">
        <a class="text-muted" href="/galvanize/galvanize-data-science-03-03/" title="2015-06-17T10:20:00-07:00">Wed 17 June 2015</a>
      </li>
      <li class="muted">&middot;</li>
      <li class="post-category">
        <a href="http://www.bryantravissmith.com/category/galvanize.html">Galvanize</a>
      </li>
        <li class="muted">&middot;</li>
        <li>
          <address class="post-author">
            By <a href="http://www.bryantravissmith.com/author/bryan-smith.html">Bryan Smith</a>
          </address>
        </li>
    </ul>
    <div class="post-content">
      <h1>Galvanize Immersive Data Science</h1>
<h2>Week 3 - Day 3</h2>
<p>Today we had a miniquiz on cleaning data.  Identify missing values, incorrect format, and outliers.   It was a straight forward assignment.   I was struck with there is no clear end to data cleaning without a purpose.   It would be nice ot set a stoping criteria or variable specification so there are clear evaluation criteria.  Obviously with raw data the data has to be explored, and these criteria does not exist.   </p>
<h2>Morning: One-fold Cross Validation</h2>
<p>This morning we started using cross validation on the boston data set from sklearn.  The goal is not to make the best model, but just work through th eprocess of using cross validation. </p>
<p>Descriptions for each column in <code>features</code>:</p>
<p><code>Attribute Information (in order):
    - CRIM     per capita crime rate by town
    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
    - INDUS    proportion of non-retail business acres per town
    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    - NOX      nitric oxides concentration (parts per 10 million)
    - RM       average number of rooms per dwelling
    - AGE      proportion of owner-occupied units built prior to 1940
    - DIS      weighted distances to five Boston employment centres
    - RAD      index of accessibility to radial highways
    - TAX      full-value property-tax rate per $10,000
    - PTRATIO  pupil-teacher ratio by town
    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    - LSTAT    % lower status of the population
    - MEDV     Median value of owner-occupied homes in $1000's</code></p>
<div class="highlight"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">sc</span>
<span class="kn">from</span> <span class="nn">pandas.tools.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span> <span class="c"># housing price</span>
</pre></div>


<p>We are using the train, test split to make a training set of 70% of the data and a test set of 30% of the data.  </p>
<div class="highlight"><pre>train_feature, test_feature, train_target, test_target = train_test_split(features, target, test_size=0.3)
</pre></div>


<p>We are going to use sklearn's LinearRegresion on this data set.  There are likely issues with multicolinearity, but we will skip that for now.   We will train the data on the training set, and test it on the testing set.  </p>
<div class="highlight"><pre>linear = LinearRegression()
linear.fit(train_feature, train_target)
# You can call predict to get the predicted values for training and test
train_predicted = linear.predict(train_feature)
test_predicted = linear.predict(test_feature)
</pre></div>


<p>I wrote a mean square function to compare to sklearn's function.   I expect the test set to have larger value then the training set because the model was not trained on the test set.  </p>
<div class="highlight"><pre>def MSE(x1,x2):
    return np.sum(np.power(x2-x1,2))/len(x1)
print &quot;Train MSE: &quot;, MSE(train_predicted,train_target),mean_squared_error(train_predicted,train_target)
print &quot;Test MSE: &quot;, MSE(test_predicted,test_target), mean_squared_error(test_predicted,test_target)


Train MSE:  21.1395296142 21.1395296142
Test MSE:  24.6555052851 24.6555052851
</pre></div>


<h2>K-fold Cross Validation</h2>
<p>In K-fold cross validation the data is split into <strong>k</strong> groups. One group
out of the k groups will be the test set, the rest (<strong>k-1</strong>) groups will
be the training set. In the next iteration, another group will be the test set,
and the rest will be the training set. The process repeats for k iterations (k-fold).
In each fold, a metric for accuracy (MSE in this case) will be calculated and
an overall average of that metric will be calculated over k-folds.</p>
<ol>
<li>
<p>To do this we need to manage randomly sampling <strong>k</strong> folds.</p>
</li>
<li>
<p>Properly combining those <strong>k</strong> folds into a test and training set on
   your <strong>on the training dataset</strong>. Outside of the k-fold, there should be
   another set which will be referred to as the <strong>hold-out set</strong>.</p>
</li>
<li>
<p>Train your model on your constructed training set and evaluate on the given test set</p>
</li>
<li>
<p>Repeat steps <strong>2</strong> and <strong>3</strong> <em>k</em> times.</p>
</li>
<li>
<p>Average your results of your error metric.</p>
</li>
<li>
<p>Compare the MSE for your test set in Part 1. and your K-fold cross validated error in <code>4.</code>.</p>
<p>indexes = np.arange(len(train_feature))
indexes2 = indexes.copy()
linear = LinearRegression()
count = len(indexes)
mse = []
k = 10.
for i in range(int(k)):
    choices = np.random.choice(indexes2,int(count/k),replace=False)
    indexes2 = np.setdiff1d(indexes2,choices)
    train_index = np.setdiff1d(indexes,choices)
    linear.fit(train_feature[train_index], train_target[train_index])
    test_predicted = linear.predict(train_feature[choices])
    mse.append(mean_squared_error(train_target[choices],test_predicted))</p>
<p>print "Avg MSE: ",sum(mse)/k</p>
<p>Avg MSE:  23.9048376671</p>
<p>def scorer(model,X,y):
    return mean_squared_error(y,model.predict(X))</p>
<p>cross_val_score(LinearRegression(),train_feature,train_target,scoring=scorer,cv=10).mean()</p>
<p>24.72849552718905</p>
</li>
</ol>
<p>I made my own k-fold validation and compared it to the sklearn method.  There is an obviously efficiency in code.  I did my own wrapper for the scoring because the string method sometimes returned negative values.   </p>
<p>We can look at the comparison between traiing and cross validaton as we increase the sample size.  </p>
<div class="highlight"><pre>sample_sizes = np.arange(10,350,10)
train_mse = []
cross_mse = []
for size in sample_sizes:
    linear = LinearRegression()

    linear.fit(train_feature[:size],train_target[:size])
    train_predicted = linear.predict(train_feature[:size])
    train_mse.append(mean_squared_error(train_predicted[:size],train_target[:size]))
    cross_mse.append(cross_val_score(linear,train_feature[:size],train_target[:size],scoring=scorer,cv=5).mean())

print len(sample_sizes),len(train_mse)
plt.figure(figsize=(10,8))
plt.plot(sample_sizes,train_mse,color=&#39;seagreen&#39;,lw=2,alpha=0.8,label=&#39;Train Set MSE&#39;)
plt.plot(sample_sizes,cross_mse,color=&#39;steelblue&#39;,lw=2,alpha=0.8,label=&#39;CV Set MSE&#39;)
plt.legend()
plt.xlabel(&quot;Sample Size&quot;)
plt.ylabel(&quot;MSE&quot;)
plt.ylim([0,100])
plt.show()

34 34
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_13_1.png" /></p>
<p>In this case we see that the two values start to converge, but the CV is higher.  This make sense because it is hold out values to test on.   It should be a better approximation to applying it to a test set. </p>
<p>We can look at the difference between the test and training sets as we increase the training size.</p>
<div class="highlight"><pre>sample_sizes = np.arange(10,350,10)
test_mse = []
train_mse = []
for size in sample_sizes:
    linear = LinearRegression()
    linear.fit(train_feature[:size],train_target[:size])
    train_predicted = linear.predict(train_feature)
    test_predicted = linear.predict(test_feature)
    train_mse.append(mean_squared_error(train_predicted,train_target))
    test_mse.append(mean_squared_error(test_predicted,test_target))

plt.figure(figsize=(10,8))
plt.plot(sample_sizes,train_mse,color=&#39;seagreen&#39;,lw=2,alpha=0.8,label=&#39;Train Set MSE&#39;)
plt.plot(sample_sizes,test_mse,color=&#39;steelblue&#39;,lw=2,alpha=0.8,label=&#39;Test Set MSE&#39;)
plt.legend()
plt.xlabel(&quot;Sample Size&quot;)
plt.ylabel(&quot;MSE&quot;)
plt.ylim([0,100])
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_15_0.png" /></p>
<div class="highlight"><pre>test_mse = []
train_mse = []
num_feat = range(1,len(train_feature[0,:]))
for i in num_feat:
    linear = LinearRegression()
    linear.fit(train_feature[:,:i],train_target)
    train_predicted = linear.predict(train_feature[:,:i])
    test_predicted = linear.predict(test_feature[:,:i])
    train_mse.append(mean_squared_error(train_predicted,train_target))
    test_mse.append(mean_squared_error(test_predicted,test_target))

plt.figure(figsize=(10,8))
plt.plot(num_feat,train_mse,color=&#39;seagreen&#39;,lw=2,alpha=0.8,label=&#39;Train Set MSE&#39;)
plt.plot(num_feat,test_mse,color=&#39;steelblue&#39;,lw=2,alpha=0.8,label=&#39;Test Set MSE&#39;)
plt.legend()
plt.xlabel(&quot;Number Features&quot;)
plt.ylabel(&quot;MSE&quot;)
plt.ylim([0,100])
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_16_0.png" /></p>
<p>As we increase features and data, our results on the training set go down and the test set values, in this case, look to converge.   </p>
<h2>Stepwise Regression</h2>
<p>While stepwise regression has its many <a href="http://andrewgelman.com/2014/06/02/hate-stepwise-regression/">critics</a>, it is a useful exercise to introduce the concept of feature selection in the context of linear regression. This extra credit exercise has two components of different difficulties. First, use the <code>scikit-learn</code> reverse feature elimation (a greedy feature elimination algorithm) to implement something similar to sequential backward selection. The second, more difficult part is implementing sequential forward selection.</p>
<p>We generate a random dataset that has 100 features, but only 5 of them influence the response variable.  We can use sklearn's RFE to fit and attempt to rank the features.  </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">ranking_</span>




<span class="n">array</span><span class="p">([</span>  <span class="mi">3</span><span class="p">,</span>   <span class="mi">2</span><span class="p">,</span>  <span class="mi">71</span><span class="p">,</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">4</span><span class="p">,</span>  <span class="mi">47</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>  <span class="mi">36</span><span class="p">,</span>  <span class="mi">94</span><span class="p">,</span>  <span class="mi">61</span><span class="p">,</span>  <span class="mi">14</span><span class="p">,</span>  <span class="mi">39</span><span class="p">,</span>  <span class="mi">24</span><span class="p">,</span>
        <span class="mi">58</span><span class="p">,</span>  <span class="mi">20</span><span class="p">,</span>  <span class="mi">75</span><span class="p">,</span>  <span class="mi">98</span><span class="p">,</span>  <span class="mi">79</span><span class="p">,</span>  <span class="mi">41</span><span class="p">,</span>  <span class="mi">32</span><span class="p">,</span>  <span class="mi">78</span><span class="p">,</span>  <span class="mi">54</span><span class="p">,</span>  <span class="mi">76</span><span class="p">,</span>  <span class="mi">86</span><span class="p">,</span>  <span class="mi">55</span><span class="p">,</span>  <span class="mi">43</span><span class="p">,</span>
        <span class="mi">29</span><span class="p">,</span>  <span class="mi">66</span><span class="p">,</span>  <span class="mi">35</span><span class="p">,</span>  <span class="mi">68</span><span class="p">,</span>  <span class="mi">37</span><span class="p">,</span>   <span class="mi">8</span><span class="p">,</span>  <span class="mi">92</span><span class="p">,</span>  <span class="mi">51</span><span class="p">,</span>  <span class="mi">13</span><span class="p">,</span>  <span class="mi">89</span><span class="p">,</span>  <span class="mi">46</span><span class="p">,</span>  <span class="mi">82</span><span class="p">,</span>  <span class="mi">83</span><span class="p">,</span>
        <span class="mi">30</span><span class="p">,</span>  <span class="mi">96</span><span class="p">,</span>  <span class="mi">77</span><span class="p">,</span>  <span class="mi">31</span><span class="p">,</span>  <span class="mi">72</span><span class="p">,</span>  <span class="mi">90</span><span class="p">,</span>  <span class="mi">62</span><span class="p">,</span>  <span class="mi">67</span><span class="p">,</span>  <span class="mi">59</span><span class="p">,</span>  <span class="mi">73</span><span class="p">,</span>  <span class="mi">15</span><span class="p">,</span>  <span class="mi">11</span><span class="p">,</span>  <span class="mi">85</span><span class="p">,</span>
        <span class="mi">91</span><span class="p">,</span>  <span class="mi">17</span><span class="p">,</span>  <span class="mi">88</span><span class="p">,</span>  <span class="mi">26</span><span class="p">,</span>  <span class="mi">56</span><span class="p">,</span>  <span class="mi">28</span><span class="p">,</span>  <span class="mi">33</span><span class="p">,</span>  <span class="mi">53</span><span class="p">,</span>  <span class="mi">18</span><span class="p">,</span>  <span class="mi">10</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>   <span class="mi">6</span><span class="p">,</span>  <span class="mi">38</span><span class="p">,</span>
        <span class="mi">48</span><span class="p">,</span>  <span class="mi">50</span><span class="p">,</span>  <span class="mi">19</span><span class="p">,</span>  <span class="mi">84</span><span class="p">,</span>  <span class="mi">34</span><span class="p">,</span>  <span class="mi">93</span><span class="p">,</span>  <span class="mi">23</span><span class="p">,</span>  <span class="mi">27</span><span class="p">,</span>  <span class="mi">63</span><span class="p">,</span>  <span class="mi">70</span><span class="p">,</span>  <span class="mi">16</span><span class="p">,</span>  <span class="mi">12</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span>
        <span class="mi">44</span><span class="p">,</span>  <span class="mi">80</span><span class="p">,</span>  <span class="mi">45</span><span class="p">,</span>  <span class="mi">97</span><span class="p">,</span>  <span class="mi">87</span><span class="p">,</span>   <span class="mi">9</span><span class="p">,</span>  <span class="mi">21</span><span class="p">,</span>   <span class="mi">5</span><span class="p">,</span>  <span class="mi">52</span><span class="p">,</span>  <span class="mi">60</span><span class="p">,</span>  <span class="mi">57</span><span class="p">,</span>  <span class="mi">49</span><span class="p">,</span>  <span class="mi">25</span><span class="p">,</span>
        <span class="mi">40</span><span class="p">,</span>  <span class="mi">95</span><span class="p">,</span>  <span class="mi">81</span><span class="p">,</span>  <span class="mi">42</span><span class="p">,</span>  <span class="mi">99</span><span class="p">,</span>  <span class="mi">74</span><span class="p">,</span>  <span class="mi">65</span><span class="p">,</span>  <span class="mi">69</span><span class="p">,</span>  <span class="mi">64</span><span class="p">])</span>
</pre></div>


<p>We are going to loop through these variables and use them as a fit.   We want to see if a there is a point where adding more features will not improve the model.  </p>
<div class="highlight"><pre>linear = LinearRegression()
rsq = []
for i in range(2,50):
    linear.fit(X[:,rfe.ranking_ &lt; i],y)
    rsq.append(linear.score(X[:,rfe.ranking_ &lt; i],y))
plt.plot(range(2,50),rsq,lw=3,color=&#39;seagreen&#39;,alpha=0.5,label=&#39;R-Square&#39;)
plt.xlabel(&quot;Number of Variables&quot;)
plt.ylabel(&#39;R-Squared&#39;)
plt.ylim([0,1])
plt.axvline(x=5,linestyle=&#39;--&#39;,color=&#39;gray&#39;)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_21_0.png" /></p>
<p>Instead of using RFE to do backward selection, we can create a class that implements sequential forward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model comparison criterion, adding the variable (if any) that improves the model the most, and repeating this process until none improves the model.</p>
<h4>Reference</h4>
<ul>
<li>
<p><a href="https://onlinecourses.science.psu.edu/stat501/node/88">Stepwise Regression Procedure</a></p>
<p>class ForwardRegression:</p>
<div class="highlight"><pre>def __init__(self,X,y):
    self.nFeatures = X.shape[1]
    self.nRow = X.shape[0]
    self.X = X
    self.y = y
    self.linear = LinearRegression()
    self.scores = []
    self.best_features = []
    self.best_model = False

def next_best(self):
    if not self.best_model:
        max_rsq = 0
        best_index = -1
        for i in range(self.nFeatures):
            if i not in self.best_features:
                self.linear.fit(self.X[:,(self.best_features+[i])],self.y)
                score = self.linear.score(self.X[:,(self.best_features+[i])],self.y)
                if score &gt; max_rsq:
                    max_rsq = score
                    best_index = i
        if best_index != -1:
            if len(self.scores) &gt; 1:
                if (max_rsq-self.scores[-1])/self.scores[-1] &gt; 0.01:
                    self.best_features.append(best_index)
                    self.scores.append(max_rsq)
                else:
                    self.best_model = True

            else:
                self.best_features.append(best_index)
                self.scores.append(max_rsq)
</pre></div>


<p>fr = ForwardRegression(X,y)
for i in range(5):
    fr.next_best()
print fr.scores
print fr.best_features
print fr.best_model</p>
<p>[0.32517954427506357, 0.49706338477081047, 0.64962046580083821, 0.74271747755945094]
[3, 1, 0, 4]
True</p>
<p>rfe.ranking_</p>
<p>array([  3,   2,  71,   1,   4,  47,  22,  36,  94,  61,  14,  39,  24,
        58,  20,  75,  98,  79,  41,  32,  78,  54,  76,  86,  55,  43,
        29,  66,  35,  68,  37,   8,  92,  51,  13,  89,  46,  82,  83,
        30,  96,  77,  31,  72,  90,  62,  67,  59,  73,  15,  11,  85,
        91,  17,  88,  26,  56,  28,  33,  53,  18,  10,   7,   6,  38,
        48,  50,  19,  84,  34,  93,  23,  27,  63,  70,  16,  12, 100,
        44,  80,  45,  97,  87,   9,  21,   5,  52,  60,  57,  49,  25,
        40,  95,  81,  42,  99,  74,  65,  69,  64])</p>
</li>
</ul>
<p>This matches with the previous result for this example.</p>
<h2>Afternoon - Lasso and Ridge Regresson</h2>
<p>The goal fo this exercise is to get a feel for the shrinkage that these models do for coefficients.   We will start with the Ridge Regression in sklearn on the diabetes dataset.  We did a basic fit to start. </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span>

<span class="n">diabetes</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">150</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">150</span><span class="p">]</span>
<span class="n">XT</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">150</span><span class="p">:]</span>
<span class="n">yT</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">150</span><span class="p">:]</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;MSE: &quot;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>

<span class="n">MSE</span><span class="p">:</span>  <span class="mf">4251.12234379</span>
</pre></div>


<p>Now we will look at how the parameters change as we increasee alpha.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span> <span class="n">k</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alphas</span><span class="p">):</span>
    <span class="n">X_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">fit</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_28_0.png" /></p>
<p>Given a large enough alpha/lambda, the coefficients go to zero   This is a property of the model, and we want to try to use this paramter to find the best model.  </p>
<div class="highlight"><pre>k = X.shape[1]
alphas = np.logspace(-3,0,1000)
mse_train = []
mse_test = []
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Ridge(alpha=a, normalize=True).fit(X_data, y)
    mse_train.append(mean_squared_error(y,fit.predict(X_data)))
    mse_test.append(mean_squared_error(yT,fit.predict(scaler.transform(XT))))
plt.figure(figsize=(14,6))
plt.plot(alphas, mse_train)
plt.plot(alphas, mse_test,color=&#39;green&#39;)
plt.show()


print &quot;Best Alpha: &quot;, alphas[mse_test.index(min(mse_test))]
alpha_ridge = alphas[mse_test.index(min(mse_test))]
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_30_0.png" /></p>
<div class="highlight"><pre>Best Alpha:  0.252582002696
</pre></div>


<p>Now we are going to do the same for Lasso Regresion</p>
<div class="highlight"><pre>k = X.shape[1]
alphas = np.logspace(-2, 2)
params = np.zeros((len(alphas), k))
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Lasso(alpha=a, normalize=True).fit(X_data, y)
    params[i] = fit.coef_

plt.figure(figsize=(14,6))
for param in params.T:
    plt.plot(alphas, param)
plt.show()
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_32_0.png" /></p>
<p>The Lasso drops the paramters to zero much more quickly than the Ridge Method.  This is because of the absolut value in the prior.  </p>
<div class="highlight"><pre>k = X.shape[1]
alphas = np.logspace(-3,0,1000)
mse_train = []
mse_test = []
for i,a in enumerate(alphas):
    X_data = scaler.fit_transform(X)
    fit = Lasso(alpha=a, normalize=True).fit(X_data, y)
    mse_train.append(mean_squared_error(y,fit.predict(X_data)))
    mse_test.append(mean_squared_error(yT,fit.predict(scaler.transform(XT))))
plt.figure(figsize=(14,6))
plt.plot(alphas, mse_train)
plt.plot(alphas, mse_test,color=&#39;green&#39;)
plt.show()

print &quot;Best Alpha: &quot;, alphas[mse_test.index(min(mse_test))]
alpha_lasso = alphas[mse_test.index(min(mse_test))]
</pre></div>


<p><img alt="png" src="http://www.bryantravissmith.com/img/output_34_0.png" /></p>
<div class="highlight"><pre>Best Alpha:  0.286059553518
</pre></div>


<p>We are going to compare the 3 models.</p>
<div class="highlight"><pre>ridge = Ridge(alpha=alpha_ridge)
ridge.fit(X_data,y)

lasso = Lasso(alpha=alpha_lasso)
lasso.fit(X_data,y)

ols = LinearRegression()
ols.fit(X_data,y)

print &quot;Ridge MSE:&quot;,mean_squared_error(yT,ridge.predict(scaler.transform(XT)))
print &quot;Lasso MSE:&quot;,mean_squared_error(yT,lasso.predict(scaler.transform(XT)))
print &quot;OLS MSE:&quot;,mean_squared_error(yT,ols.predict(scaler.transform(XT)))
print &quot; &quot;
print &quot;Ridge,Lasso,OLS&quot; 
for i in range(len(ridge.coef_)):
    print round(ridge.coef_[i],2),round(lasso.coef_[i],2),round(ols.coef_[i],2)

Ridge MSE: 3184.60043415
Lasso MSE: 3160.04592755
OLS MSE: 3190.98282981

Ridge,Lasso,OLS
-3.28 -2.95 -3.29
-17.46 -17.04 -17.51
20.52 20.39 20.55
14.68 14.28 14.7
2.45 -0.0 3.85
-15.54 -12.27 -16.78
-12.38 -11.89 -13.07
5.94 4.43 5.84
26.36 27.43 25.93
4.47 4.34 4.44
</pre></div>


<p>The Lasso model on the lower MSE on the test set, and also dropped one of the variables in the model.   Over all its coefficients are lower, and this is consistent with what we saw in the previous plots.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="k">print</span> <span class="s">&quot;Ridge MSE:&quot;</span><span class="p">,</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yT</span><span class="p">,</span><span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">XT</span><span class="p">)))</span>
<span class="k">print</span> <span class="s">&quot;Lasso MSE:&quot;</span><span class="p">,</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yT</span><span class="p">,</span><span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">XT</span><span class="p">)))</span>
<span class="k">print</span> <span class="s">&quot;OLS MSE:&quot;</span><span class="p">,</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yT</span><span class="p">,</span><span class="n">ols</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">XT</span><span class="p">)))</span>

<span class="n">Ridge</span> <span class="n">MSE</span><span class="p">:</span> <span class="mf">0.473709418009</span>
<span class="n">Lasso</span> <span class="n">MSE</span><span class="p">:</span> <span class="mf">0.477767322867</span>
<span class="n">OLS</span> <span class="n">MSE</span><span class="p">:</span> <span class="mf">0.472654656259</span>
</pre></div>


<p>It is clear that this is far from the final story...</p>
    </div>
  </div>
  <hr class="separator">
  <div class="col-md-8 col-md-offset-2">
  <div id="disqus_thread">
    <script>
      var disqus_shortname = 'bryansmithphd';
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] ||
         document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript=bryansmithphd">
        comments powered by Disqus.
      </a>
    </noscript>
    <a href="https://disqus.com" class="dsq-brlink">
      blog comments powered by <span class="logo-disqus">Disqus</span>
    </a>
  </div>
  </div>
  </div>
<footer class="footer">
  <div class="container">
    <p class="text-center">
      Bryan Smith, <a href="" target="_blank"></a> unless otherwise noted.
    </p>
    <div class="text-center">
      Generated by <a href="http://getpelican.com" target="_blank">Pelican</a> with the <a href="http://github.com/nairobilug/pelican-alchemy">alchemy</a> theme.
    </div>
  </div>
</footer> <!-- /.footer -->
  <script src="http://www.bryantravissmith.com/theme/js/jquery.min.js"></script>
  <script src="http://www.bryantravissmith.com/theme/js/bootstrap.min.js"></script>
</body> <!-- 42 -->
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$$','$$'], ['\\(','\\)']]}
});
</script>
</html>