{
    "pages": [
        {
            "title": "Galvanize - Week 01 - Day 4", 
            "text":"Galvanize Immersive Data Science Week 1 - Day 4 The day started out with a mini-quiz on object-oriented programming, and that was followed by an introduction to git and sophisticated join queries. Our instructor for the day used to work at Facebook, and she walked us through some the queries she would do on the job. She then gave us a simulated data set that match the structure, but not the content, of Facebook tables and we had an individual sprint attempting to complete 10 queries in 2 hours. After lunch we had a lecture on pyscopg2, a python library to use to connect and interact with a PostgreSQL server. We ran a server locally, loaded with the same data as the morning, and were given an assignment to construct a pipeline that we could run each day to give us an updated status of our users. We were to check on results for today being set to Aug 14, 2014. Our resulting script is below: import psycopg2 from datetime import datetime conn = psycopg2.connect(dbname=&#39;socialmedia&#39;, user=&#39;postgres&#39;, password=&#39;password&#39;, host=&#39;localhost&#39;) c = conn.cursor() today = &#39;2014-08-14&#39; timestamp = datetime.strptime(today, &#39;%Y-%M-%d&#39;).strftime(&#34;%s&#34;) c.execute( &#39;&#39;&#39;CREATE TABLE logins_7d_%s AS WITH main AS ( SELECT r.userid, tmstmp::date AS reg_date, CASE WHEN optout.userid IS NULL then 0 ELSE 1 END AS opt_out FROM registrations r LEFT OUTER JOIN optout ON r.userid = optout.userid ORDER BY r.userid), last AS ( SELECT userid, MAX(tmstmp::date) AS last_login FROM logins GROUP BY userid ORDER BY userid), last7 AS ( SELECT t.userid, COUNT(t.dt) AS logins_7d FROM ( SELECT DISTINCT userid, tmstmp::date AS dt FROM logins WHERE logins.tmstmp &gt; timestamp &#39;2014-08-14&#39; - interval &#39;7 days&#39; GROUP BY userid, tmstmp::date ORDER BY userid) t GROUP BY t.userid), last7m AS ( SELECT t.userid, COUNT(t.dt) AS logins_7m FROM ( SELECT DISTINCT userid, tmstmp::date AS dt FROM logins WHERE logins.tmstmp &gt; timestamp &#39;2014-08-14&#39; - interval &#39;7 days&#39; AND logins.type = &#39;mobile&#39; GROUP BY userid, tmstmp::date ORDER BY userid) t GROUP BY t.userid), last7w AS ( SELECT t.userid, COUNT(t.dt) AS logins_7w FROM ( SELECT DISTINCT userid, tmstmp::date AS dt FROM logins WHERE logins.tmstmp &gt; timestamp &#39;2014-08-14&#39; - interval &#39;7 days&#39; AND logins.type = &#39;web&#39; GROUP BY userid, tmstmp::date ORDER BY userid) t GROUP BY t.userid), uf1 AS ( (SELECT * FROM friends) UNION ALL (SELECT userid2, userid1 FROM friends)), uf2 AS ( SELECT DISTINCT * FROM uf1), friend_cnt AS ( SELECT userid1 AS userid, COUNT(1) AS num_friends FROM uf2 GROUP BY userid) SELECT main.userid, reg_date, last_login, coalesce(logins_7d,0) AS logins_7d, coalesce(logins_7m,0) AS logins_7d_mobile, coalesce(logins_7w,0) AS logins_7d_web, coalesce(num_friends,0) AS num_friends, opt_out FROM main LEFT OUTER JOIN last ON main.userid = last.userid LEFT OUTER JOIN last7 ON main.userid = last7.userid LEFT OUTER JOIN last7m ON main.userid = last7m.userid LEFT OUTER JOIN last7w ON main.userid = last7w.userid LEFT OUTER JOIN friend_cnt ON main.userid = friend_cnt.userid;&#39;&#39;&#39; % timestamp ) conn.commit() conn.close() We also learned how pull data and load the data into a pandas dataframe. from pandas.io import sql from pandas.io.sql import read_sql conn = psycopg2.connect(dbname=&#39;socialmedia&#39;, user=&#39;postgres&#39;, password=&#39;password&#39;, host=&#39;localhost&#39;) sql = &#39;SELECT * FROM logins_7d_1389686880 LIMIT 20;&#39; df = read_sql(sql, conn, index_col=&#34;userid&#34;, coerce_float=True, params=None) conn.close() df reg_date last_login logins_7d logins_7d_mobile logins_7d_web num_friends opt_out userid 0 2014-06-23 2014-08-13 3 3 0 32 0 1 2013-12-21 2014-08-12 5 2 4 16 0 2 2014-04-18 2014-08-14 4 4 1 38 0 3 2013-12-17 2014-08-13 3 2 1 14 0 4 2014-08-11 2014-08-09 1 1 0 16 0 5 2013-08-31 2014-08-10 1 1 0 29 0 6 2013-08-18 2014-08-12 1 0 1 22 0 7 2014-03-21 2014-08-12 2 2 0 17 0 8 2014-05-03 2014-08-11 2 0 2 15 0 9 2014-06-06 2014-08-11 1 1 1 22 0 10 2013-08-31 2014-08-10 2 1 2 32 1 11 2013-08-16 2014-08-10 2 1 1 24 0 12 2013-09-12 2014-08-13 2 1 1 27 0 13 2014-07-29 2014-08-14 4 4 0 23 0 14 2013-11-03 2014-08-11 3 1 2 37 0 15 2013-10-09 2014-08-13 1 1 0 14 0 16 2014-02-16 2014-08-12 1 1 0 27 1 17 2014-04-20 2014-08-14 3 3 1 21 0 18 2014-07-02 2014-08-12 1 0 1 20 0 19 2014-08-14 2014-05-10 0 0 0 16 0 Today was a very intense day. But the programming is delivering on what it promised: Hands On Learning From Experience Professions!", 
            "tags": "Galvanize", 
            "loc": "http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-04/"
        },
        {
            "title": "Galvanize - Week 01 - Day 3", 
            "text":"Galvanize Immersive Data Science Week 1 - Day 3 Today was an &#39;introduction&#39; to SQL and PostgreSQL. I put introduction in quotes because it does not properly describe what we did. The pre-reading was to complete all 9 (1-9) tutorials on SQLZoo. This took me about 5 hours. During lecture we have a review of the order of operation of SQL queries, as well as a detailed explanation of joins. The sprint for the day involved install PostgreSQL locally, loading a database into it, then completing ~25 basic and 10 advance (extra credit) queries. Our database had 3 tables with 300k, 500k, and 5k entries respectively. These were a great set of assignment because of how they &#39;leveled-up&#39;. Even the few among us that were sophisticated with SQL had difficulty with the advance problems. Now that I have completed ~10 hours of SQL queries today, I am going to end this post now. SELECT * FROM bryan JOIN bed ON bryan.location=bed.location AND bryan.state=&#39;sleep&#39; AND bed.state=&#39;comfy&#39;", 
            "tags": "Galvanize", 
            "loc": "http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-03/"
        },
        {
            "title": "Galvanize - Week 01 - Day 2", 
            "text":"Galvanize Immersive Data Science Week 1 - Day 2 Today was he first &#39;regular&#39; day in the program. I showed up about 90 minutes before the mini-quiz to review the readings for the day&#39;s lecture on Object Oriented Programming (OOP). At 9:30 we started the mini-quiz on SQL statements and results. I found it rather simple. We were given 30 minutes to complete it, and I finished in about 10 minutes. Most the topics involved analogs in pandas that I am familiar with, so I think that&#39;s why I finished rather quickly. Lecture We had two lectures today. The first lecture was on object oriented structures, and how to implement them in python. The afternoon lecture was on scoping in python, and a little bit of debugging. We were introduced to pdb, but told that the use of debuggers is not well integrated in the data science community. LEGB We were told the variables are looked for in the order of local, enclosing function, global, and python build-in. I made a set of functions to try to illustrate it for myself. x = 5 def printer(): print x #globally finds x printer() def printer1(): x = -1 print x #locally finds x printer1() def printer2(): def innerprinter(): print x #globally finds x innerprinter() printer2() def printer3(): x=3 def innerprinter(): print x #encapsulating function finds x innerprinter() printer3() def printer4(): def innerprinter(): print int #built-in function finds int innerprinter() printer4() def printer5(): int = 3 def innerprinter(): print int #encapsulating function finds int innerprinter() printer5() int = 6 def printer6(): def innerprinter(): print int #globally finds int innerprinter() printer6() 5 -1 5 3 &lt;type &#39;int&#39;&gt; 3 6 Paired Programming Sprint Today&#39;s project involved programming a text based game of black jack with a dealer and 1 number of players. We also had the extra credit options adding n-players, AI/Bot players, double down, and split. I am happy to report that we that my partner and I were able to complete the first three, but ran out of time before implementing split. We started off with pencil and paper using the noun,verb method of abstraction. We settled on making a Deck, a Player, and Hand, and Game, and an AI. Its clear at the end that we should have abstracted the game more, and given the Hand class more responsibilities to best implement the split method. After we finished we had our dealer hit until 17 or above, while our AI bot hit until soft 17 below. We also had it implement a doubling bettering strategy. In our sample, the AI agent one more often then not came out ahead from this setup. It added a little credence to the ways dealer&#39;s seem to play in Las Vegas. The repo is currently private, because it could be a project for future cohorts. I do not want to make a copy public, but will if they give permission.", 
            "tags": "Galvanize", 
            "loc": "http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-02/"
        },
        {
            "title": "Galvanize - Week 01 - Day 1", 
            "text":"Galvanize Immersive Data Science Week 1 - Day 1 Today is my first day attending Galvanize&#39;s Immersive Data Science Program in San Francisco, CA. The program is a 12 week program that is approximately 10 hours a day of learning and activities to reinforce and refine the learning. I am very excited to be a part of this program. My Background I have a Ph.d in Theoretical High Energy Particle Physics and Cosmology, earned a Data Analysis Nano-degree from Udacity.com, and also am current working on a M.S. in Computer Science from GA Tech. I have also spent the last 8 years teaching high school physics and robotics. I will likely have some strength with math and theory, but I have no doubt that my programming will significantly improve over the next 12 weeks. Everyone in the program is well educated and intelligent, and each one of them have strengths in some areas and room for improvements in others. It seems to be a strength for this program. No matter your weakness, there are students that have that as a strength. Summary of the Day The first half of the day is getting to know our instructors, hour cohort, and the amazingly nice galvanize complex. It is a 5 story building filled with startup companies, work spaces, galvanize students, and other tech visitors. I found this to be an impressive building. After about an hour of getting to know everyone, we were given a presentation. That was followed by a tour of the galvanize building. We were then given an assessment on the pre-course material that was given to us before we showed up. After lunch, we had an 90 minute lecture, then worked on a paired sprint assignment for about 3 hours. This assignment involved... After we finished the sprint, we were invited to a Galvanize happy hour to socialize over beer and wine. I feel very lucky to be apart of this program, and have been impressed with my cohort, the instructors, and Galvanize. The Test The pre-course material required us to complete material on the following topics: Python Linear Algebra SQL Numpy/Pandas Probability Statistics Hypothesis Testing Web Awareness The initial assessment we were given was a 120 minute test on the first 5 topics. It was an &#39;open book&#39; test, but that does not mean it was easy. A majority of my peers did not finish within the allotted time. I am not going to post details on the test because I would hate to ruin the thrill of discovery for potential future students. LUNCH!!!! They provide us a lunch on the first day, but most days we have an 75 minute break for lunch. There are kitchen, fridges, storage for us to use if we wish. The lunch was nice, from a local Thai place. Lecture The lecture, in my opinion, was a little redundant with the course material. It seemed structure under the assumption that you didn&#39;t read or review the python pre-course materials. I understand its important that everyone is on the same starting point, but I wish we got to jump in a little deeper. I did learn and see the importance of using generators when possible. It save both memory and time. Paired programming After the lecture we grouped up for a paired programming assignment. We trade off roles of being the driver and the navigator in 20 to 30 minute rotations for a 3 hour block of programming. We start of by forking the day&#39;s assignment from a Github repo and cloning it locally. Today we then worked two projects. The first project was completing a list of functions based on a description of the function, including inputs and outputs. The second project was fixing inefficiently running code. A simple example is checking if a key is in a dictionary. Before today I might have checked to see if it was in the keys() results, but we can see that for medium size dictionaries that it is almost 40x slower than just using in in the dictionary. from collections import Counter from random import randint cnt = Counter([randint(0,1000) for x in range(10000)]) %timeit 1 in cnt.keys() %timeit 1 in cnt 100000 loops, best of 3: 4.17 µs per loop 10000000 loops, best of 3: 111 ns per loop We saw similar results for iter compared to iteritems, range to xrange, and izip to zip. It was a useful assignment for the content and the practice of collaborating with someone else. After reception Galvanize SF now runs two cohorts 6 weeks apart. We had a mixer with previous cohort, enjoying beer, wine, and conversation on the roof of the building. After 11 hours at Galvanize, I decided it was time to head home. Definitely looking forward to day 2.", 
            "tags": "Galvanize", 
            "loc": "http://www.bryantravissmith.com/galvanize/galvanize-data-science-01-01/"
        },
        {
            "title": "Udacity - Data Analysis NanoDegree - Project 5", 
            "text":"Project 5 - Data Visualization with D3.js Summary My data visualization is highlighting the chemical properties of wine that are associated with the ranking of the wine. The data comes from http://www3.dsi.uminho.pt/pcortez/wine/ and is a ranking of 6000+ red and white wines from Portugal along with 11 chemical properties. The visualization guides the user through some interesting graphics, then allows the user to explore the data themselves at the end of a guided story. Final Visualization Wine Story Visualization Explore All Data Visualization Wine Data Visualization Design My initial goal was to guild an simple way to see visualization of all the wine data. I wanted to make it easy for someone new to see the trends and patterns in all the data, and to explore it, without needs to do coding. I thought my story would be to show some of the interesting associations between quality and chemical properties. My initial design was a sketch of the User Interface that I showed on of my students who has built and sold numerous mobile apps. He liked the separation of the menu and graphics, but suggested that I move the menu to the left side. He said most sights have menus on the left side. Additionally he suggest using a css framework. After reviewing some I decided to use Pure CSS. I wanted to let the user subset data so they can decide to plot only wines of a certain quality or of a specific type. For instance, only white wines with quality greater than 6. This required that I have a way to both subset the data and visual indicate the which data is selected. This was done using the Pure CSS styling options, and initially with the D3.select. As the page grew more than 500 lines of code, I decided to switch to jQuery for the user interactions because it was easier to choose and change properties on the page, as well as to simulate users selections for the guided story. In the initial implementation, I implanted a percent/density visualization in the histograms because of feedback from my wife. She was confused why some numbers changed when putting in the count, but seemed to describe the distributions accurately when they were displayed as percentages. She also found the side menu intuitive, but didn&#39;t understand what the variables meant. I added the description under the graphic to help give the user context to the data and to develop a narrative during the initial animation. My wife also did not like the colors I used for the scatter plots. She called them ugly. We settled on the yellow and reds used the final product. My friend Travis thought some trend lines would help digest the information better than the scatter plots. He also had some formatting issues when he looked at in on his computer. I decided to add another plot option of to show the average and 1 standard deviation, as well as make the plot resize in the browser. After the Udacity review, it was clear to me that I had build something for exploring instead of explaining the data. My scope was too big, and my goal unfocused. I decided I would focus on red wine, which the data has some relatively strong association with quality and chemical properties. The results also match the intuition one gets for reds if they read any wine literature. I decided to refocus on the story - good wines are alcoholic and fruity, and they are not yet vinegar. I limited the variables I would display, and allow the user to control, to alcohol, citric.acid, volatile.acidity, and quality. I also remove the control bar for the first half of the story, not showing it until it added new information, and did not give the option for the user to skip it. Every graphic in the story supports the idea that well rated reds tend to be fruiter, alcoholic, and not vinegary. Feedback My feed back was gaged in person where I asked people near me to &#34;look at this&#34; and guided them through the final story I have implemented. This conversation were less then 15 minutes. 1 A student suggested that I use a css framework and move the menu from the right side of the graphic to the left side of the graphic. 2 My wife examined an early version of the visualization. She thought I should use percentages instead of raw counts for displaying the values in the histogram bars, and to add descriptions of the variables/graphs below each graph. She liked the sidebar menu to control the visualization. She was also sure that drinking wine is better then making graphs of wine. 3 My friend Travis said that there were too many points to get a feel for trends and would have liked to added a best fit line. He also said the plot was not fitting in his browser. He was very interested in the wine, being a self professed &#39;Wino&#39;, and expressed that he would like to see this again if I found similar data for California wines. 4 Udacity review stated that I have given too much control to the user, and that dozens of plots can be generated with current setup. I should focus on an explanatory story instead of an exploratory setup. I did not tell a specific enough story, and should focus on one story with a few key variables and plots. Resources http://bl.ocks.org/mbostock/3048166 I used this site for the initial design and implementation of the histogram. Stack Overflow I used stack overflow to look up any specific issues in trying to implement my visualization. More often than not it was syntax or not being currently familiar with javascript methods of interacting with different datatypes. Pure CSS - http://purecss.io/ I ended up using this framework because it was externally hosted, light weight, and easy to use.", 
            "tags": "Udacity", 
            "loc": "http://www.bryantravissmith.com/udacity/udacity-project-5/"
        },
        {
            "title": "Udacity - Data Analysis NanoDegree - Project 4", 
            "text":"Udacity - NanoDegree - Project 4 - Identifying POI Project Goal The goal of this project is to develop and tune a supervised classification algorithm to identify persons of interest (POI) in the Enron scandal based on a combination of publically available Enron financial data and email records. The modest goals are to have recall and precision scores above 0.3. The compiled data set contains information for 144 people employed at Enron, a ‘Travel Agency in the Park’, and the total compensations for all of these sources. Additionally, the Udacity.com course designer created email features that give the total number of e-mails sent and received for each user, and the total number of emails sent to and receieved from a POI. Of the 144 people, 18 of them are labeled as POIs. Lets start by reading in the data; import sys import pickle import numpy as np import pandas as pd #Load Udacity Tools for Testing Results sys.path.append(&#34;../tools/&#34;) from feature_format import featureFormat, targetFeatureSplit from tester import test_classifier, dump_classifier_and_data ### Load the dictionary containing the dataset data_dict = pickle.load(open(&#34;final_project_dataset.pkl&#34;, &#34;r&#34;) ) data = pd.DataFrame.from_dict(data_dict) #Remove Invalide Rows data = data.drop([&#39;TOTAL&#39;,&#39;THE TRAVEL AGENCY IN THE PARK&#39;],axis=1) data = data.transpose() #Give Each Person their own row data bonus deferral_payments deferred_income director_fees email_address exercised_stock_options expenses from_messages from_poi_to_this_person from_this_person_to_poi ... long_term_incentive other poi restricted_stock restricted_stock_deferred salary shared_receipt_with_poi to_messages total_payments total_stock_value ALLEN PHILLIP K 4175000 2869717 -3081055 NaN phillip.allen@enron.com 1729541 13868 2195 47 65 ... 304805 152 False 126027 -126027 201955 1407 2902 4484442 1729541 BADUM JAMES P NaN 178980 NaN NaN NaN 257817 3486 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 182466 257817 BANNANTINE JAMES M NaN NaN -5104 NaN james.bannantine@enron.com 4046157 56301 29 39 0 ... NaN 864523 False 1757552 -560222 477 465 566 916197 5243487 BAXTER JOHN C 1200000 1295738 -1386055 NaN NaN 6680544 11200 NaN NaN NaN ... 1586055 2660303 False 3942714 NaN 267102 NaN NaN 5634343 10623258 BAY FRANKLIN R 400000 260455 -201641 NaN frank.bay@enron.com NaN 129142 NaN NaN NaN ... NaN 69 False 145796 -82782 239671 NaN NaN 827696 63014 BAZELIDES PHILIP J NaN 684694 NaN NaN NaN 1599641 NaN NaN NaN NaN ... 93750 874 False NaN NaN 80818 NaN NaN 860136 1599641 BECK SALLY W 700000 NaN NaN NaN sally.beck@enron.com NaN 37172 4343 144 386 ... NaN 566 False 126027 NaN 231330 2639 7315 969068 126027 BELDEN TIMOTHY N 5249999 2144013 -2334434 NaN tim.belden@enron.com 953136 17355 484 228 108 ... NaN 210698 True 157569 NaN 213999 5521 7991 5501630 1110705 BELFER ROBERT NaN -102500 NaN 3285 NaN 3285 NaN NaN NaN NaN ... NaN NaN False NaN 44093 NaN NaN NaN 102500 -44093 BERBERIAN DAVID NaN NaN NaN NaN david.berberian@enron.com 1624396 11892 NaN NaN NaN ... NaN NaN False 869220 NaN 216582 NaN NaN 228474 2493616 BERGSIEKER RICHARD P 250000 NaN -485813 NaN rick.bergsieker@enron.com NaN 59175 59 4 0 ... 180250 427316 False 659249 NaN 187922 233 383 618850 659249 BHATNAGAR SANJAY NaN NaN NaN 137864 sanjay.bhatnagar@enron.com 2604490 NaN 29 0 1 ... NaN 137864 False -2604490 15456290 NaN 463 523 15456290 NaN BIBI PHILIPPE A 1000000 NaN NaN NaN philippe.bibi@enron.com 1465734 38559 40 23 8 ... 369721 425688 False 378082 NaN 213625 1336 1607 2047593 1843816 BLACHMAN JEREMY M 850000 NaN NaN NaN jeremy.blachman@enron.com 765313 84208 14 25 2 ... 831809 272 False 189041 NaN 248546 2326 2475 2014835 954354 BLAKE JR. NORMAN P NaN NaN -113784 113784 NaN NaN 1279 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 1279 NaN BOWEN JR RAYMOND M 1350000 NaN -833 NaN raymond.bowen@enron.com NaN 65907 27 140 15 ... 974293 1621 True 252055 NaN 278601 1593 1858 2669589 252055 BROWN MICHAEL NaN NaN NaN NaN michael.brown@enron.com NaN 49288 41 13 1 ... NaN NaN False NaN NaN NaN 761 1486 49288 NaN BUCHANAN HAROLD G 500000 NaN NaN NaN john.buchanan@enron.com 825464 600 125 0 0 ... 304805 1215 False 189041 NaN 248017 23 1088 1054637 1014505 BUTTS ROBERT H 750000 NaN -75000 NaN bob.butts@enron.com NaN 9410 NaN NaN NaN ... 175000 150656 False 417619 NaN 261516 NaN NaN 1271582 417619 BUY RICHARD B 900000 649584 -694862 NaN rick.buy@enron.com 2542813 NaN 1053 156 71 ... 769862 400572 False 901657 NaN 330546 2333 3523 2355702 3444470 CALGER CHRISTOPHER F 1250000 NaN -262500 NaN christopher.calger@enron.com NaN 35818 144 199 25 ... 375304 486 True 126027 NaN 240189 2188 2598 1639297 126027 CARTER REBECCA C 300000 NaN -159792 NaN rebecca.carter@enron.com NaN NaN 15 29 7 ... 75000 540 False 307301 -307301 261809 196 312 477557 NaN CAUSEY RICHARD A 1000000 NaN -235000 NaN richard.causey@enron.com NaN 30674 49 58 12 ... 350000 307895 True 2502063 NaN 415189 1585 1892 1868758 2502063 CHAN RONNIE NaN NaN -98784 98784 NaN NaN NaN NaN NaN NaN ... NaN NaN False 32460 -32460 NaN NaN NaN NaN NaN CHRISTODOULOU DIOMEDES NaN NaN NaN NaN diomedes.christodoulou@enron.com 5127155 NaN NaN NaN NaN ... NaN NaN False 950730 NaN NaN NaN NaN NaN 6077885 CLINE KENNETH W NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN False 662086 -472568 NaN NaN NaN NaN 189518 COLWELL WESLEY 1200000 27610 -144062 NaN wes.colwell@enron.com NaN 16514 40 240 11 ... NaN 101740 True 698242 NaN 288542 1132 1758 1490344 698242 CORDES WILLIAM R NaN NaN NaN NaN bill.cordes@enron.com 651850 NaN 12 10 0 ... NaN NaN False 386335 NaN NaN 58 764 NaN 1038185 COX DAVID 800000 NaN -41250 NaN chip.cox@enron.com 117551 27861 33 0 4 ... NaN 494 False 378082 NaN 314288 71 102 1101393 495633 CUMBERLAND MICHAEL S 325000 NaN NaN NaN NaN NaN 22344 NaN NaN NaN ... 275000 713 False 207940 NaN 184899 NaN NaN 807956 207940 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... SAVAGE FRANK NaN NaN -121284 125034 NaN NaN NaN NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 3750 NaN SCRIMSHAW MATTHEW NaN NaN NaN NaN matthew.scrimshaw@enron.com 759557 NaN NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN NaN 759557 SHANKMAN JEFFREY A 2000000 NaN NaN NaN jeffrey.shankman@enron.com 1441898 178979 2681 94 83 ... 554422 1191 False 630137 NaN 304110 1730 3221 3038702 2072035 SHAPIRO RICHARD S 650000 NaN NaN NaN richard.shapiro@enron.com 607837 137767 1215 74 65 ... NaN 705 False 379164 NaN 269076 4527 15149 1057548 987001 SHARP VICTORIA T 600000 187469 NaN NaN vicki.sharp@enron.com 281073 116337 136 24 6 ... 422158 2401 False 213063 NaN 248146 2477 3136 1576511 494136 SHELBY REX 200000 NaN -4167 NaN rex.shelby@enron.com 1624396 22884 39 13 14 ... NaN 1573324 True 869220 NaN 211844 91 225 2003885 2493616 SHERRICK JEFFREY B NaN NaN NaN NaN jeffrey.sherrick@enron.com 1426469 NaN 25 39 18 ... NaN NaN False 405999 NaN NaN 583 613 NaN 1832468 SHERRIFF JOHN R 1500000 NaN NaN NaN john.sherriff@enron.com 1835558 NaN 92 28 23 ... 554422 1852186 False 1293424 NaN 428780 2103 3187 4335388 3128982 SKILLING JEFFREY K 5600000 NaN NaN NaN jeff.skilling@enron.com 19250000 29336 108 88 30 ... 1920000 22122 True 6843672 NaN 1111258 2042 3627 8682716 26093672 STABLER FRANK 500000 NaN NaN NaN frank.stabler@enron.com NaN 16514 NaN NaN NaN ... NaN 356071 False 511734 NaN 239502 NaN NaN 1112087 511734 SULLIVAN-SHAKLOVITZ COLLEEN 100000 181993 NaN NaN NaN 1362375 NaN NaN NaN NaN ... 554422 162 False NaN NaN 162779 NaN NaN 999356 1362375 SUNDE MARTIN 700000 NaN NaN NaN marty.sunde@enron.com NaN NaN 38 37 13 ... 476451 111122 False 698920 NaN 257486 2565 2647 1545059 698920 TAYLOR MITCHELL S 600000 227449 NaN NaN mitchell.taylor@enron.com 3181250 NaN 29 0 0 ... NaN NaN False 563798 NaN 265214 300 533 1092663 3745048 THORN TERENCE H NaN 16586 NaN NaN terence.thorn@enron.com 4452476 46145 41 0 0 ... 200000 426629 False 365320 NaN 222093 73 266 911453 4817796 TILNEY ELIZABETH A 300000 NaN -575000 NaN elizabeth.tilney@enron.com 591250 NaN 19 10 11 ... 275000 152055 False 576792 NaN 247338 379 460 399393 1168042 UMANOFF ADAM S 788750 NaN NaN NaN adam.umanoff@enron.com NaN 53122 18 12 0 ... NaN NaN False NaN NaN 288589 41 111 1130461 NaN URQUHART JOHN A NaN NaN -36666 36666 NaN NaN 228656 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 228656 NaN WAKEHAM JOHN NaN NaN NaN 109298 NaN NaN 103773 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 213071 NaN WALLS JR ROBERT H 850000 NaN NaN NaN rob.walls@enron.com 4346544 50936 146 17 0 ... 540751 2 False 1552453 NaN 357091 215 671 1798780 5898997 WALTERS GARETH W NaN 53625 NaN NaN NaN 1030329 33785 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 87410 1030329 WASAFF GEORGE 325000 831299 -583325 NaN george.wasaff@enron.com 1668260 NaN 30 22 7 ... 200000 1425 False 388167 NaN 259996 337 400 1034395 2056427 WESTFAHL RICHARD K NaN NaN -10800 NaN dick.westfahl@enron.com NaN 51870 NaN NaN NaN ... 256191 401130 False 384930 NaN 63744 NaN NaN 762135 384930 WHALEY DAVID A NaN NaN NaN NaN NaN 98718 NaN NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN NaN 98718 WHALLEY LAWRENCE G 3000000 NaN NaN NaN greg.whalley@enron.com 3282960 57838 556 186 24 ... 808346 301026 False 2796177 NaN 510364 3920 6019 4677574 6079137 WHITE JR THOMAS E 450000 NaN NaN NaN thomas.white@enron.com 1297049 81353 NaN NaN NaN ... NaN 1085463 False 13847074 NaN 317543 NaN NaN 1934359 15144123 WINOKUR JR. HERBERT S NaN NaN -25000 108579 NaN NaN 1413 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 84992 NaN WODRASKA JOHN NaN NaN NaN NaN john.wodraska@enron.com NaN NaN NaN NaN NaN ... NaN 189583 False NaN NaN NaN NaN NaN 189583 NaN WROBEL BRUCE NaN NaN NaN NaN NaN 139130 NaN NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN NaN 139130 YEAGER F SCOTT NaN NaN NaN NaN scott.yeager@enron.com 8308552 53947 NaN NaN NaN ... NaN 147950 True 3576206 NaN 158403 NaN NaN 360300 11884758 YEAP SOON NaN NaN NaN NaN NaN 192758 55097 NaN NaN NaN ... NaN NaN False NaN NaN NaN NaN NaN 55097 192758 144 rows × 21 columns In regards to the financial information, I defined outliers as having values that are more than 3 standard deviations from the mean value for the group. This is not the traditional definition or criteria for an outlier which is 1.5 times the interquartile range below the first quartile or Above the third quartile. I used my definition after I have replaced missing values with zero. Using this definition of outliers there are 25 people in the data set that are financial outliers: finance = [&#39;salary&#39;, &#39;deferral_payments&#39;, &#39;total_payments&#39;, &#39;exercised_stock_options&#39;, &#39;bonus&#39;, &#39;restricted_stock&#39;, &#39;restricted_stock_deferred&#39;, &#39;total_stock_value&#39;, &#39;expenses&#39;, &#39;loan_advances&#39;, &#39;other&#39;, &#39;director_fees&#39;, &#39;deferred_income&#39;, &#39;long_term_incentive&#39;] from scipy import stats #Use unique because some people are financial outliers in multiple variables data[np.abs(stats.zscore(data[finance].replace(&#39;NaN&#39;,0.0))) &gt; 3].index.unique() array([&#39;ALLEN PHILLIP K&#39;, &#39;BELDEN TIMOTHY N&#39;, &#39;BHATNAGAR SANJAY&#39;, &#39;BLAKE JR. NORMAN P&#39;, &#39;FREVERT MARK A&#39;, &#39;GRAMM WENDY L&#39;, &#39;HANNON KEVIN P&#39;, &#39;HIRKO JOSEPH&#39;, &#39;HORTON STANLEY C&#39;, &#39;HUMPHREY GENE E&#39;, &#39;JAEDICKE ROBERT&#39;, &#39;LAVORATO JOHN J&#39;, &#39;LAY KENNETH L&#39;, &#39;LEMAISTRE CHARLES&#39;, &#39;MARTIN AMANDA K&#39;, &#39;MCCLELLAN GEORGE&#39;, &#39;MENDELSOHN JOHN&#39;, &#39;PAI LOU L&#39;, &#39;RICE KENNETH D&#39;, &#39;SAVAGE FRANK&#39;, &#39;SHANKMAN JEFFREY A&#39;, &#39;SKILLING JEFFREY K&#39;, &#39;URQUHART JOHN A&#39;, &#39;WAKEHAM JOHN&#39;, &#39;WHITE JR THOMAS E&#39;, &#39;WINOKUR JR. HERBERT S&#39;], dtype=object) This accounts for 17% of the data being considered an outlier and also contains 33% of the POI. Ultimately I decided that financial outliers were relevant information, and decided not to remove them from the data for this analysis. New Feature I decided to create two features I thought were be informative to the data: the ratio of emails received from a POI to the total number of emails received and the ratio of emails sent to a POI to the total number of emails sent. I thought these features would be more informative than the total number of emails sent or received from a POI because it normalizes by how active or how popular a person is. If a person sends 10 emails, and 5 of them are to a POI, that seems more relevant than if a person sends 1000 emails and 5 of them are to a POI. A person who sends 50% of their emails to a person of interest seems more suspect than a person who only sends 0.5% of their emails to a POI. The inverse is not true, however. If a person received 10 emails from a POI, that is as relevant regardless if the person receives 20 emails or 2000 emails. The important idea is that how often is a POI is contacting this person, and how does that affect the likelihood that a person is also a POI. This ratio does not capture that idea, but I created it because I was willing to be proven wrong. data.from_this_person_to_poi = data.from_this_person_to_poi.astype(float) data.from_poi_to_this_person = data.from_poi_to_this_person.astype(float) data.to_messages = data.to_messages.astype(float) data.from_messages = data.from_messages.astype(float) data[&#39;from_this_person_to_poi_ratio&#39;] = data.from_this_person_to_poi/data.to_messages data[&#39;from_poi_to_this_person_ratio&#39;] = data.from_poi_to_this_person/data.from_messages data = data.replace(&#39;NaN&#39;,0.0) After the creation of these features I used sklearn&#39;s Pipeline, FeatureUnion, and GridSearch to search through a combintation of variable to find the set of features that gave the best performance for a Decision Tree Classifier. I search between 1 and 9 of the best features using a 5 folds stratified cross validation to gauge performance for each set. I am using a f1 score to maximize the weight combintation of recall and precision. from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.cross_validation import StratifiedKFold from sklearn.metrics import roc_auc_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.metrics import f1_score from sklearn.metrics import roc_auc_score from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.grid_search import GridSearchCV from sklearn.decomposition import PCA from sklearn.feature_selection import SelectKBest from sklearn.preprocessing import StandardScaler from sklearn.metrics import roc_curve from sklearn.learning_curve import learning_curve tru = data.poi.values trn = data.drop([&#39;poi&#39;,&#39;email_address&#39;],axis=1).values clf = DecisionTreeClassifier() #pca = PCA(n_components=2) selection = SelectKBest(k=1) #combined_features = FeatureUnion([(&#34;pca&#34;, pca), (&#34;univ_select&#34;, selection)]) combined_features = FeatureUnion([(&#34;univ_select&#34;, selection)]) X_features = combined_features.fit(trn, tru).transform(trn) pipeline = Pipeline([(&#34;features&#34;, combined_features), (&#34;clf&#34;, clf)]) #param_grid = dict(features__pca__n_components=range(0,15), # features__univ_select__k=range(1,10)) param_grid = dict(features__univ_select__k=range(1,10)) skf = StratifiedKFold(tru, n_folds=5) grid_search = GridSearchCV(pipeline, cv=skf, param_grid=param_grid, scoring=&#39;f1&#39;) grid_search.fit(trn, tru) print grid_search.best_score_ print grid_search.best_params_ var_scores = grid_search.best_estimator_.steps[0][1].get_params()[&#39;univ_select&#39;].scores_ print var_scores num_features = grid_search.best_params_[&#39;features__univ_select__k&#39;] best_features = data.drop([&#39;poi&#39;,&#39;email_address&#39;],axis=1).columns[np.argsort(var_scores)[::-1][:num_features]] best_features 0.36813973064 {&#39;features__univ_select__k&#39;: 3} [ 21.06000171 0.21705893 11.59554766 2.10765594 25.09754153 6.23420114 0.1641645 5.34494152 2.42650813 7.2427304 10.07245453 4.24615354 9.34670079 0.06498431 18.57570327 8.74648553 1.69882435 8.87383526 24.46765405 4.16908382 5.20965022] Index([u&#39;exercised_stock_options&#39;, u&#39;total_stock_value&#39;, u&#39;bonus&#39;], dtype=&#39;object&#39;) Result of Search The search is finished and the best f1 score of 0.368 with 3 &#39;best features&#39;. Because of the random shuffling involved in the scoring, it is possible to get a variety in number and combintations of best features. The best features for predicting if a person is a person of interest are financal features: Exercised stock options, total stock value, and bonus. Even when the number of best features fluxuate, these 3 are always among them. Turning Model Using the paramemters from the above search we will now tune the classifier to optimize its performance. I am searching through the spliting and depth criteria for the Decision Tree Classifier. tru = data.poi.values trn = data[best_features].values clf = DecisionTreeClassifier() pipeline = Pipeline([(&#34;clf&#34;, clf)]) param_grid = dict(clf__criterion=(&#34;gini&#34;,&#34;entropy&#34;), clf__min_samples_split=[1,2,4,8,16,32], clf__min_samples_leaf=[1,2,4,8,16,32], clf__max_depth=[None,1,2,4,8,16,32]) skf = StratifiedKFold(tru, n_folds=5) grid_search = GridSearchCV(pipeline, cv=skf, param_grid=param_grid, scoring=&#39;f1&#39;) grid_search.fit(trn, tru) print grid_search.best_params_ print grid_search.best_score_ best_clf = grid_search.best_estimator_ best_clf {&#39;clf__criterion&#39;: &#39;gini&#39;, &#39;clf__max_depth&#39;: 32, &#39;clf__min_samples_leaf&#39;: 1, &#39;clf__min_samples_split&#39;: 1} 0.47075617284 Pipeline(steps=[(&#39;clf&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=32, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=1, min_weight_fraction_leaf=0.0, random_state=None, splitter=&#39;best&#39;))]) The best parameters for the Decision Tree Classifier has a max depth of 32, a min sample leaf size of 1, and requires at least 1 values to split the tree when fitting. These results produced the best averge f1 score of 0.47. Importance of Validation Validation is an attempt to confirm that a model will give reasonable or consistent results on new, untrained data. A classic mistake is to test the results of a model on the data used to train the model. This is no doubt give the best possible score, but can over fit the data leading to less than desired results on new data. Validation protects against this mistake by training the model on one set of data and testing on yet another. I used 5-Fold Statfied Cross Validation for investigating and comparing algorithms in this analysis. This is where there the algorithm is trained on the on the data 5 times using 80% of the data as a training set and 20% of the data as a testing set. Each set has approxiamately the same ratio of positive and negative examples of POI. Each time this is done, the training and testing set are shuffled to create a new 80/20 split on the data. I then used the average performance as an estimate of its performance on new data. Final Performance Using the Udacity &#39;test_classifier&#39; function I evaluate my classifer to see if the recall and precision scores are both greater than 0.3. #tt = pd.DataFrame(pca.fit_transform(trn),index=data.index) #data2 = data.join(tt) #print data2.columns my_data = data.transpose().to_dict() features = best_features.tolist() features = [&#39;poi&#39;]+features test_classifier(best_clf, my_data, features) Pipeline(steps=[(&#39;clf&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=32, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=1, min_weight_fraction_leaf=0.0, random_state=None, splitter=&#39;best&#39;))]) Accuracy: 0.80492 Precision: 0.37066 Recall: 0.38400 F1: 0.37721 F2: 0.38125 Total predictions: 13000 True positives: 768 False positives: 1304 False negatives: 1232 True negatives: 9696 The results of the tuned classifier are shown above. The average precision of the 13000 prediction is 37% and the average recall is 38%. The precision value is that out of all predictions of people being a POI, 38% of them are actually POI. The recall value is that out of all of the POI, 37% are actually predicted to be POI by my tuned classifier.", 
            "tags": "Udacity", 
            "loc": "http://www.bryantravissmith.com/udacity/udacity-project-4/"
        },
        {
            "title": "Udacity Project 3", 
            "text":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } Project 3 - Wine Analysis Bryan Smith Saturday, January 10, 2015 Introduction This study will explore the quality of red and white wines using data available for public research. The details of the data and initial analysis can be found in Cortez et. all: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236. The data contains the following twelve attributes for both red and white wines: fixed acidity (tartaric acid - g / dm^3) volatile acidity (acetic acid - g / dm^3) citric acid (g / dm^3) residual sugar (g / dm^3) chlorides (sodium chloride - g / dm^3 free sulfur dioxide (mg / dm^3) total sulfur dioxide (mg / dm^3) density (g / cm^3) pH sulfates (potassium sulfate - g / dm3) alcohol (% by volume) quality (score between 0 and 10 - average of 3 wine expert ratings) Exploratory Data Analysis We will begin the analysis by loading the data, but first I will convert all the variables but pH to g/dm^3 (g/liter). red &lt;- read.csv(&#34;WineQualityReds.csv&#34;) white &lt;- read.csv(&#34;WineQualityWhites.csv&#34;) red$type = &#39;red&#39; white$type = &#39;white&#39; wine &lt;- rbind(red,white) wine$type &lt;- factor(wine$type) wine$quality &lt;- factor(wine$quality) wine$density &lt;- 1000*wine$density wine$free.sulfur.dioxide &lt;- wine$free.sulfur.dioxide/1000 wine$total.sulfur.dioxide &lt;- wine$total.sulfur.dioxide/1000 summary(wine$type) ## red white ## 1599 4898 summary(wine$quality) ## 3 4 5 6 7 8 9 ## 30 216 2138 2836 1079 193 5 The fact that there are so few wines in the higher and lower qualities might suggest a reclassification of quality later on. Box Plots of Each Variable by Quality &amp; Type To get a feel for the data we will explore a summary plot of each variable by type and quality. ###Density summary(wine$density) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 987.1 992.3 994.9 994.7 997.0 1039.0 ggplot(wine,aes(factor(type),density,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),density,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),density,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),density,fill=quality))+geom_boxplot() White wines seem to be less dense then red wines, but there is overlap. Both tend to be less dense than water (1000 g/liter). There seems to be a slight decrease in density in higher qualities wines. This could have something to do with chemicals that have to do with wine quality are less dense or absorb more gasses. Fixed Acidity Tartaric acid is found in higher concentrations in wines where grapes come from cooler climates. It is associated with the sourness of a wine. summary(wine$fixed.acidity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.800 6.400 7.000 7.215 7.700 15.900 ggplot(wine,aes(factor(type),fixed.acidity,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),fixed.acidity,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),fixed.acidity,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),fixed.acidity,fill=quality))+geom_boxplot() ggplot(wine,aes(fixed.acidity,fill=type))+geom_histogram() ggplot(wine,aes(fixed.acidity,fill=quality,alpha=0.1))+ geom_density()+ facet_wrap(~quality+type) The white wines typically have lower values of fixed acidity, but there is a significant overlap between the two types of wines. There does not seem to be a strong associated between quality and the amount of tartaric acid. It almost looks like in the red that there are two sources of tartaric acid, and higher quality wines reduce the second source. Volatile Acidity Volatile acidity is acetic acid usually caused by bacteria. This chemical is associated with a vinegar taste in wines. summary(wine$volatile.acidity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0800 0.2300 0.2900 0.3397 0.4000 1.5800 ggplot(wine,aes(factor(type),volatile.acidity,fill=type))+ geom_boxplot() ggplot(wine,aes(factor(quality),volatile.acidity,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(quality),volatile.acidity,fill=type))+ geom_boxplot() ggplot(wine,aes(factor(type),volatile.acidity,fill=quality))+ geom_boxplot() ggplot(wine,aes(volatile.acidity,fill=type))+ geom_histogram() ggplot(wine,aes(volatile.acidity,fill=quality,alpha=0.1))+ geom_density()+ facet_wrap(~quality+type) White wines have lower values of volatile acidity, and there does not seem to be a strong associated with quality. There does seem to be a strong associated with lower volatile acidity with higher quality wines. Citric Acid Citric acid is associated with fruitiness or liveliness of a wine. Wines low is citric acid tend to be considered ‘flat’. summary(wine$citric.acid) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.2500 0.3100 0.3186 0.3900 1.6600 ggplot(wine,aes(factor(type),citric.acid,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),citric.acid,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),citric.acid,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),citric.acid,fill=quality))+geom_boxplot() ggplot(wine,aes(citric.acid,fill=type))+geom_histogram() ggplot(wine,aes(citric.acid,fill=quality,alpha=0.1))+ geom_density()+facet_wrap(~quality+type) White wines seem to be centered around a similar concentration of citric acid, while high quality red wines seem to have high concentrations of citric acid. pH pH is the total measure of acidity of a wine that runs from 1 (acid) to 14 (base). To avoid spoilage most wines need to have a pH below 4. Most wine makers shoot for values between 3 and 3.3 for whites and 3.2-3.5 for reds. summary(wine$pH) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.720 3.110 3.210 3.219 3.320 4.010 ggplot(wine,aes(factor(type),pH,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),pH,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),pH,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),pH,fill=quality))+geom_boxplot() ggplot(wine,aes(pH,fill=type))+geom_histogram() ggplot(wine,aes(pH,fill=quality,alpha=0.1))+ geom_density()+facet_wrap(~quality+type) It is interesting to see the bell shapes for almost all wine types and qualities. Reading online suggest that this is an aspect of the wine that is engineered I am wondering if the sum or ratios of these acidity are correlated to quality in any strong way. wine$total.acidity = wine$citric.acid+wine$volatile.acidity+wine$fixed.acidity ggplot(wine,aes(factor(type),total.acidity,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),fixed.acidity/total.acidity,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),volatile.acidity/total.acidity,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),citric.acid/total.acidity,fill=quality))+ geom_boxplot() These ratios seem to highlight some of the differences in the quality a little bit better than raw values. I am interested in seeing these relationships with the pH values as well. pH is a log scale based on concentration. ggplot(wine,aes(factor(type),log10(total.acidity/density)/pH,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),log10(fixed.acidity/density)/pH,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),log10(volatile.acidity/density)/pH,fill=quality))+ geom_boxplot() ggplot(wine,aes(factor(type),log10(citric.acid/density)/pH,fill=quality))+ geom_boxplot() ## Warning: Removed 151 rows containing non-finite values (stat_boxplot). These plots do not illustrate anything new but before I move on I want to look at some 2D plots. ggplot(wine,aes(x=total.acidity, y=pH,color=quality))+ geom_point()+facet_wrap(~quality) ggplot(wine,aes(x=total.acidity, y=volatile.acidity,color=quality))+ geom_point()+facet_wrap(~quality+type) ggplot(wine,aes(x=total.acidity, y=fixed.acidity,color=quality))+ geom_point()+facet_wrap(~quality+type) ggplot(wine,aes(x=total.acidity, y=citric.acid,color=quality))+ geom_point()+facet_wrap(~quality+type) ggplot(wine,aes(x=volatile.acidity, y=citric.acid,color=quality))+ geom_point()+facet_wrap(~quality+type) summary(wine$total.acidity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.130 7.020 7.600 7.874 8.380 17.040 summary(wine$volatile.acidity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0800 0.2300 0.2900 0.3397 0.4000 1.5800 summary(wine$fixed.acidity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.800 6.400 7.000 7.215 7.700 15.900 summary(wine$citric.acid) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.2500 0.3100 0.3186 0.3900 1.6600 The total wine acidity is dominated by fixed acidity. There does not seem to be any strong direct associated with any one acid. It is hard to tell because of the lower sample sizes, but it looks that higher quality wines seem to shrink down to an ellipse in the citric acid/volatile acidity space. Residual Surgar Residual sugar is associated with the sweetness of the wine. White wines tend to have higher amounts of residual sugar compared to red wines. summary(wine$residual.sugar) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.600 1.800 3.000 5.443 8.100 65.800 ggplot(wine,aes(factor(type),residual.sugar,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),residual.sugar,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),residual.sugar,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),residual.sugar,fill=quality))+geom_boxplot() ggplot(wine,aes(residual.sugar,fill=type))+geom_histogram() ggplot(wine,aes(residual.sugar,fill=quality,alpha=0.1))+ geom_density()+facet_wrap(~quality+type) Chlorides This is a measure of sodium chlorides (salt) found in the wine. The effect of salt is disputed on wines, but ‘moderate’ amounts tend to ‘smooth-out’ the wine. summary(wine$chlorides) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00900 0.03800 0.04700 0.05603 0.06500 0.61100 ggplot(wine,aes(factor(type),chlorides,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),chlorides,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),chlorides,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),chlorides,fill=quality))+geom_boxplot() ggplot(wine,aes(chlorides,fill=type))+geom_histogram() ggplot(wine,aes(chlorides,fill=quality,alpha=0.1))+geom_density()+facet_wrap(~quality+type) Whites have lower chlorides levels then reds, and the higher quality reds have distributions that are lower than those lower quality reds. Free Sulfer Dioxide Sulfur dioxide can bind with odor producing chemicals and be an antibiotic agent. It also reacts with a number of other chemicals found in wines, so it is used gingerly. summary(wine$free.sulfur.dioxide) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00100 0.01700 0.02900 0.03053 0.04100 0.28900 ggplot(wine,aes(factor(type),free.sulfur.dioxide,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),free.sulfur.dioxide,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),free.sulfur.dioxide,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),free.sulfur.dioxide,fill=quality))+geom_boxplot() ggplot(wine,aes(free.sulfur.dioxide,fill=type))+geom_histogram() ggplot(wine,aes(free.sulfur.dioxide,fill=quality,alpha=0.1))+geom_density()+facet_wrap(~quality+type) Red wines have lower amounts of S02 compared to white wines, and there seems to a small association between poor white wines and lower free sulfur dioxide levels. Total Sulur Dioxide summary(wine$total.sulfur.dioxide) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0060 0.0770 0.1180 0.1157 0.1560 0.4400 ggplot(wine,aes(factor(type),total.sulfur.dioxide,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),total.sulfur.dioxide,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),total.sulfur.dioxide,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),total.sulfur.dioxide,fill=quality))+geom_boxplot() ggplot(wine,aes(total.sulfur.dioxide,fill=type))+geom_histogram() ggplot(wine,aes(total.sulfur.dioxide,fill=quality,alpha=0.1))+ geom_density()+facet_wrap(~quality+type) Total sulfur dioxide levels seem consistent across quality, but white wines have higher values over red wines. Sulphites (Sulphates are not in wine) The data set states sulfates but sulfides are added to wine to limit microbial growth. summary(wine$sulphates) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2200 0.4300 0.5100 0.5313 0.6000 2.0000 ggplot(wine,aes(factor(type),sulphates,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),sulphates,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),sulphates,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),sulphates,fill=quality))+geom_boxplot() ggplot(wine,aes(sulphates,fill=type))+geom_histogram() ggplot(wine,aes(sulphates,fill=quality,alpha=0.1))+ geom_density()+facet_wrap(~quality+type) White wines have lower amounts of sulfates, and higher quality red wines seem to have more sulfates. Alcohol Alcohol is why we drink wine. summary(wine$alcohol) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 8.00 9.50 10.30 10.49 11.30 14.90 ggplot(wine,aes(factor(type),alcohol,fill=type))+geom_boxplot() ggplot(wine,aes(factor(quality),alcohol,fill=quality))+geom_boxplot() ggplot(wine,aes(factor(quality),alcohol,fill=type))+geom_boxplot() ggplot(wine,aes(factor(type),alcohol,fill=quality))+geom_boxplot() This amuses me to no end! This is the only metric that agrees with quality across both types of wine. Red and white wines of higher quality have more alcohol! Since alcohol is low density it is interesting to me to see if there is a relationships between density and % alcohol. ggplot(wine,aes(x=alcohol,y=density))+geom_point() A quick Google search pulls up this equation for the density of wine: 789%-alcohol + 1587%-residual.sugar + (1-%-alcohol-%-residual.surfer)*1000 = density. Lets check it wine$density.predicted = 789*wine$alcohol wine$density.predicted = wine$density.predicted + 1587*wine$residual.sugar/10 wine$density.predicted = wine$density.predicted + (100-wine$alcohol-wine$residual.sugar/10)*1000 wine$density.predicted = wine$density.predicted/100 summary(wine$density.predicted) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 971.0 977.9 980.5 981.1 983.6 1014.0 summary(wine$density) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 987.1 992.3 994.9 994.7 997.0 1039.0 ggplot(wine,aes(x=density,y=density.predicted,color=type))+ geom_point()+facet_wrap(~type) ggplot(wine,aes(density.predicted-density,fill=type))+geom_histogram() The actual densities are larger than the predicted density for the wines. The difference is larger for red wines. There also seems to be more scatter in red wines, but the white wines seem to be fairly linear. I am if other quantities are associated with this difference. ggplot(wine,aes(x=sulphates,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=fixed.acidity,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=volatile.acidity,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=citric.acid,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=chlorides,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=free.sulfur.dioxide,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=total.sulfur.dioxide,y=density.predicted-density,color=type))+ geom_point() ggplot(wine,aes(x=quality,y=density.predicted-density,color=type))+ geom_point() It looks like sulfates(sulfates) are the only variable that correlates to the difference in density. On further reading about the affect of SO2 on wine, it looks like it is something that reduces free acidity. Lets look: ggplot(wine,aes(x=total.sulfur.dioxide,y=fixed.acidity,color=type))+geom_point() ggplot(wine,aes(x=free.sulfur.dioxide,y=fixed.acidity,color=type))+geom_point() summary(wine$residual.sugar) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.600 1.800 3.000 5.443 8.100 65.800 This plot confirms that lower sulfur dioxide is correlated with higher levels of fixed acidity. It also shows that white wines tend to have more SO2 and less acidity then red wines. The density of S02 at standard temp and pressure is 1032 g/L, but this is very temperature dependent. wine$density.predicted = 789*wine$alcohol wine$density.predicted = wine$density.predicted + 1587*wine$residual.sugar/10 wine$density.predicted = wine$density.predicted + 1032*wine$total.sulfur.dioxide/10 wine$density.predicted = wine$density.predicted + (100-wine$alcohol-wine$residual.sugar/10-wine$total.sulfur.dioxide/10)*1000 wine$density.predicted = wine$density.predicted/100 summary(wine$density.predicted) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 971.0 977.9 980.5 981.1 983.6 1014.0 summary(wine$density) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 987.1 992.3 994.9 994.7 997.0 1039.0 ggplot(wine,aes(x=density,y=density.predicted,color=type))+geom_point()+facet_wrap(~type) ggplot(wine,aes(density.predicted-density,fill=type))+geom_histogram() Not close for making up the discrepancy because of the low amount in the wine. It is correlated with acidity, and citric acid has a density of 1660 g/L. Tartaric acid also had a density of 1790 g/L.I’m going to include all the relevant densities in this final plot. Lets try these! wine$density.predicted = 789*wine$alcohol wine$density.predicted = wine$density.predicted + 1587*wine$residual.sugar/10 wine$density.predicted = wine$density.predicted + 1032*wine$total.sulfur.dioxide/10 wine$density.predicted = wine$density.predicted + 1660*wine$citric.acid/10+1790*wine$fixed.acidity/10+1050*wine$volatile.acidity/10 wine$density.predicted = wine$density.predicted + (100-wine$alcohol-wine$residual.sugar/10-wine$total.sulfur.dioxide/10-wine$citric.acid/10-wine$fixed.acidity/10-wine$volatile.acidity/10)*1000 wine$density.predicted = wine$density.predicted/100 summary(wine$density.predicted) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 975.3 983.8 986.6 987.0 989.6 1021.0 summary(wine$density) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 987.1 992.3 994.9 994.7 997.0 1039.0 ggplot(wine,aes(x=density,y=density.predicted,color=type))+geom_point() ggplot(wine,aes(density.predicted-density,fill=type))+geom_histogram() summary(wine$sulphates) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2200 0.4300 0.5100 0.5313 0.6000 2.0000 Including all the relevant chemicals drops the discrepency down to -5 g/L for whites and -10 g/L for red. Thats very surprising, but not a large improvement on the first/second trial. New Quality Factors wine$Quality &lt;- ifelse(as.numeric(wine$quality)+2 &lt; 7,&#34;Avg Rating &lt;= 6&#34;,&#34;Avg Rating &gt;= 7&#34;) wine$Quality &lt;- factor(wine$Quality) summary(factor(wine$Quality)) ## Avg Rating &lt;= 6 Avg Rating &gt;= 7 ## 5220 1277 Remake some of the previous plots with new quality classification ggplot(wine,aes(factor(type),density,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),fixed.acidity,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),volatile.acidity,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),citric.acid,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),residual.sugar,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),chlorides,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),free.sulfur.dioxide,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),total.sulfur.dioxide,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),sulphates,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),alcohol,fill=Quality))+geom_boxplot() ggplot(wine,aes(factor(type),pH,fill=Quality))+geom_boxplot() Final Plots and Summary ggplot(wine,aes(type,alcohol,fill=Quality))+ xlab(&#34;Wine Type&#34;)+ ylab(&#34;% Alcohol By Volume&#34;)+ ggtitle(&#34;Alcohol Content by Type and Quality&#34;)+ coord_flip()+ theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.background = element_blank(), legend.position=&#34;top&#34;, panel.grid.major.x = element_line(color=&#39;gray&#39;))+ scale_y_continuous(limits=c(8,15),breaks=c(8,9,10,11,12,13,14,15))+ scale_x_discrete(limits=c(&#34;red&#34;,&#34;white&#34;),labels=c(&#34;Red Wines&#34;,&#34;White Wines&#34;))+ scale_fill_manual(name = &#34;Quality&#34;, values = c(&#34;#C0C0C0&#34;, &#34;gold&#34;))+ geom_vline(xintercept=1.5,size=8, color=&#39;white&#39;)+ geom_boxplot() The above box plot shows the alcohol content of white wines (top) and red wines (bottom) for high qualities wines (gold) and lower qualities wines (silver). Across wine types, higher alcohol content is associated with being a wine that is rated higher. ggplot(wine,aes(x=density,y=density.predicted,color=type))+ theme(panel.background = element_blank(), legend.position=&#34;top&#34;, panel.grid.major = element_line(color=&#39;gray&#39;))+ xlab(&#34;Measured Denisty of Wine (g/L)&#34;)+ ylab(&#34;Predicted Density From Chemical Measurements (g/L)&#34;)+ ggtitle(&#34;Predicted Density vs Measured Density&#34;)+ scale_y_continuous(limits=c(970,1000))+ scale_x_continuous(limits=c(980,1005))+ geom_abline(intercept=0,slope=1,linetype=2,color=&#39;#554400&#39;,)+ geom_point() ## Warning: Removed 7 rows containing missing values (geom_point). The above plot shows the predicted density for the wine based on all the chemical measurements vs the measured density of the wine. The dashed line shows the values at which the two values would back. Since the points all fall below the line, the measured density is systematically higher than the predicted density. Additionally, there is something about red and white wines that make this discrepancy systematically different. ggplot(red,aes(x=quality,y=volatile.acidity,))+ theme(panel.background = element_blank(), legend.position=&#34;top&#34;, panel.grid.major = element_line(color=&#39;gray&#39;))+ xlab(&#34;Average Wine Rating&#34;)+ ylab(&#34;Tartaric Acid Density (g/L)&#34;)+ ggtitle(&#34;Fixed Acidity vs Average Rating for Red Wines&#34;)+ geom_line(stat=&#39;summary&#39;,fun.y=mean,color=&#34;#990000&#34;,size=1.2)+ geom_line(stat=&#39;summary&#39;,fun.y=quantile,prob=0.25,linetype=2,color=&#34;#990000&#34;,size=1)+ geom_line(stat=&#39;summary&#39;,fun.y=quantile,prob=0.75,linetype=2,color=&#34;#990000&#34;,size=1)+ geom_line(stat=&#39;summary&#39;,fun.y=quantile,prob=0.1,linetype=3,color=&#34;#990000&#34;)+ geom_line(stat=&#39;summary&#39;,fun.y=quantile,prob=0.9,linetype=3,color=&#34;#990000&#34;) The above line plot shows the average (solid), 25th and 75th percentiles (dashed), and 10th and 90th percentiles (dotted) tartaric acid density measured in the red wines. This chemical is responsible for the vinegar taste that can be found in many low prices and some high priced wines. There is a clear correlation with lower tartaric acid consecrations being associated with higher quality wines. Reflection This analysis was interesting and challenging for a number of reasons. One, these are chemical properties and I know I did not use the proper concentration models for chemical properties. Also, this is a data set that is not well documented on how it was assembled, how the quantities were measured, or the errors associated with the measurements. I personally would have also liked to know the temperature of the wines and the altitude of measurements. A number of chemical properties and their affect on taste depend on these values. Assuming we can trust the data, there are some interesting illuminations. More alcohol is associated with higher quality wines, density can fairly well predicted by the chemical information in the data set, and lower amounts of tartaric acid in red wines is correlated with higher average ratings. // add bootstrap table styles to pandoc tables $(document).ready(function () { $(&#39;tr.header&#39;).parent(&#39;thead&#39;).parent(&#39;table&#39;).addClass(&#39;table table-condensed&#39;); }); (function () { var script = document.createElement(&#34;script&#34;); script.type = &#34;text/javascript&#34;; script.src = &#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;; document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script); })();", 
            "tags": "udacity", 
            "loc": "http://www.bryantravissmith.com/udacity/udacity-project-3/"
        },
        {
            "title": "Udacity - Data Analysis NanoDegree - Project 2", 
            "text":"Udacity - Data Analysis NanoDegree Project 2 - Data Mugging The goal of this project is to read in data from openstreetmap.com stored in an XML format, reformat the data into JSON, load it into a MongoDB database, and audit the data for cleanliness and consistency. The dataset I used is of Colorado Springs CO, a place I have visited several times to see family and friends. The data set can be found here. A summary of the OpenStreetMap format and structur can be found on their documentation page The openstreetmap.com OSM XML file consists of three types of primitive elements: nodes, ways, and relations. Nodes are gps coordinates on a map, and they are used to identify features in a city or, in collections, use to construct ways. Nodes could have features, and these features are stored in subtags called “tag”. Ways, as just suggested, are a collection of nodes that create a path. Ways also have descriptions that are stored in “tag” subtags. Relations are a collection of ways, nodes, and relations that describe features of the map. They can be routes, restrictions, and multipolygon boundaries. The Colorado Springs data is contained in a 103 MB file that has the following numbers of each primitive element. Element Type No Sub-Elements Total Elements Node 468,637 480,216 Ways 0 55,926 Relations 0 153 Most of the nodes being empty signify that most nodes are likely to be used solely for defining ways. This is something that can be checked for consistency. There are two following up questions that one could explore here: Does the node for each way exist in this dataset? Is each node referenced in this data set? These questions are computational expensive, but could be necessary depending on the ultimate purpose of this data set. Since both ways and relationship are collections of subelements, it would not make sense that these elements are empty. Finding that any of them are empty would be a sign that there is something about the data the needs to be fixed. The documentation for these primitives states that ways and relationships are collections of 2 or more elements, something that will be checked later. Nodes Of the 11,624 nodes with subelement tags, 10,206 of them have a single description tag. Most of them are minimal descriptions with keys of “highway” (8185) and “power” (1204). There are some that are clearly incomplete. There are 157 nodes with the sole tag descriptions is “addr:house number.” Since an address usually consists of a number, street name, and a zip code, it seems clear that these 157 nodes are incomplete descriptions of a feature with an address. There is also one node with a key value “FIXME” and the value of “Denver area needs work.” The google maps of the position coordinates associated with this node shows that this is St. Francis Hospital in Colorado Springs. It is not clear why user “AMY-jin” put this information here because this hospital is 65 miles from Denver, Co. This is 1 of 4 contributions this user made There are only 1418 nodes with multi-tag descriptions. Ways Of the 35,928 ways, 24 of them only contain a single “nd” tag. The documentation defines a way of a polyline between 2 and 2000 nodes. These ways only contain 1 node. Thus there are 24 ways that are needed to be either updated or removed from the data set. Relations Of the 153 relations, 10 of them contain a single “member” tag. The documentation defines a relation between 2 or more members consisting of ways, nodes, or relationships. A relation with only 1 member is ill defined. Therefor there are 10 relationship that need to be updated or removed from the dataset. Problem Character Problems characters are defined as characters that have issues associated with storage in a database. These characters consists of the following: = + / &amp; &lt; &gt; ; &#39; &#34; ? % # $ @ , . \t \r \n There were no problem characters with any of the elements’ key-values. Of all the element values associated with the previously mentioned keys, there were 1,716 values that had problem characters associated with them. These values will be dropped before the data is inserted into the MongoDB Database MongoDB The nodes and ways from the dataset was formatted into the following JSON form, { &#34;id&#34;: &#34;2406124091&#34;, &#34;type&#34;: &#34;node&#34;, &#34;visible&#34;:&#34;true&#34;, &#34;created&#34;: { &#34;version&#34;:&#34;2&#34;, &#34;changeset&#34;:&#34;17206049&#34;, &#34;timestamp&#34;:&#34;2013-08-03T16:43:42Z&#34;, &#34;user&#34;:&#34;linuxUser16&#34;, &#34;uid&#34;:&#34;1219059&#34; }, &#34;pos&#34;: [41.9757030, -87.6921867], &#34;address&#34;: { &#34;housenumber&#34;: &#34;5157&#34;, &#34;postcode&#34;: &#34;60625&#34;, &#34;street&#34;: &#34;North Lincoln Ave&#34; }, &#34;key1&#34;:&#34;val1&#34;, &#34;key2&#34;:&#34;val2&#34;, } and inserted into a local MongoDB database. As measured in the previous section, key/value pairs with problem characters were dropped from database. The following commands using the pymongo library: c = MongoClient() db = c[&#39;test-database&#39;] osm = db.osm osm.count() osm.find({&#39;type&#39;:&#39;node&#39;}).count() osm.find({&#39;type&#39;:&#39;way&#39;}).count() Of the 536,142 nodes and ways listed in the original xml files, all 536,142 appeared in the database. The count of elements with type “ways” is 480216, and the count of elements with type “ways” is 55926. These values are consistent with the original examination of the XML file. Address - Street Looking at the address information we can find with the following commands that 5689 entries contain an “address” field and 5453 of these elements contain a subfield of “street”. pipeline = [{&#34;$match&#34;:{&#34;address&#34;:{&#34;$exists&#34;:1}}}] print len(osm.aggregate(pipeline)[&#39;result&#39;]) pipeline = [{&#34;$match&#34;:{&#34;address.street&#34;:{&#34;$exists&#34;:1}}}] print len(osm.aggregate(pipeline)[&#39;result&#39;]) An examination of the street fields shows that the data is mostly consistently formatted and clean. The street names do not generally have abbreviations, though there are some exceptions. Only 10 of the 5689 data points have values that are not just street names: pipeline = [{&#34;$match&#34;:{&#34;address.street&#34;:{&#34;$exists&#34;:1}}}, {&#34;$group&#34;:{&#34;_id&#34;:&#34;$address.street&#34;,&#34;count&#34;:{&#34;$sum&#34;:1}}}, {&#34;$sort&#34;:{&#34;_id&#34;:-1}}] street_descriptions = set() for x in osm.aggregate(pipeline)[&#39;result&#39;]: print x[&#39;_id&#39;] street_split = x[&#39;_id&#39;].split(&#34; &#34;) length = len(street_split) street_descriptions.add(street_split[length-1]) for y in street_descriptions: print y The common inconsistencies in the street names are abbreviations. Some examples are that “Dr.” is used for “Drive”, “E.” is used for “East”, and “Ave” is used for “Avenue”. These values are exceptions, surprisingly. Users The dataset was produced by 300 users, the top user named “FrozenFlame22” contributing 424,585 of the 536,142 data entries (79.1%). The top users are: Username Number of Entries Associated with User Percent of Dataset FrozenFlame22 424,585 79.8% CS_Mur 20185 3.79% Jason R Surrat 15930 3.0% Your Village Maps 15628 2.94% GPS_dr 12643 2.38% Chris CA 7194 1.35% Mark Newnham 3108 0.58% SpenSmith 2738 0.51% woodpeck_fixbo 2566 0.48% This information was generated with the following code: pipeline = [{&#34;$group&#34;:{&#34;_id&#34;:&#34;$created.user&#34;,&#34;count&#34;:{&#34;$sum&#34;:1}}}, {&#34;$sort&#34;:{&#34;count&#34;:-1}}] result = osm.aggregate(pipeline)[&#39;result&#39;] sum = 0 for x in result: print x, x[&#39;count&#39;]/536142.0 sum += x[&#39;count&#39;] print sum/536142.0 print len(result) It is possible that a majority of these users are cleaning data, and not generating data, for openstreetmap. Whenever a new data point is entered the server automatically generates it with version of “1”. Looking at the subset of entries with version equal to 1 shows the following top 10 users. Username New Entries FrozenFlame22 343854 Jason R Surrat 14780 CS_Mur 12646 Your Village Maps 10684 GPS_dr 9486 Chris CA 4036 Mark Newnham 3034 SpenSmith 2430 gps_pilot 2187 Rub21 1615 The 9 of the 10 top-ten contributes to the osm data are also the top-ten original generators of edits of Colorado Springs dataset. pipeline = [{&#34;$match&#34;:{&#34;created.version&#34;:&#34;1&#34;}}, {&#34;$group&#34;:{&#34;_id&#34;:&#34;$created.user&#34;,&#34;count&#34;:{&#34;$sum&#34;:1}}}, {&#34;$sort&#34;:{&#34;count&#34;:-1}}, {&#34;$limit&#34;:10}] result = osm.aggregate(pipeline)[&#39;result&#39;] for x in result: print x, x[&#39;count&#39;] Fences Fences are way to enforce a boundary around a given area. Given there is a military base in Colorado springs, one would expect to find a number of barbed wire fences. pipeline = [{&#34;$match&#34;:{&#34;fence_type&#34;:{&#34;$exists&#34;:1}}}, {&#34;$group&#34;:{&#34;_id&#34;:&#34;$fence_type&#34;,&#34;count&#34;:{&#34;$sum&#34;:1}}}] result = osm.aggregate(pipeline)[&#39;result&#39;] fences = [] counts = [] for x in result: print x fences.append(x[&#39;_id&#39;]) counts.append(x[&#39;count&#39;]) import numpy as np import matplotlib.pyplot as plt ind = np.arange(len(fences)) # the x locations for the groups width = 0.35 # the width of the bars: can also be len(x) sequence p1 = plt.bar(ind, np.array(counts),width, color=&#39;r&#39;) plt.ylabel(&#39;Fence Count&#39;) plt.title(&#39;Count of Fences in Colorado Springs, CO&#39;) plt.xticks(ind+width/2., fences, rotation=10 ) plt.show() The previous code generate the following plot. The most cataloged fence in the dataset is a pole fence. There are only 2 barbed wire fences. I know for fact that this is far from a full catalog of the fences in Colorado Spring. My visit to the military base there showed me there is infact more than 2 fences with barbed wire. This is one clear aspect of the map in need of improvement. The fact that fences are ways, and not nodes, is consistent with the definition of ways being paths and a collection of nodes. pipeline = [{&#34;$match&#34;:{&#34;fence_type&#34;:&#34;barbed_wire&#34;}}] result = osm.aggregate(pipeline)[&#39;result&#39;] for nd in result[0][&#39;node_refs&#39;]: for y in osm.find({&#34;id&#34;:nd}): print y[&#39;pos&#39;] print &#39;next&#39; for nd in result[1][&#39;node_refs&#39;]: for y in osm.find({&#34;id&#34;:nd}): print y[&#39;pos&#39;] The first barbed wire fence in the dataset is around the Zebulon Pike Detention Center, and the second barbed wire fence is also around the same Zebulon Pike Detention Center. These two fences could be the same fence around the complex, or two sub fences around buildings. The google street view of the area shows that there appears to be a single short barbed wire fence around the property. This highlights one of the major issues I see with auditing the openstreetmap data. Nodes are points, but ways are collections of points. Two ways may have different number of nodes, likes the barbed wire fences, use a difference collection of nodes, like the two barbed wire fences, but are attempting to describe the same feature. An examination of the user that created these two separate features shows that it was the same user, Jason R Surratt. If the same user is capable of double describing the same feature for a map, then it highlights an issues with openstreetmap’s ability to uniquely describe features in an area or on a map that are a collection of points. Conclusion Raw data is messy. Even with a dataset that has been ‘cleaned’ by dozens of users, like the dataset used in this paper, there are hundreds of issues and inconsistency. Before the data could be loaded into MongoDB there are over 1700 key/values with invalid characters. Additionally there are dozens of ways and relationships with only a single subelement. A review of the address data showed that there was inconsistencies with the street names. This data was obviously previously cleaned, and there are still aspects that escaped being cleaned. It is important to realizing that any cleaning process will not catch all the inconsistencies in a dataset. Additionally the dataset has relationships, and with relationship produce an additional layer of complexity. Relationships need to be correctly and consistently formatted. The examination of barbed wire fences in the Colorado Spring dataset showed that even in the case of data entered/edited by the same user can lead to the same fence being imputed twice with complete different sets of nodes. There are doubtlessly a number of features in this data yet undiscovered that will have the same problem.", 
            "tags": "Udacity", 
            "loc": "http://www.bryantravissmith.com/udacity/udacity-project-2/"
        },
        {
            "title": "Udacity - Data Analysis NanoDegree - Project 1", 
            "text":"Udacity - Data Analysis NanoDegree Project 1 The goal of project one is to apply some of the concepts in Udacity.com&#39;s Intro to Data Science course to find interesting patterns or trends in New York subway data. A descript of the variables in the data set can be found here. The question that we are investigating is: Does subway ridership change when it rains? Lets begin with loading the data %matplotlib inline import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(&#39;ggplot&#39;) plt.rcParams[&#39;figure.figsize&#39;] = (12, 12) from ggplot import * import scipy import scipy.stats import datetime import statsmodels.api as sm import statsmodels.formula.api as smf import math from IPython.display import display from IPython.display import HTML filename = &#39;turnstile_weather_v2.csv&#39; subway_data = pd.read_csv(filename, parse_dates=[&#39;datetime&#39;]) subway_data.head() UNIT DATEn TIMEn ENTRIESn EXITSn ENTRIESn_hourly EXITSn_hourly datetime hour day_week ... pressurei rain tempi wspdi meanprecipi meanpressurei meantempi meanwspdi weather_lat weather_lon 0 R003 05-01-11 00:00:00 4388333 2911002 0 0 2011-05-01 00:00:00 0 6 ... 30.22 0 55.9 3.5 0 30.258 55.98 7.86 40.700348 -73.887177 1 R003 05-01-11 04:00:00 4388333 2911002 0 0 2011-05-01 04:00:00 4 6 ... 30.25 0 52.0 3.5 0 30.258 55.98 7.86 40.700348 -73.887177 2 R003 05-01-11 12:00:00 4388333 2911002 0 0 2011-05-01 12:00:00 12 6 ... 30.28 0 62.1 6.9 0 30.258 55.98 7.86 40.700348 -73.887177 3 R003 05-01-11 16:00:00 4388333 2911002 0 0 2011-05-01 16:00:00 16 6 ... 30.26 0 57.9 15.0 0 30.258 55.98 7.86 40.700348 -73.887177 4 R003 05-01-11 20:00:00 4388333 2911002 0 0 2011-05-01 20:00:00 20 6 ... 30.28 0 52.0 10.4 0 30.258 55.98 7.86 40.700348 -73.887177 5 rows × 27 columns Exploration print &#34;Subway Data Variable Names&#34;, subway_data.columns print &#34;&#34; print &#34;Number of Subway Units&#34;, subway_data.UNIT.nunique() print &#34;Number of Subway Stations&#34;, subway_data.station.nunique() Subway Data Variable Names Index([u&#39;UNIT&#39;, u&#39;DATEn&#39;, u&#39;TIMEn&#39;, u&#39;ENTRIESn&#39;, u&#39;EXITSn&#39;, u&#39;ENTRIESn_hourly&#39;, u&#39;EXITSn_hourly&#39;, u&#39;datetime&#39;, u&#39;hour&#39;, u&#39;day_week&#39;, u&#39;weekday&#39;, u&#39;station&#39;, u&#39;latitude&#39;, u&#39;longitude&#39;, u&#39;conds&#39;, u&#39;fog&#39;, u&#39;precipi&#39;, u&#39;pressurei&#39;, u&#39;rain&#39;, u&#39;tempi&#39;, u&#39;wspdi&#39;, u&#39;meanprecipi&#39;, u&#39;meanpressurei&#39;, u&#39;meantempi&#39;, u&#39;meanwspdi&#39;, u&#39;weather_lat&#39;, u&#39;weather_lon&#39;], dtype=&#39;object&#39;) Number of Subway Units 240 Number of Subway Stations 207 The data consists of subway data for 240 turnstiles in 207 stations with data take every 4 hours recording the number of people entering and exiting the subway through the turnstiles, the date and time information associated with the measurement, and the corresponding weather measurements at the time of each measurement from a weather station. Because this investigation has to do with how the weather, specifically rain, affects how the subway is used, it is important to get a feel for the data set. A google calendar is displayed below that marks the dates that rain occurred at or around New York subway stations. There are also two events in New York that happened during the time the data taken. It should be noted that the 6 of the 7 days which it rained in New York of May 2011 were weekdays. Only one was during a weekend which was Saturday, May 15. It should also be noticed that May 30th is Memorial Day. It is not a weekday, but it is not a workday. Finally, Bin Laden was reported killed on May 2nd, 2011. This is important because there was a vigil that night at the World Trade Center, and a speech given by President Obama at the World Trade Center on May 5th, 2011. These events may have altered normal subway traffic, though there is not any obvious effect in the below graphs. plt.figure() sd_date = subway_data.groupby([&#39;DATEn&#39;]) sd_date.ENTRIESn_hourly.sum().plot(legend=True) sd_date.EXITSn_hourly.sum().plot(legend=True) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;Total Riders&#39;) plt.savefig(&#39;test1.png&#39;, bbox_inches=&#39;tight&#39;) plt.figure() sd_date = subway_data.groupby([&#39;DATEn&#39;,&#39;hour&#39;]) sd_date.ENTRIESn_hourly.sum().plot(legend=True) sd_date.EXITSn_hourly.sum().plot(legend=True) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;Total Riders&#39;) plt.savefig(&#39;test2.png&#39;, bbox_inches=&#39;tight&#39;) The above figures shows the total number of hourly exits (blue) and entries (red) of the New York subway system during the month of May. The top figure is summed over each day, while the bottome figure is summed over each 4 hour block. There is an cyclic pattern in both graphs. The top graph shows a 7 day cycle of high ridership during the week and lower ridership during the weekends. The bottom graph shows a 24 hour cycle that is double peaked for week days and single peaked for weekends. The entry peaks on weekdays are approximately 2.5 times larger than the weekends, with the exception of May 30th, which has a peak consistent with weekends. This further justifies that thinking about workdays vs non-workdays is more relavant than classifying days as weekends and non-weekends. It is also noteworthy that the number of exits are always less than the number of entries. If true, it implies that there is a net flow of people leaving the subways that are not being monitored. For this reason the analysis will only be done on the entry data, and not the exit data On weekdays/workdays, there are double peaks, one in the morning and one in the afternoon. The minimum number of entries also appears to have a cycle, peaking in the middle of the week and being smallest on Sunday mornings. One last point is that for a given day of the week, the values of hourly entries for any time/hour fluctuate from week to week but are similar in value. subway_data.station.unique() sd_station = subway_data.groupby([&#39;station&#39;]) plt.figure() temp.plot(kind=&#39;hist&#39;,legend=True) plt.ylabel(&#34;Number of Stations&#34;) plt.figure() temp = sd_station.ENTRIESn_hourly.sum() temp.sort(&#39;ENTRIESn_hourly&#39;) temp[-15:].plot(kind=&#39;barh&#39;,legend=True) plt.ylabel(&#34;Station&#34;) plt.xlabel(&#34;Total Entries Per Month&#34;) plt.figure() temp = sd_station.ENTRIESn_hourly.sum() temp.sort(&#39;ENTRIESn_hourly&#39;) temp[:15].plot(kind=&#39;barh&#39;,legend=True) plt.ylabel(&#34;Station&#34;) plt.xlabel(&#34;Total Entries Per Month&#34;) &lt;matplotlib.text.Text at 0x114773250&gt; The amount of ridership also varies from station to station. Most stations have less than 250,000 total entires per month. The most active stations have almost 3 million total entries per month while the least active states have less than 10,000 entries per month. wt_data = subway_data[subway_data.station==&#39;WORLD TRADE CTR&#39;] wt_data.plot(x=&#39;datetime&#39;, y=[&#39;ENTRIESn_hourly&#39;,&#39;EXITSn_hourly&#39;],legend=False) plt.xticks(rotation=20) plt.xlabel(&#34;Date and Hour&#34;) plt.ylabel(&#34;WTC Total Entries per 4 hours&#34;) &lt;matplotlib.text.Text at 0x1197b06d0&gt; Finally, an inspection of the World Trade Center graph shows that there is not any obviously change in the number of people entering or exiting this station of May 2nd or May 5th. News reports from the dates do not convey the number of people that attended the events. Photos from the May 5th speech show no more than a few hundred people, and news reports estimate, on the high end, a few thousand in attendance on May 2nd vigil. Both counts are small compared to the peak ridership in excess of 20,000. It is also not clear, even if thousands attended, how many would have rode the subway specifically to attend the vigil. The Effect of Rain The goal of this analysis is to determine the effect rain has on ridership, which can be measured by the numbr of people entering the subway system. We can examine the results of rain_data = subway_data.groupby(&#39;rain&#39;) plt.figure() rain_data.ENTRIESn_hourly.plot(kind=&#39;hist&#39;,legend=True) plt.legend([&#39;No Rain&#39;,&#39;Rain&#39;]) &lt;matplotlib.legend.Legend at 0x11dc2c050&gt; The obvious feature is the distribution looks similar to a Poisson distribution, a distribution that describes the occurrence of a given number of events in a fix interval (spatial or temporal). In this case the occurrence would be the number of people entering the station. A second feature is the scale/frequency of the two distributions are different. This is largely due there being 7 rain days and 24 non­rain days. To make a comparison of the distributions this data must be normalized and a corresponding density plot be produced. plt.figure() rain_data.ENTRIESn_hourly.plot(kind=&#39;kde&#39;,legend=True) plt.legend([&#39;No Rain&#39;,&#39;Rain&#39;]) x1,x2,y1,y2 = plt.axis() plt.axis((0,20000,y1,y2)) (0, 20000, 0.0, 0.00050000000000000001) The above density plot shows that the two distributions have similar forms, still similar to the Poisson distribution, but the density of hourly entries when it rains is more skewed right than when it does not rain. The area under each curve is 1, so the scale is taken away. This is relevant because even though the distributions are visually different, we can not determine if it is because fewer people are riding when ridership is low, more people are riding when ridership is high, or some other effects are occurring when it rains. One possible effect is that 6 of the 7 rainy days occur on weekdays, where the biggest peaks occur. Also 9 of the 11 days off are days where it does not rain, and these have the lowest values of the hourly entries. It is not clear whether the difference in these distributions are due to the rain, or what days the rain occurred. Statistical tests will help get to the bottom of this question. Statistical Test The goal of a statistical test is to use probability to estimate whether a discrepancy between an expected set of measurements is significantly different than the actual results of those measurements. A variety of statistical tests exists, and the validity of these tests depend on conditions of the test. Some of these tests ascertain whether some summary statistic is significantly different from an expected summary statistic, and others help ascertain whether the distribution of the populations are significantly different. The two used in this analysis will be the student t-test and the Mann-Whitney U-test. The significant level is the cutoff probability that the produced/experimental/observed distribution could be produced through randomness if our expected results were true. A typical value for this quantity is 5%, which is what will be used in this analysis. Student t-test The student t-test is a comparison of the averages of the two distributions. This is a valid test because the sample size is much larger than 30, so by the central limit theorem the distribution of averages is normally distributed even though the original distribution is similar to a Poisson distribution. The validity of the test also assumes that the observed values are independent. This is probably not a true assumption because of the cyclic nature of the events. But since the original distribution is similar to a Poisson distribution, a distribution that assumed independence, it is probably approximately true assumption for this test. The naive comparison of dividing the data into the a set when it rained and when it does not rain leads to the following statistics: temp = pd.DataFrame(rain_data.ENTRIESn_hourly.count()) temp.columns = [&#39;count&#39;] temp[&#39;mean&#39;] = rain_data.ENTRIESn_hourly.mean() temp[&#39;median&#39;] = rain_data.ENTRIESn_hourly.median() temp[&#39;std&#39;] = rain_data.ENTRIESn_hourly.std() temp count mean median std rain 0 33064 1845.539439 893 2878.770848 1 9585 2028.196035 939 3189.433373 rain = subway_data[subway_data.rain==1] norain = subway_data[subway_data.rain==0] results = scipy.stats.ttest_ind(norain.ENTRIESn_hourly,rain.ENTRIESn_hourly,equal_var=False) print &#34;T Score:&#34;, results[0] print &#34;p-value&#34;, results[1] T Score: -5.04288274762 p-value 4.64140243163e-07 This data produces a t-value of -5.042, and a two-sided p-value of 4.64 x 10­7. This implies that these two populations have statistically significant averages. Keep in mind that a statistical test does not give a clear attribution to the underlying cause of the difference. As pointed out in the previous section, the difference in the average number of riders could have more to do with what days it rained on rather than the fact that it rained. To separate out this effect, the data should be segmented into days that are weekends and holidays, and days that are not. subway_data[&#39;workday&#39;] = 1 subway_data.loc[(subway_data.weekday==0)|(subway_data.DATEn==&#39;05-30-11&#39;),&#39;workday&#39;] = 0 rw_data = subway_data.groupby([&#39;workday&#39;,&#39;rain&#39;]) temp = pd.DataFrame(rw_data.ENTRIESn_hourly.count()) temp.columns = [&#39;count&#39;] temp[&#39;mean&#39;] = rw_data.ENTRIESn_hourly.mean() temp[&#39;median&#39;] = rw_data.ENTRIESn_hourly.median() temp[&#39;std&#39;] = rw_data.ENTRIESn_hourly.std() temp count mean median std workday rain 0 0 11789 1204.860548 662.0 1701.321541 1 1793 1065.858338 596.0 1539.449874 1 0 21275 2200.555347 1070.0 3304.904186 1 7792 2249.637449 1057.5 3421.444563 Before we separated the data into workdays and non-workdays, the data showed a significant increase in average hourly entires when it rained. There was also an significant increase in the median values of hourly entries. Now that we look at the data subsetted into workdays and non-wordays. Non-work days see a decrease in the average hourly entries when it rains, and workdays show a slight increase in average hourly entries. The same trends is also observed for the median values of hourly entries into the subway. nowork_rain = subway_data.loc[(subway_data.rain==1)&amp;(subway_data.workday==0)] nowork_norain = subway_data.loc[(subway_data.rain==0)&amp;(subway_data.workday==0)] results = scipy.stats.ttest_ind(nowork_norain.ENTRIESn_hourly,nowork_rain.ENTRIESn_hourly,equal_var=False) print &#34;2-Sided T-Test for change in average number of hourly entries on non-workdays&#34; print &#34;T Score:&#34;, results[0] print &#34;p-value&#34;, results[1] print &#34;&#34; work_rain = subway_data.loc[(subway_data.rain==1)&amp;(subway_data.workday==1)] work_norain = subway_data.loc[(subway_data.rain==0)&amp;(subway_data.workday==1)] results = scipy.stats.ttest_ind(work_norain.ENTRIESn_hourly,work_rain.ENTRIESn_hourly,equal_var=False) print &#34;2-Sided T-Test for change in average number of hourly entries on workdays&#34; print &#34;T Score:&#34;, results[0] print &#34;p-value&#34;, results[1] 2-Sided T-Test for change in average number of hourly entries on non-workdays T Score: 3.51114255229 p-value 0.000454063724831 2-Sided T-Test for change in average number of hourly entries on workdays T Score: -1.09321652976 p-value 0.274318319876 The non-workday data produces a t-value of 3.51, and a two-sided p-value of 4.54 x 10-3. This says that these two populations have statistically significant difference in averages. What should be noted is that previously it looked like more people ride the subway when it rained, but when the data is subsetted into workdays and not-workdays, it looks like less people ride the subway when it rains on a non-workday. It is the opposite effect observed in the naive investigation. It has a name, Simpson’s Paradox. The workday data produces a t-value of -1.09, and a two-sided p-value of 0.274. The difference in these average is not statistically significant because the p-value is above 0.05. These difference could likely be produced by pure chance, and we would need more data to determine if rain does alter the ridership of the subway on workdays. Mann-Whitney U-test The Mann-Whitney U-Test is a non-parametric test that determines the probability that a randomly chosen member of ‘larger’ set is larger than a randomly chosen member of a ‘smaller’ set. This is a valid test because the values are independent (same assumption as before) and the values have an order. Typically this test is not valid unless both sets have at least 20 points, which has been shown above in the section on the student t-test. The results for this test give a 1 sided p-value. The summary of the results can be found in the table below. This test is a better test for non-normal distributions, like the one we have. Because of the size of our data set, however, the results are very comparable. It should be noted that if the results did not match then the Mann-Whitney would be a more authoritative test. nowork_rain = subway_data.loc[(subway_data.rain==1)&amp;(subway_data.workday==0)] nowork_norain = subway_data.loc[(subway_data.rain==0)&amp;(subway_data.workday==0)] results = scipy.stats.mannwhitneyu(nowork_norain.ENTRIESn_hourly,nowork_rain.ENTRIESn_hourly) print &#34;U-Test different distrubtions of hourly entries on non-workdays&#34; print &#34;U Score:&#34;, results[0] print &#34;p-value&#34;, results[1] print &#34;&#34; work_rain = subway_data.loc[(subway_data.rain==1)&amp;(subway_data.workday==1)] work_norain = subway_data.loc[(subway_data.rain==0)&amp;(subway_data.workday==1)] results = scipy.stats.mannwhitneyu(work_norain.ENTRIESn_hourly,work_rain.ENTRIESn_hourly) print &#34;U-Test different distrubtions of hourly entries on workdays&#34; print &#34;U Score:&#34;, results[0] print &#34;p-value&#34;, results[1] 2-Sided T-Test for change in average number of hourly entries on non-workdays T Score: 10160674.0 p-value 0.00415933264978 2-Sided T-Test for change in average number of hourly entries on workdays T Score: 82547361.5 p-value 0.295771577293 The results for the Mann-Whitney U-Test are very similar to the those of the student t-test. When all the data is considered there is a statically significant difference between the size of the ridership when it rains compared to when it is not raining. The medians demonstrate that the ‘larger’ population for this statistical test is the population of riders when it is raining. The other two results demonstrate the previously mentioned Simpson&#39;s Paradox in that the order of the results of the test is reversed. Days without rain have a larger ridership on both workdays and non-workdays when considered separately. And just like the student t-test, the difference is statistically significant on days off, but not on workdays. Summary of Statistical Tests In summary, the statistical test allows for a way to ask questions about two populations being different. On a population as a whole, it is clear that both the student t-test and the Mann-Whitney U-test support the hypothesis that there are more riders when it rains. In the first section it was noted that weekday ridership was drastically larger than weekend ridership, and that more rainy days fell on weekdays where ridership was large. The total population being composed of a subset of two populations with different riding behavior bias this result, so the data was subsumed into the two underlying populations. ￼ The effect of rain on workday riders was not statistically different for either statistical test. The effect of riders entering the subway on non-workdays was statistically different, and both tests showed that ridership dropped when it rained. These tests are not conclusive about the effect of rain. The hour of the day is showed to have strong explanatory power for the number of hourly entries is explained in the next section. This is also obvious in the previous graphs. The effect on this variable, however, can not be explored with this data set because the rain is only recorded daily and not hourly. This dataset does not have accurate information for rain in 4 hour increments, so further analysis will be deferred. This analysis focused on the total number of people entering the subway system when it rained, but there are 207 entry points into this system. Even though we have showed there are statistically significant differences, the underlying cause is still not illuminated. That, too, would require further analysis. Regression The goal of regression is to make predictions or find functional relationships for dependent variables from independent data based on a model. Ordinary Least Squares Regressions are models where the predicted variable, y, is a linear sum of variables, x: $y = a_0 + a_1 x_1+ a_2 x_2 + ....$ where y is the dependant variable, $x_i$ is an independent variable, and $a_i$ are the coefficients. The biggest features that were observed in the initial analysis were that the number of riders varied cyclically over 24 hours, different stations had different amounts of ridership, and workdays had significantly more ridership than non-workdays. The goal of this analysis is to create a regression that models these features, then determine if including information about the rain significantly alters the model. In order to facilitate the fit we will include new variables and that depend on the cyclic variation. These variables will be $ cos(2 \pi hour / 24), sin(2 \pi hour / 24), cos(2 \pi day_week / 7), &amp; cos(2 \pi day_week / 7) .$ Additioanlly, we will also make a training set of the first 24 days of the month, and test the predictive ability on the last 7 days of the month. totals = subway_data.groupby([&#39;datetime&#39;,&#39;day_week&#39;,&#39;hour&#39;,&#39;workday&#39;,&#39;rain&#39;]) total_data = totals.ENTRIESn_hourly.sum().reset_index() total_data[&#39;cos_day&#39;] = np.cos(2*math.pi*total_data.day_week/7) total_data[&#39;sin_day&#39;] = np.sin(2*math.pi*total_data.day_week/7) total_data[&#39;cos_hour&#39;] = np.cos(2*math.pi*total_data.hour/24) total_data[&#39;sin_hour&#39;] = np.sin(2*math.pi*total_data.hour/24) train = total_data[total_data.datetime &lt; &#39;2011-05-25&#39;] test = total_data[total_data.datetime &gt; &#39;2011-05-25&#39;] The graphs at the beginning of this analysis showed the number of entries into the subway system is heavily influenced by the hour of the day and if the day is a workday or not. The most general linear combination of these variables are fitted to the first 24 days of data. result = smf.ols(formula=&#34;ENTRIESn_hourly ~ hour*workday*cos_hour + hour*workday*sin_hour&#34;, data=train).fit() result.summary() OLS Regression Results Dep. Variable: ENTRIESn_hourly R-squared: 0.595 Model: OLS Adj. R-squared: 0.569 Method: Least Squares F-statistic: 22.61 Date: Wed, 20 May 2015 Prob (F-statistic): 6.41e-28 Time: 07:46:20 Log-Likelihood: -2466.8 No. Observations: 181 AIC: 4958. Df Residuals: 169 BIC: 4996. Df Model: 11 Covariance Type: nonrobust coef std err t P&gt;|t| [95.0% Conf. Int.] Intercept -1.235e+06 1.53e+06 -0.808 0.420 -4.25e+06 1.78e+06 hour 1.529e+05 1.52e+05 1.003 0.317 -1.48e+05 4.54e+05 workday -2.806e+06 1.8e+06 -1.562 0.120 -6.35e+06 7.41e+05 hour:workday 3.18e+05 1.79e+05 1.774 0.078 -3.58e+04 6.72e+05 cos_hour 1.545e+06 1.52e+06 1.016 0.311 -1.46e+06 4.55e+06 hour:cos_hour -1.037e+05 1.06e+05 -0.983 0.327 -3.12e+05 1.05e+05 workday:cos_hour 2.74e+06 1.79e+06 1.534 0.127 -7.86e+05 6.27e+06 hour:workday:cos_hour -1.799e+05 1.24e+05 -1.451 0.149 -4.24e+05 6.48e+04 sin_hour -1.673e+05 3.04e+05 -0.551 0.582 -7.67e+05 4.32e+05 hour:sin_hour 7.931e+04 6.26e+04 1.266 0.207 -4.43e+04 2.03e+05 workday:sin_hour -6.124e+04 3.54e+05 -0.173 0.863 -7.6e+05 6.38e+05 hour:workday:sin_hour 1.61e+05 7.39e+04 2.178 0.031 1.51e+04 3.07e+05 Omnibus: 80.504 Durbin-Watson: 2.164 Prob(Omnibus): 0.000 Jarque-Bera (JB): 228.585 Skew: -1.923 Prob(JB): 2.31e-50 Kurtosis: 6.940 Cond. No. 3.81e+03 Using this fit, we make a prediction on what the number of hourly entires for the last 7 days should look like. test2 = test.set_index(&#39;datetime&#39;) test2[&#39;predict&#39;] = result.predict(test) plt.figure() test2.ENTRIESn_hourly.plot() test2.predict.plot() &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f33de50&gt; This fit captures a number of features, including the double peak on workdays and the different behavior for memorial days. We can find the $r^2$ for this fit on the test data. diff = test2.ENTRIESn_hourly.values-test2.ENTRIESn_hourly.mean() diff2 = test2.ENTRIESn_hourly.values-test2.predict.values print &#34;R-squared on prediction:&#34;, 1-(diff2.dot(diff2)/diff.dot(diff)) R-squared on prediction: 0.811683091424 Over 80% of the variance of this test data is accounted for by the model. We can now see if including information abou the rain improve the performance. result = smf.ols(formula=&#34;ENTRIESn_hourly ~ rain*hour*workday*cos_hour + rain*hour*workday*sin_hour&#34;, data=train).fit() result.summary() OLS Regression Results Dep. Variable: ENTRIESn_hourly R-squared: 0.604 Model: OLS Adj. R-squared: 0.546 Method: Least Squares F-statistic: 10.40 Date: Wed, 20 May 2015 Prob (F-statistic): 2.05e-21 Time: 07:47:25 Log-Likelihood: -2464.9 No. Observations: 181 AIC: 4978. Df Residuals: 157 BIC: 5055. Df Model: 23 Covariance Type: nonrobust coef std err t P&gt;|t| [95.0% Conf. Int.] Intercept -1.39e+06 1.81e+06 -0.768 0.443 -4.96e+06 2.18e+06 rain 5.582e+05 3.64e+06 0.153 0.878 -6.64e+06 7.75e+06 hour 1.719e+05 1.8e+05 0.953 0.342 -1.84e+05 5.28e+05 rain:hour -6.956e+04 3.63e+05 -0.191 0.848 -7.87e+05 6.48e+05 workday -2.621e+06 2.16e+06 -1.213 0.227 -6.89e+06 1.65e+06 rain:workday -6.436e+05 4.19e+06 -0.154 0.878 -8.91e+06 7.62e+06 hour:workday 2.946e+05 2.16e+05 1.367 0.174 -1.31e+05 7.2e+05 rain:hour:workday 8.311e+04 4.18e+05 0.199 0.843 -7.42e+05 9.08e+05 cos_hour 1.742e+06 1.8e+06 0.968 0.334 -1.81e+06 5.29e+06 rain:cos_hour -7.256e+05 3.62e+06 -0.200 0.841 -7.88e+06 6.43e+06 hour:cos_hour -1.167e+05 1.25e+05 -0.934 0.352 -3.63e+05 1.3e+05 rain:hour:cos_hour 4.757e+04 2.51e+05 0.189 0.850 -4.48e+05 5.43e+05 workday:cos_hour 2.497e+06 2.15e+06 1.162 0.247 -1.75e+06 6.74e+06 rain:workday:cos_hour 8.677e+05 4.16e+06 0.209 0.835 -7.35e+06 9.09e+06 hour:workday:cos_hour -1.652e+05 1.49e+05 -1.108 0.270 -4.6e+05 1.29e+05 rain:hour:workday:cos_hour -5.263e+04 2.88e+05 -0.182 0.855 -6.22e+05 5.17e+05 sin_hour -1.916e+05 3.6e+05 -0.532 0.595 -9.03e+05 5.2e+05 rain:sin_hour 1.001e+05 7.2e+05 0.139 0.890 -1.32e+06 1.52e+06 hour:sin_hour 8.999e+04 7.39e+04 1.217 0.225 -5.6e+04 2.36e+05 rain:hour:sin_hour -3.917e+04 1.5e+05 -0.261 0.794 -3.36e+05 2.57e+05 workday:sin_hour -1.605e+04 4.25e+05 -0.038 0.970 -8.55e+05 8.23e+05 rain:workday:sin_hour -1.69e+05 8.25e+05 -0.205 0.838 -1.8e+06 1.46e+06 hour:workday:sin_hour 1.467e+05 8.88e+04 1.651 0.101 -2.88e+04 3.22e+05 rain:hour:workday:sin_hour 5.061e+04 1.73e+05 0.293 0.770 -2.91e+05 3.92e+05 Omnibus: 81.829 Durbin-Watson: 2.161 Prob(Omnibus): 0.000 Jarque-Bera (JB): 242.045 Skew: -1.932 Prob(JB): 2.76e-53 Kurtosis: 7.143 Cond. No. 9.63e+03 test3 = test.set_index(&#39;datetime&#39;) test3[&#39;predict&#39;] = result.predict(test) test3.ENTRIESn_hourly.plot() test3.predict.plot() &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f306a90&gt; diff = test3.ENTRIESn_hourly.values-test2.ENTRIESn_hourly.mean() diff2 = test3.ENTRIESn_hourly.values-test2.predict.values print &#34;R-squared on prediction:&#34;, 1-(diff2.dot(diff2)/diff.dot(diff)) R-squared on prediction: 0.811683091424 This is the same $r^2$ as the fit without including information about the rain. For this simple model, rain does not imporve its performance. Summary There is not any evidence that the information of the rain has any explanatory power for being able to estimate the number of riders entering the subway at a given station. The time of day and the day being a workday or not offer the majority of the explanatory power of the number of riders entering a given station. Rain does not have significant explanatory power on these regression fits. Conclusion The goal of this analysis is to determine if the number of people riding the subway changes when it rains. There is no evidence to support this statement based on the data. When the data is subsetted into the two ridership patterns of workday and non-workdays, there is not a significant increase in ridership on rainy workdays. Rainy non-workdays, on the other hand, show a decrease in ridership. The Mann-Whitney analysis supports that the size of the change in ridership is a small effect. The regression of the data show that the time of day, the station under consideration, and fact of if the day is a workday or not has great explanatory power on the data set. Over 90% of the variance can be explained by the fit shown in this analysis. Attempting to incorporate rain in the regression does not have an statistically significant improvements in the fits. Reflection Obviously with any statistical test there is a possibility to confuse a statistically significant difference with an attribution of the independent variables causing the shift or change in the dependant variables. The data is not fine enough to do a set of statistical tests that subset the data by hour and workday since the categorial rain variable is set by day and not by hour. There is a variable called ‘condition’ set hourly, but there were days where rain = 0 and the conditions = ‘Scattered Showers’. This condition variable is inconsistent with the rain variable. Since it was not in the original data set, rain was used in this analysis as the more dependable variable. It could be possible that the original rain variable was incorrect. The data only contained 31 days of data. There were 7 rainy days and 1 was a holiday. Even though the data covered 240 turnstiles at 207 stations, there were only 7 rainy days at a given time of day, and 24 non-rainy days at the same given time of day for any single station. Because ridership varies so much with time, comparing similar times against each other when it is raining and not raining would be necessary to draw any strong conclusions.", 
            "tags": "Udacity", 
            "loc": "http://www.bryantravissmith.com/udacity/udacity-project-1/"
        },
        {
            "title": "About", 
            "text":"Bryan Travis Smith, Ph.D Hidy-Ho! Welcome to my corner of the interwebs. This site exists to show off projects I am working on, and to post things I am interesting in. Topics will likely include particle phsyics, robotics, artifical intelligence, machine learning, and martial arts. I hold a Ph.D in High Energy Particle Physics from UC Irvine, am half way through a online M.S. in Computer Science from Georgia Tech, and have just finished a Nanodegree from Udacity.com for Data Analysis. I will be spending the summer of 2015 in San Francisco at the Zipfian Academy completing a data science boootcamp. For the last 7 years I have been teaching physics and robotics at The Buckley School, New Roads, and The Bishop&#39;s School, have mentored multiple FIRST and VEX highschool robotics team, and have completed programming projects in my spare time. On top of being analytically minded, I am also a life long Martial Artist. I currently hold the rank of Yondan (4th Degree Black Belt) in Okinawan Karate and am currently one of three Assitant Chief Instructors for the Okinawan Goju Ryu Kenkyu Kai In a past life I did some origianl research. Feel free to check it out. SCIENTIFIC PUBLICATIONS Goldilocks Supersymmetry: Simultaneous Solution to the Dark Matter and Flavor Problems of Supersymmetry, Phys.Rev.Lett.100:021302, 2008 Determining Spins of Metastable Sleptons at the Large Hadron Collider, Phys.Rev. D76:115004, 2007 Discovering SUSY with m02 &lt; 0 in the First LHC Physics Run, Phys.Rev.D75:115015, 2007 Minimal Supergravity with m02 &lt; 0, Phys.Rev. D74 (2006) 015013 Collider Signatures of SuperWIMP Warm Dark Matter Jose A. R. Cembranos, Jonathan L. Feng, Arvind Rajaraman, Bryan T. Smith, Fumihiro Takayama SuperWIMP Cosmology and Collider Physics Jonathan L. Feng, Arvind Rajaraman, Bryan T. Smith, Shufang Su, Fumihiro Takayama Slepton Trapping at the Large Hadron and International Linear Colliders, Phys.Rev. D71:015004, 2005; Erratum-ibid.D71:019904, 2005", 
            "tags": "pages", 
            "loc": "http://www.bryantravissmith.com/about/"
        }        
    ]
}